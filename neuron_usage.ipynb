{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications import VGG19, InceptionV3\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.engine.topology import InputLayer\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras import backend as K\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "from attr import attrs, attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_size = (224, 224, 3)\n",
    "\n",
    "random_img = np.random.uniform(size=im_size)\n",
    "model_input = keras.layers.Input(shape=im_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-eddd003c3f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_vgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muntrained_vgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhash/.local/lib/python3.5/site-packages/keras/applications/vgg19.py\u001b[0m in \u001b[0;36mVGG19\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    165\u001b[0m             weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n\u001b[1;32m    166\u001b[0m                                     \u001b[0mWEIGHTS_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                                     cache_subdir='models')\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
      "\u001b[0;32m/home/dhash/.local/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 urlretrieve(origin, fpath,\n\u001b[0;32m--> 201\u001b[0;31m                             functools.partial(dl_progress, progbar=progbar))\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m    935\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                   self.__class__)\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \"\"\"\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_vgg = VGG19(input_tensor=model_input)\n",
    "untrained_vgg = VGG19(input_tensor=model_input, weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@attrs\n",
    "class GraphableNeruon(object):\n",
    "    weights = attrib()\n",
    "    bias = attrib()\n",
    "    layer = attrib()\n",
    "    neuron_number = attrib()\n",
    "    layer_type = attrib()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neurons(net):\n",
    "    neurons = []\n",
    "    for i, layer in enumerate(net.layers):\n",
    "        if type(layer) is Conv2D:\n",
    "            wts, bias = layer.get_weights()\n",
    "            for nn, (w, b) in enumerate(zip(wts.T, bias)):\n",
    "                neurons.append(GraphableNeruon(w, b, i, nn, layer.get_config()))\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_neurons(neurons, net):\n",
    "    last_neuron = neurons[0]\n",
    "    layer = net.layers[last_neuron.layer]\n",
    "    layer_weights, layer_bias = layer.get_weights()\n",
    "    layer_weights = layer_weights.T\n",
    "    for n in neurons:\n",
    "        if not last_neuron.layer == n.layer:\n",
    "            layer.set_weights([layer_weights.T, layer_bias])\n",
    "            layer = net.layers[n.layer]\n",
    "            layer_weights, layer_bias = layer.get_weights()\n",
    "            layer_weights = layer_weights.T\n",
    "            last_neuron = n\n",
    "        \n",
    "        layer_weights[n.neuron_number] = n.weights\n",
    "        layer_bias[n.neuron_number] = n.bias\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f set_neurons set_neurons(get_neurons(trained_vgg), trained_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neurons = get_neurons(trained_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neurons[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_vgg.layers[1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "untrained_vgg.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "untrained_vgg.fit(x_train, y_train, epochs=1, verbose=1, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 2.2510 - acc: 0.1900 - val_loss: 2.2162 - val_acc: 0.2495\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 2.21623696899\n",
      "Test accuracy: 0.2495\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 2.1750 - acc: 0.2400 - val_loss: 2.0653 - val_acc: 0.4992\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 2.06529358788\n",
      "Test accuracy: 0.4992\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 2.0052 - acc: 0.4200 - val_loss: 1.8722 - val_acc: 0.6350\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 1.87216213474\n",
      "Test accuracy: 0.635\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.7211 - acc: 0.5100 - val_loss: 1.5087 - val_acc: 0.5869\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 1.50867934589\n",
      "Test accuracy: 0.5869\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.4953 - acc: 0.5200 - val_loss: 1.2677 - val_acc: 0.6488\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 1.26770366325\n",
      "Test accuracy: 0.6488\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.4954 - acc: 0.5500 - val_loss: 1.2288 - val_acc: 0.6162\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 1.22882126999\n",
      "Test accuracy: 0.6162\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.4563 - acc: 0.5400 - val_loss: 1.0676 - val_acc: 0.6947\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 1.0675838232\n",
      "Test accuracy: 0.6947\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.1749 - acc: 0.6000 - val_loss: 1.0819 - val_acc: 0.6152\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 1.08191457663\n",
      "Test accuracy: 0.6152\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.1865 - acc: 0.6000 - val_loss: 0.9723 - val_acc: 0.6972\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.972253913307\n",
      "Test accuracy: 0.6972\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.0405 - acc: 0.6700 - val_loss: 0.7326 - val_acc: 0.8013\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.732570315742\n",
      "Test accuracy: 0.8013\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 1.0577 - acc: 0.6800 - val_loss: 0.6977 - val_acc: 0.8016\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.69766946888\n",
      "Test accuracy: 0.8016\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.9660 - acc: 0.6300 - val_loss: 0.6643 - val_acc: 0.8003\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.664346033764\n",
      "Test accuracy: 0.8003\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.8651 - acc: 0.7200 - val_loss: 0.6629 - val_acc: 0.7942\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.66291317234\n",
      "Test accuracy: 0.7942\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.9202 - acc: 0.7000 - val_loss: 0.6826 - val_acc: 0.7843\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.682621328545\n",
      "Test accuracy: 0.7843\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.7054 - acc: 0.7800 - val_loss: 0.5660 - val_acc: 0.8343\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.565979580402\n",
      "Test accuracy: 0.8343\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.6993 - acc: 0.7400 - val_loss: 0.5042 - val_acc: 0.8543\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.50420092299\n",
      "Test accuracy: 0.8543\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.6110 - acc: 0.8500 - val_loss: 0.5055 - val_acc: 0.8506\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.505458733106\n",
      "Test accuracy: 0.8506\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4310 - acc: 0.8900 - val_loss: 0.4679 - val_acc: 0.8659\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.467894723701\n",
      "Test accuracy: 0.8659\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4847 - acc: 0.8300 - val_loss: 0.4471 - val_acc: 0.8709\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.447110357082\n",
      "Test accuracy: 0.8709\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.7152 - acc: 0.7900 - val_loss: 0.4471 - val_acc: 0.8704\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.447113676524\n",
      "Test accuracy: 0.8704\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.6405 - acc: 0.8200 - val_loss: 0.4354 - val_acc: 0.8814\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.435358274865\n",
      "Test accuracy: 0.8814\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5194 - acc: 0.8300 - val_loss: 0.4144 - val_acc: 0.8889\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.414413591421\n",
      "Test accuracy: 0.8889\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4218 - acc: 0.8800 - val_loss: 0.3994 - val_acc: 0.8825\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.399384390223\n",
      "Test accuracy: 0.8825\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4221 - acc: 0.8900 - val_loss: 0.3823 - val_acc: 0.8913\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.38233070116\n",
      "Test accuracy: 0.8913\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5025 - acc: 0.8400 - val_loss: 0.3631 - val_acc: 0.8965\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.363091016066\n",
      "Test accuracy: 0.8965\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3274 - acc: 0.8700 - val_loss: 0.4337 - val_acc: 0.8624\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.433651623583\n",
      "Test accuracy: 0.8624\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5956 - acc: 0.8600 - val_loss: 0.4008 - val_acc: 0.8775\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.400828453398\n",
      "Test accuracy: 0.8775\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5341 - acc: 0.8500 - val_loss: 0.3470 - val_acc: 0.8956\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.346957607627\n",
      "Test accuracy: 0.8956\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4591 - acc: 0.8500 - val_loss: 0.3409 - val_acc: 0.9022\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.340896794116\n",
      "Test accuracy: 0.9022\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4235 - acc: 0.8500 - val_loss: 0.3243 - val_acc: 0.9059\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.324268613958\n",
      "Test accuracy: 0.9059\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.4670 - acc: 0.8600 - val_loss: 0.3887 - val_acc: 0.8794\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.388688118947\n",
      "Test accuracy: 0.8794\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4714 - acc: 0.8100 - val_loss: 0.3112 - val_acc: 0.9105\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.311202207333\n",
      "Test accuracy: 0.9105\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4720 - acc: 0.8900 - val_loss: 0.3276 - val_acc: 0.9071\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.327553956413\n",
      "Test accuracy: 0.9071\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4537 - acc: 0.8800 - val_loss: 0.3175 - val_acc: 0.9090\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.317456137842\n",
      "Test accuracy: 0.909\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3097 - acc: 0.9200 - val_loss: 0.3070 - val_acc: 0.9087\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.306973975274\n",
      "Test accuracy: 0.9087\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4193 - acc: 0.8700 - val_loss: 0.3189 - val_acc: 0.9099\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.318863476396\n",
      "Test accuracy: 0.9099\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4257 - acc: 0.8600 - val_loss: 0.3088 - val_acc: 0.9109\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.30881463573\n",
      "Test accuracy: 0.9109\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5171 - acc: 0.8700 - val_loss: 0.3202 - val_acc: 0.9080\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.320200185764\n",
      "Test accuracy: 0.908\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3322 - acc: 0.8900 - val_loss: 0.2789 - val_acc: 0.9216\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.278866030484\n",
      "Test accuracy: 0.9216\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4029 - acc: 0.8500 - val_loss: 0.2640 - val_acc: 0.9229\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.26397842342\n",
      "Test accuracy: 0.9229\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4489 - acc: 0.8800 - val_loss: 0.2751 - val_acc: 0.9224\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.275060353762\n",
      "Test accuracy: 0.9224\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4395 - acc: 0.8700 - val_loss: 0.2685 - val_acc: 0.9256\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.268488838691\n",
      "Test accuracy: 0.9256\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3630 - acc: 0.9000 - val_loss: 0.2722 - val_acc: 0.9260\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.272180554324\n",
      "Test accuracy: 0.926\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3754 - acc: 0.8600 - val_loss: 0.2540 - val_acc: 0.9292\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.254011343211\n",
      "Test accuracy: 0.9292\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2037 - acc: 0.9400 - val_loss: 0.2444 - val_acc: 0.9308\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.244354245943\n",
      "Test accuracy: 0.9308\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3384 - acc: 0.9000 - val_loss: 0.2689 - val_acc: 0.9193\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.2688827077\n",
      "Test accuracy: 0.9193\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3820 - acc: 0.8600 - val_loss: 0.2481 - val_acc: 0.9275\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.248145559275\n",
      "Test accuracy: 0.9275\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5010 - acc: 0.8600 - val_loss: 0.2571 - val_acc: 0.9234\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.257063913572\n",
      "Test accuracy: 0.9234\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3369 - acc: 0.9000 - val_loss: 0.2536 - val_acc: 0.9267\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.253550201738\n",
      "Test accuracy: 0.9267\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4052 - acc: 0.9000 - val_loss: 0.2384 - val_acc: 0.9327\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.238376178652\n",
      "Test accuracy: 0.9327\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2347 - acc: 0.9400 - val_loss: 0.2223 - val_acc: 0.9350\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.222252580181\n",
      "Test accuracy: 0.935\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5229 - acc: 0.8100 - val_loss: 0.2339 - val_acc: 0.9330\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.23388303228\n",
      "Test accuracy: 0.933\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2091 - acc: 0.9800 - val_loss: 0.2188 - val_acc: 0.9378\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.218815043807\n",
      "Test accuracy: 0.9378\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3851 - acc: 0.8900 - val_loss: 0.2232 - val_acc: 0.9365\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.223248014167\n",
      "Test accuracy: 0.9365\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2209 - acc: 0.9400 - val_loss: 0.2115 - val_acc: 0.9383\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.211542964521\n",
      "Test accuracy: 0.9383\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3611 - acc: 0.9100 - val_loss: 0.2266 - val_acc: 0.9346\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.226649281311\n",
      "Test accuracy: 0.9346\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3979 - acc: 0.9000 - val_loss: 0.2219 - val_acc: 0.9374\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.221861521825\n",
      "Test accuracy: 0.9374\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2924 - acc: 0.9000 - val_loss: 0.2435 - val_acc: 0.9311\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.243465895191\n",
      "Test accuracy: 0.9311\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3382 - acc: 0.9100 - val_loss: 0.2212 - val_acc: 0.9356\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.221246492404\n",
      "Test accuracy: 0.9356\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2026 - acc: 0.9400 - val_loss: 0.2055 - val_acc: 0.9387\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.205531962088\n",
      "Test accuracy: 0.9387\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1687 - acc: 0.9500 - val_loss: 0.2052 - val_acc: 0.9393\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.2051716528\n",
      "Test accuracy: 0.9393\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2305 - acc: 0.9500 - val_loss: 0.2041 - val_acc: 0.9390\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.204134816118\n",
      "Test accuracy: 0.939\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3053 - acc: 0.8700 - val_loss: 0.2107 - val_acc: 0.9372\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.210704989859\n",
      "Test accuracy: 0.9372\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1466 - acc: 0.9700 - val_loss: 0.2164 - val_acc: 0.9347\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.21641702693\n",
      "Test accuracy: 0.9347\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3414 - acc: 0.9000 - val_loss: 0.2257 - val_acc: 0.9351\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.225747965147\n",
      "Test accuracy: 0.9351\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1737 - acc: 0.9500 - val_loss: 0.2005 - val_acc: 0.9430\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.200537754469\n",
      "Test accuracy: 0.943\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2870 - acc: 0.9100 - val_loss: 0.2013 - val_acc: 0.9419\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.201260257962\n",
      "Test accuracy: 0.9419\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1586 - acc: 0.9300 - val_loss: 0.2125 - val_acc: 0.9412\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.212504911062\n",
      "Test accuracy: 0.9412\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3919 - acc: 0.8900 - val_loss: 0.2128 - val_acc: 0.9386\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.212794092722\n",
      "Test accuracy: 0.9386\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3609 - acc: 0.8700 - val_loss: 0.2135 - val_acc: 0.9387\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.213514809123\n",
      "Test accuracy: 0.9387\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3716 - acc: 0.9000 - val_loss: 0.1973 - val_acc: 0.9430\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.197345735735\n",
      "Test accuracy: 0.943\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3237 - acc: 0.9000 - val_loss: 0.2095 - val_acc: 0.9393\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.209499990611\n",
      "Test accuracy: 0.9393\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4510 - acc: 0.8900 - val_loss: 0.2019 - val_acc: 0.9416\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.201867964867\n",
      "Test accuracy: 0.9416\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3204 - acc: 0.9100 - val_loss: 0.2053 - val_acc: 0.9428\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.205293085092\n",
      "Test accuracy: 0.9428\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1531 - acc: 0.9600 - val_loss: 0.2006 - val_acc: 0.9413\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.200638485229\n",
      "Test accuracy: 0.9413\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2379 - acc: 0.9400 - val_loss: 0.2043 - val_acc: 0.9399\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.204296192309\n",
      "Test accuracy: 0.9399\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2209 - acc: 0.9500 - val_loss: 0.2022 - val_acc: 0.9400\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.202175586572\n",
      "Test accuracy: 0.94\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3783 - acc: 0.8700 - val_loss: 0.2130 - val_acc: 0.9356\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.212959936019\n",
      "Test accuracy: 0.9356\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3813 - acc: 0.8700 - val_loss: 0.1896 - val_acc: 0.9421\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.189620566748\n",
      "Test accuracy: 0.9421\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4004 - acc: 0.9200 - val_loss: 0.1795 - val_acc: 0.9489\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.179464675424\n",
      "Test accuracy: 0.9489\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1380 - acc: 0.9500 - val_loss: 0.1784 - val_acc: 0.9481\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.178394174393\n",
      "Test accuracy: 0.9481\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2910 - acc: 0.9200 - val_loss: 0.1824 - val_acc: 0.9465\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.182440346622\n",
      "Test accuracy: 0.9465\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4903 - acc: 0.8900 - val_loss: 0.1894 - val_acc: 0.9470\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.189435308287\n",
      "Test accuracy: 0.947\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1818 - acc: 0.9400 - val_loss: 0.1782 - val_acc: 0.9483\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.178225858705\n",
      "Test accuracy: 0.9483\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3889 - acc: 0.9000 - val_loss: 0.1794 - val_acc: 0.9492\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.179423176399\n",
      "Test accuracy: 0.9492\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2064 - acc: 0.9300 - val_loss: 0.1747 - val_acc: 0.9490\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.174701771058\n",
      "Test accuracy: 0.949\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4696 - acc: 0.8400 - val_loss: 0.1878 - val_acc: 0.9455\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.187787420158\n",
      "Test accuracy: 0.9455\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.5567 - acc: 0.8400 - val_loss: 0.1832 - val_acc: 0.9485\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.183185400815\n",
      "Test accuracy: 0.9485\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3495 - acc: 0.9100 - val_loss: 0.1855 - val_acc: 0.9480\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.185478211451\n",
      "Test accuracy: 0.948\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3403 - acc: 0.9300 - val_loss: 0.1907 - val_acc: 0.9460\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.190674858484\n",
      "Test accuracy: 0.946\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.2411 - acc: 0.9600 - val_loss: 0.1776 - val_acc: 0.9502\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.177617257701\n",
      "Test accuracy: 0.9502\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1916 - acc: 0.9500 - val_loss: 0.1749 - val_acc: 0.9487\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.174936590178\n",
      "Test accuracy: 0.9487\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2460 - acc: 0.9300 - val_loss: 0.1783 - val_acc: 0.9493\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.178349420442\n",
      "Test accuracy: 0.9493\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2245 - acc: 0.9300 - val_loss: 0.1750 - val_acc: 0.9480\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.174968700647\n",
      "Test accuracy: 0.948\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3198 - acc: 0.9300 - val_loss: 0.1736 - val_acc: 0.9493\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.173583813104\n",
      "Test accuracy: 0.9493\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2292 - acc: 0.9400 - val_loss: 0.1709 - val_acc: 0.9505\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.17091102418\n",
      "Test accuracy: 0.9505\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1952 - acc: 0.9500 - val_loss: 0.1849 - val_acc: 0.9454\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.184902770126\n",
      "Test accuracy: 0.9454\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2852 - acc: 0.9300 - val_loss: 0.1740 - val_acc: 0.9505\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.173996570113\n",
      "Test accuracy: 0.9505\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1568 - acc: 0.9300 - val_loss: 0.1652 - val_acc: 0.9522\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.165159356548\n",
      "Test accuracy: 0.9522\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1389 - acc: 0.9500 - val_loss: 0.1723 - val_acc: 0.9492\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.172250917138\n",
      "Test accuracy: 0.9492\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2340 - acc: 0.9400 - val_loss: 0.1663 - val_acc: 0.9506\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.166291728814\n",
      "Test accuracy: 0.9506\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2603 - acc: 0.9200 - val_loss: 0.1618 - val_acc: 0.9513\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.161767257491\n",
      "Test accuracy: 0.9513\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4327 - acc: 0.8900 - val_loss: 0.1793 - val_acc: 0.9477\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.179323811197\n",
      "Test accuracy: 0.9477\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1190 - acc: 0.9800 - val_loss: 0.1753 - val_acc: 0.9481\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.175315520267\n",
      "Test accuracy: 0.9481\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1488 - acc: 0.9400 - val_loss: 0.1602 - val_acc: 0.9527\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.160176866841\n",
      "Test accuracy: 0.9527\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0747 - acc: 0.9800 - val_loss: 0.1608 - val_acc: 0.9502\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.160755709347\n",
      "Test accuracy: 0.9502\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0959 - acc: 0.9800 - val_loss: 0.1566 - val_acc: 0.9525\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.156640217207\n",
      "Test accuracy: 0.9525\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2968 - acc: 0.9000 - val_loss: 0.1583 - val_acc: 0.9520\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.158307492426\n",
      "Test accuracy: 0.952\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3310 - acc: 0.9100 - val_loss: 0.1512 - val_acc: 0.9543\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.151248322947\n",
      "Test accuracy: 0.9543\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2464 - acc: 0.9300 - val_loss: 0.1544 - val_acc: 0.9532\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.154421154731\n",
      "Test accuracy: 0.9532\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2093 - acc: 0.9400 - val_loss: 0.1599 - val_acc: 0.9535\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.159861022006\n",
      "Test accuracy: 0.9535\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1805 - acc: 0.9300 - val_loss: 0.1576 - val_acc: 0.9535\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.157575810385\n",
      "Test accuracy: 0.9535\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2705 - acc: 0.9300 - val_loss: 0.1525 - val_acc: 0.9532\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.152485353943\n",
      "Test accuracy: 0.9532\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1696 - acc: 0.9300 - val_loss: 0.1531 - val_acc: 0.9534\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.153116754998\n",
      "Test accuracy: 0.9534\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1331 - acc: 0.9700 - val_loss: 0.1542 - val_acc: 0.9531\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.154209736481\n",
      "Test accuracy: 0.9531\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3752 - acc: 0.8800 - val_loss: 0.1525 - val_acc: 0.9547\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.152513093443\n",
      "Test accuracy: 0.9547\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2636 - acc: 0.9200 - val_loss: 0.1693 - val_acc: 0.9491\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.169251944934\n",
      "Test accuracy: 0.9491\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3628 - acc: 0.9000 - val_loss: 0.1615 - val_acc: 0.9528\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.161537025017\n",
      "Test accuracy: 0.9528\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1899 - acc: 0.9600 - val_loss: 0.1554 - val_acc: 0.9560\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.155436759662\n",
      "Test accuracy: 0.956\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1981 - acc: 0.9500 - val_loss: 0.1556 - val_acc: 0.9541\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.155581715377\n",
      "Test accuracy: 0.9541\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1718 - acc: 0.9600 - val_loss: 0.1495 - val_acc: 0.9535\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.149500614768\n",
      "Test accuracy: 0.9535\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1902 - acc: 0.9200 - val_loss: 0.1845 - val_acc: 0.9440\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.184519983345\n",
      "Test accuracy: 0.944\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3164 - acc: 0.9200 - val_loss: 0.1589 - val_acc: 0.9496\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.158915231044\n",
      "Test accuracy: 0.9496\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2528 - acc: 0.9400 - val_loss: 0.1593 - val_acc: 0.9522\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.159284259377\n",
      "Test accuracy: 0.9522\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2469 - acc: 0.9400 - val_loss: 0.1522 - val_acc: 0.9545\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.152173659005\n",
      "Test accuracy: 0.9545\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4150 - acc: 0.8700 - val_loss: 0.1618 - val_acc: 0.9525\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.161847028486\n",
      "Test accuracy: 0.9525\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3896 - acc: 0.9000 - val_loss: 0.1516 - val_acc: 0.9563\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.151574692599\n",
      "Test accuracy: 0.9563\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1255 - acc: 0.9700 - val_loss: 0.1449 - val_acc: 0.9568\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.1449345393\n",
      "Test accuracy: 0.9568\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1445 - acc: 0.9500 - val_loss: 0.1458 - val_acc: 0.9567\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.145795458744\n",
      "Test accuracy: 0.9567\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2566 - acc: 0.9200 - val_loss: 0.1432 - val_acc: 0.9577\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.14318976268\n",
      "Test accuracy: 0.9577\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3544 - acc: 0.9100 - val_loss: 0.1419 - val_acc: 0.9572\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.141865372656\n",
      "Test accuracy: 0.9572\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1963 - acc: 0.9300 - val_loss: 0.1352 - val_acc: 0.9612\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.135222492778\n",
      "Test accuracy: 0.9612\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1296 - acc: 0.9600 - val_loss: 0.1318 - val_acc: 0.9620\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.131769100362\n",
      "Test accuracy: 0.962\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3226 - acc: 0.9100 - val_loss: 0.1374 - val_acc: 0.9586\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.137383061052\n",
      "Test accuracy: 0.9586\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1874 - acc: 0.9600 - val_loss: 0.1389 - val_acc: 0.9603\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.138912754652\n",
      "Test accuracy: 0.9603\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1280 - acc: 0.9700 - val_loss: 0.1359 - val_acc: 0.9600\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.135871469744\n",
      "Test accuracy: 0.96\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2576 - acc: 0.9100 - val_loss: 0.1427 - val_acc: 0.9580\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.142674767691\n",
      "Test accuracy: 0.958\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2844 - acc: 0.9200 - val_loss: 0.1346 - val_acc: 0.9618\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.134561702644\n",
      "Test accuracy: 0.9618\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1548 - acc: 0.9400 - val_loss: 0.1303 - val_acc: 0.9621\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.130294928201\n",
      "Test accuracy: 0.9621\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3283 - acc: 0.9300 - val_loss: 0.1354 - val_acc: 0.9599\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.135426719093\n",
      "Test accuracy: 0.9599\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2769 - acc: 0.9300 - val_loss: 0.1325 - val_acc: 0.9630\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.132494899486\n",
      "Test accuracy: 0.963\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2303 - acc: 0.9300 - val_loss: 0.1302 - val_acc: 0.9613\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.13016964281\n",
      "Test accuracy: 0.9613\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2644 - acc: 0.9200 - val_loss: 0.1270 - val_acc: 0.9635\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.12700495345\n",
      "Test accuracy: 0.9635\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3648 - acc: 0.8800 - val_loss: 0.1271 - val_acc: 0.9630\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.127126546826\n",
      "Test accuracy: 0.963\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1559 - acc: 0.9400 - val_loss: 0.1244 - val_acc: 0.9629\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.124392330659\n",
      "Test accuracy: 0.9629\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2230 - acc: 0.9300 - val_loss: 0.1245 - val_acc: 0.9620\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.124508397559\n",
      "Test accuracy: 0.962\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2358 - acc: 0.9400 - val_loss: 0.1250 - val_acc: 0.9624\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.124965173976\n",
      "Test accuracy: 0.9624\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4429 - acc: 0.8900 - val_loss: 0.1279 - val_acc: 0.9613\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.127929625265\n",
      "Test accuracy: 0.9613\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2336 - acc: 0.9200 - val_loss: 0.1215 - val_acc: 0.9634\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.121512670469\n",
      "Test accuracy: 0.9634\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1766 - acc: 0.9400 - val_loss: 0.1222 - val_acc: 0.9627\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.122219278869\n",
      "Test accuracy: 0.9627\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1211 - acc: 0.9800 - val_loss: 0.1220 - val_acc: 0.9634\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.121965521695\n",
      "Test accuracy: 0.9634\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3595 - acc: 0.9000 - val_loss: 0.1232 - val_acc: 0.9633\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.123168674879\n",
      "Test accuracy: 0.9633\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1834 - acc: 0.9400 - val_loss: 0.1214 - val_acc: 0.9631\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.121419733465\n",
      "Test accuracy: 0.9631\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1524 - acc: 0.9500 - val_loss: 0.1215 - val_acc: 0.9631\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.12147664313\n",
      "Test accuracy: 0.9631\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3186 - acc: 0.9300 - val_loss: 0.1205 - val_acc: 0.9649\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.120548437665\n",
      "Test accuracy: 0.9649\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1750 - acc: 0.9400 - val_loss: 0.1182 - val_acc: 0.9639\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.118243552577\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0604 - acc: 0.9800 - val_loss: 0.1189 - val_acc: 0.9634\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.118947469442\n",
      "Test accuracy: 0.9634\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2276 - acc: 0.9200 - val_loss: 0.1242 - val_acc: 0.9631\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.124182671152\n",
      "Test accuracy: 0.9631\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3196 - acc: 0.8800 - val_loss: 0.1160 - val_acc: 0.9639\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.115974965548\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3544 - acc: 0.9200 - val_loss: 0.1221 - val_acc: 0.9628\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.122061689355\n",
      "Test accuracy: 0.9628\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2557 - acc: 0.9300 - val_loss: 0.1176 - val_acc: 0.9645\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.11756376678\n",
      "Test accuracy: 0.9645\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1639 - acc: 0.9600 - val_loss: 0.1187 - val_acc: 0.9643\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.118715269933\n",
      "Test accuracy: 0.9643\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1492 - acc: 0.9500 - val_loss: 0.1148 - val_acc: 0.9643\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.114761375102\n",
      "Test accuracy: 0.9643\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0871 - acc: 0.9800 - val_loss: 0.1165 - val_acc: 0.9639\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.11647183617\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1789 - acc: 0.9700 - val_loss: 0.1144 - val_acc: 0.9647\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.114421164151\n",
      "Test accuracy: 0.9647\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1629 - acc: 0.9600 - val_loss: 0.1138 - val_acc: 0.9639\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.113832543718\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2600 - acc: 0.9600 - val_loss: 0.1146 - val_acc: 0.9642\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.114584208974\n",
      "Test accuracy: 0.9642\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2203 - acc: 0.9200 - val_loss: 0.1157 - val_acc: 0.9643\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.115736470271\n",
      "Test accuracy: 0.9643\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1369 - acc: 0.9700 - val_loss: 0.1140 - val_acc: 0.9641\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.113958771561\n",
      "Test accuracy: 0.9641\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2699 - acc: 0.9200 - val_loss: 0.1152 - val_acc: 0.9638\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.115163255285\n",
      "Test accuracy: 0.9638\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2945 - acc: 0.9200 - val_loss: 0.1154 - val_acc: 0.9642\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.115413181298\n",
      "Test accuracy: 0.9642\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2499 - acc: 0.9300 - val_loss: 0.1118 - val_acc: 0.9649\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.111842050979\n",
      "Test accuracy: 0.9649\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3338 - acc: 0.9200 - val_loss: 0.1129 - val_acc: 0.9650\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.112890097506\n",
      "Test accuracy: 0.965\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0735 - acc: 0.9900 - val_loss: 0.1142 - val_acc: 0.9645\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.114179984447\n",
      "Test accuracy: 0.9645\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2690 - acc: 0.9400 - val_loss: 0.1194 - val_acc: 0.9634\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.119415196437\n",
      "Test accuracy: 0.9634\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2157 - acc: 0.9300 - val_loss: 0.1148 - val_acc: 0.9639\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.114842708766\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1744 - acc: 0.9500 - val_loss: 0.1158 - val_acc: 0.9636\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.115768515839\n",
      "Test accuracy: 0.9636\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2332 - acc: 0.9300 - val_loss: 0.1179 - val_acc: 0.9635\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.117887232413\n",
      "Test accuracy: 0.9635\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1845 - acc: 0.9100 - val_loss: 0.1093 - val_acc: 0.9653\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.109276423789\n",
      "Test accuracy: 0.9653\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1296 - acc: 0.9500 - val_loss: 0.1105 - val_acc: 0.9657\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.110527157574\n",
      "Test accuracy: 0.9657\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1743 - acc: 0.9500 - val_loss: 0.1081 - val_acc: 0.9650\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.108090710956\n",
      "Test accuracy: 0.965\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1007 - acc: 0.9700 - val_loss: 0.1097 - val_acc: 0.9638\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.109739419547\n",
      "Test accuracy: 0.9638\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1051 - acc: 0.9700 - val_loss: 0.1087 - val_acc: 0.9645\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.108745452535\n",
      "Test accuracy: 0.9645\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1762 - acc: 0.9600 - val_loss: 0.1115 - val_acc: 0.9633\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.111509123858\n",
      "Test accuracy: 0.9633\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2155 - acc: 0.9300 - val_loss: 0.1241 - val_acc: 0.9618\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.124139357463\n",
      "Test accuracy: 0.9618\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1473 - acc: 0.9800 - val_loss: 0.1168 - val_acc: 0.9636\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.116810029455\n",
      "Test accuracy: 0.9636\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1173 - acc: 0.9600 - val_loss: 0.1174 - val_acc: 0.9627\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.117398562842\n",
      "Test accuracy: 0.9627\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1641 - acc: 0.9600 - val_loss: 0.1082 - val_acc: 0.9646\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.10817647676\n",
      "Test accuracy: 0.9646\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0788 - acc: 0.9700 - val_loss: 0.1078 - val_acc: 0.9648\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.10780252622\n",
      "Test accuracy: 0.9648\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0899 - acc: 0.9600 - val_loss: 0.1116 - val_acc: 0.9639\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.111594252132\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1920 - acc: 0.9500 - val_loss: 0.1091 - val_acc: 0.9648\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.109140867987\n",
      "Test accuracy: 0.9648\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2024 - acc: 0.9500 - val_loss: 0.1111 - val_acc: 0.9648\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.111113295238\n",
      "Test accuracy: 0.9648\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1908 - acc: 0.9700 - val_loss: 0.1131 - val_acc: 0.9639\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.113131761353\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1966 - acc: 0.9400 - val_loss: 0.1065 - val_acc: 0.9653\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.106513269707\n",
      "Test accuracy: 0.9653\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1248 - acc: 0.9400 - val_loss: 0.1070 - val_acc: 0.9656\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.107001004741\n",
      "Test accuracy: 0.9656\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1498 - acc: 0.9600 - val_loss: 0.1073 - val_acc: 0.9662\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.107334405596\n",
      "Test accuracy: 0.9662\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0536 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9655\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.10755340635\n",
      "Test accuracy: 0.9655\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0602 - acc: 0.9900 - val_loss: 0.1059 - val_acc: 0.9655\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.105868457515\n",
      "Test accuracy: 0.9655\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1581 - acc: 0.9500 - val_loss: 0.1128 - val_acc: 0.9639\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.112782128897\n",
      "Test accuracy: 0.9639\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1661 - acc: 0.9400 - val_loss: 0.1085 - val_acc: 0.9651\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.108524376279\n",
      "Test accuracy: 0.9651\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2336 - acc: 0.9500 - val_loss: 0.1057 - val_acc: 0.9664\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.105687578111\n",
      "Test accuracy: 0.9664\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2714 - acc: 0.9300 - val_loss: 0.1048 - val_acc: 0.9664\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.104779177248\n",
      "Test accuracy: 0.9664\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1703 - acc: 0.9500 - val_loss: 0.1060 - val_acc: 0.9664\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.106042871086\n",
      "Test accuracy: 0.9664\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1804 - acc: 0.9800 - val_loss: 0.1080 - val_acc: 0.9658\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.107986175059\n",
      "Test accuracy: 0.9658\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.1081 - val_acc: 0.9658\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.10807423449\n",
      "Test accuracy: 0.9658\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1497 - acc: 0.9600 - val_loss: 0.1005 - val_acc: 0.9678\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.100505218797\n",
      "Test accuracy: 0.9678\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1957 - acc: 0.9500 - val_loss: 0.1074 - val_acc: 0.9661\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.107440221974\n",
      "Test accuracy: 0.9661\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3773 - acc: 0.9000 - val_loss: 0.1128 - val_acc: 0.9641\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.112805217071\n",
      "Test accuracy: 0.9641\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1781 - acc: 0.9500 - val_loss: 0.1042 - val_acc: 0.9665\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.104207418732\n",
      "Test accuracy: 0.9665\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2924 - acc: 0.9000 - val_loss: 0.1008 - val_acc: 0.9686\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.100757235907\n",
      "Test accuracy: 0.9686\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.2083 - acc: 0.9300 - val_loss: 0.1031 - val_acc: 0.9684\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.103080777825\n",
      "Test accuracy: 0.9684\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1276 - acc: 0.9500 - val_loss: 0.1020 - val_acc: 0.9694\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.102019102578\n",
      "Test accuracy: 0.9694\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1011 - acc: 0.9500 - val_loss: 0.0997 - val_acc: 0.9689\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.09965657712\n",
      "Test accuracy: 0.9689\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1558 - acc: 0.9500 - val_loss: 0.1015 - val_acc: 0.9677\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.101537516271\n",
      "Test accuracy: 0.9677\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1157 - acc: 0.9800 - val_loss: 0.0999 - val_acc: 0.9685\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0999154175546\n",
      "Test accuracy: 0.9685\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1765 - acc: 0.9600 - val_loss: 0.0999 - val_acc: 0.9686\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0999304847762\n",
      "Test accuracy: 0.9686\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1265 - acc: 0.9500 - val_loss: 0.1003 - val_acc: 0.9675\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.100279361396\n",
      "Test accuracy: 0.9675\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0853 - acc: 0.9800 - val_loss: 0.1009 - val_acc: 0.9678\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.100903327077\n",
      "Test accuracy: 0.9678\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1356 - acc: 0.9400 - val_loss: 0.1039 - val_acc: 0.9673\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.103866081195\n",
      "Test accuracy: 0.9673\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0939 - acc: 0.9500 - val_loss: 0.1065 - val_acc: 0.9661\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.106461126844\n",
      "Test accuracy: 0.9661\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0500 - acc: 0.9800 - val_loss: 0.1027 - val_acc: 0.9670\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.102729828627\n",
      "Test accuracy: 0.967\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2020 - acc: 0.9300 - val_loss: 0.1045 - val_acc: 0.9673\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.104497837427\n",
      "Test accuracy: 0.9673\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1833 - acc: 0.9400 - val_loss: 0.1057 - val_acc: 0.9666\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.105667801164\n",
      "Test accuracy: 0.9666\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1116 - acc: 0.9700 - val_loss: 0.1053 - val_acc: 0.9670\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.105278978441\n",
      "Test accuracy: 0.967\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1999 - acc: 0.9300 - val_loss: 0.1088 - val_acc: 0.9667\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.108761597439\n",
      "Test accuracy: 0.9667\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3256 - acc: 0.8900 - val_loss: 0.1117 - val_acc: 0.9649\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.111703470371\n",
      "Test accuracy: 0.9649\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2422 - acc: 0.9400 - val_loss: 0.1172 - val_acc: 0.9633\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.117168926567\n",
      "Test accuracy: 0.9633\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1897 - acc: 0.9600 - val_loss: 0.1062 - val_acc: 0.9672\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.106155593844\n",
      "Test accuracy: 0.9672\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1912 - acc: 0.9300 - val_loss: 0.1054 - val_acc: 0.9669\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.10542619956\n",
      "Test accuracy: 0.9669\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0650 - acc: 0.9800 - val_loss: 0.1026 - val_acc: 0.9676\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.102625491527\n",
      "Test accuracy: 0.9676\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1940 - acc: 0.9400 - val_loss: 0.1000 - val_acc: 0.9686\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0999933413818\n",
      "Test accuracy: 0.9686\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1878 - acc: 0.9600 - val_loss: 0.0993 - val_acc: 0.9684\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0992503686715\n",
      "Test accuracy: 0.9684\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1317 - acc: 0.9500 - val_loss: 0.0987 - val_acc: 0.9684\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0986948466793\n",
      "Test accuracy: 0.9684\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1226 - acc: 0.9700 - val_loss: 0.0975 - val_acc: 0.9686\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0975477045417\n",
      "Test accuracy: 0.9686\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1095 - acc: 0.9600 - val_loss: 0.0987 - val_acc: 0.9697\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0986754107554\n",
      "Test accuracy: 0.9697\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1327 - acc: 0.9600 - val_loss: 0.0956 - val_acc: 0.9701\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0955978087092\n",
      "Test accuracy: 0.9701\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2109 - acc: 0.9200 - val_loss: 0.0976 - val_acc: 0.9689\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0976358573887\n",
      "Test accuracy: 0.9689\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1918 - acc: 0.9600 - val_loss: 0.0927 - val_acc: 0.9708\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0927198285777\n",
      "Test accuracy: 0.9708\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1513 - acc: 0.9700 - val_loss: 0.0960 - val_acc: 0.9700\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0959566737294\n",
      "Test accuracy: 0.97\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1559 - acc: 0.9700 - val_loss: 0.0957 - val_acc: 0.9698\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0956917043544\n",
      "Test accuracy: 0.9698\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1448 - acc: 0.9600 - val_loss: 0.0966 - val_acc: 0.9695\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0965651908956\n",
      "Test accuracy: 0.9695\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1178 - acc: 0.9800 - val_loss: 0.0949 - val_acc: 0.9701\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0948775232812\n",
      "Test accuracy: 0.9701\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1268 - acc: 0.9500 - val_loss: 0.0964 - val_acc: 0.9692\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0963868679369\n",
      "Test accuracy: 0.9692\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1025 - acc: 0.9600 - val_loss: 0.0966 - val_acc: 0.9696\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0965824138954\n",
      "Test accuracy: 0.9696\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2009 - acc: 0.9100 - val_loss: 0.0976 - val_acc: 0.9698\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0976276232155\n",
      "Test accuracy: 0.9698\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1649 - acc: 0.9400 - val_loss: 0.0975 - val_acc: 0.9714\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0974605549445\n",
      "Test accuracy: 0.9714\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1369 - acc: 0.9600 - val_loss: 0.0989 - val_acc: 0.9690\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0989437534597\n",
      "Test accuracy: 0.969\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2840 - acc: 0.9200 - val_loss: 0.0983 - val_acc: 0.9696\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0982587942734\n",
      "Test accuracy: 0.9696\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1309 - acc: 0.9600 - val_loss: 0.0997 - val_acc: 0.9694\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0997011770373\n",
      "Test accuracy: 0.9694\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1925 - acc: 0.9100 - val_loss: 0.1092 - val_acc: 0.9672\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.109151230796\n",
      "Test accuracy: 0.9672\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0854 - acc: 0.9800 - val_loss: 0.1044 - val_acc: 0.9680\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.104372757572\n",
      "Test accuracy: 0.968\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1340 - acc: 0.9700 - val_loss: 0.0975 - val_acc: 0.9707\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0974818320084\n",
      "Test accuracy: 0.9707\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1822 - acc: 0.9500 - val_loss: 0.0985 - val_acc: 0.9687\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.098515125343\n",
      "Test accuracy: 0.9687\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1706 - acc: 0.9400 - val_loss: 0.0976 - val_acc: 0.9700\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0976174289687\n",
      "Test accuracy: 0.97\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1390 - acc: 0.9700 - val_loss: 0.0978 - val_acc: 0.9698\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0978312635306\n",
      "Test accuracy: 0.9698\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1864 - acc: 0.9700 - val_loss: 0.0972 - val_acc: 0.9700\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0971987843826\n",
      "Test accuracy: 0.97\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1956 - acc: 0.9500 - val_loss: 0.0935 - val_acc: 0.9710\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0934578987744\n",
      "Test accuracy: 0.971\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1756 - acc: 0.9400 - val_loss: 0.0939 - val_acc: 0.9706\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0939184230514\n",
      "Test accuracy: 0.9706\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1769 - acc: 0.9700 - val_loss: 0.0951 - val_acc: 0.9701\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0951398089139\n",
      "Test accuracy: 0.9701\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2469 - acc: 0.9000 - val_loss: 0.0931 - val_acc: 0.9707\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0931321216904\n",
      "Test accuracy: 0.9707\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1401 - acc: 0.9400 - val_loss: 0.0981 - val_acc: 0.9701\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0981006802138\n",
      "Test accuracy: 0.9701\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1214 - acc: 0.9600 - val_loss: 0.0956 - val_acc: 0.9703\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0955825313464\n",
      "Test accuracy: 0.9703\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1289 - acc: 0.9700 - val_loss: 0.0935 - val_acc: 0.9703\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0935459743604\n",
      "Test accuracy: 0.9703\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1875 - acc: 0.9500 - val_loss: 0.0936 - val_acc: 0.9714\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0936281386156\n",
      "Test accuracy: 0.9714\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1972 - acc: 0.9600 - val_loss: 0.0933 - val_acc: 0.9711\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0932588494468\n",
      "Test accuracy: 0.9711\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1884 - acc: 0.9600 - val_loss: 0.0919 - val_acc: 0.9710\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0919000505675\n",
      "Test accuracy: 0.971\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3042 - acc: 0.9400 - val_loss: 0.0919 - val_acc: 0.9716\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0919438363064\n",
      "Test accuracy: 0.9716\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3142 - acc: 0.8900 - val_loss: 0.0918 - val_acc: 0.9722\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0917749235142\n",
      "Test accuracy: 0.9722\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3233 - acc: 0.9400 - val_loss: 0.0948 - val_acc: 0.9718\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0948359512206\n",
      "Test accuracy: 0.9718\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0848 - acc: 0.9700 - val_loss: 0.0923 - val_acc: 0.9721\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0922876629714\n",
      "Test accuracy: 0.9721\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.0709 - acc: 0.9800 - val_loss: 0.0911 - val_acc: 0.9719\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0910943234958\n",
      "Test accuracy: 0.9719\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2512 - acc: 0.9300 - val_loss: 0.0937 - val_acc: 0.9719\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0936851581611\n",
      "Test accuracy: 0.9719\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2158 - acc: 0.9400 - val_loss: 0.0960 - val_acc: 0.9711\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0960186756928\n",
      "Test accuracy: 0.9711\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.0948 - val_acc: 0.9710\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0948396847557\n",
      "Test accuracy: 0.971\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1316 - acc: 0.9600 - val_loss: 0.0958 - val_acc: 0.9707\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0958400180392\n",
      "Test accuracy: 0.9707\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2087 - acc: 0.9400 - val_loss: 0.0933 - val_acc: 0.9718\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0932618368585\n",
      "Test accuracy: 0.9718\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2151 - acc: 0.9200 - val_loss: 0.0939 - val_acc: 0.9723\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0938828565903\n",
      "Test accuracy: 0.9723\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1081 - acc: 0.9700 - val_loss: 0.0913 - val_acc: 0.9734\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0912975313738\n",
      "Test accuracy: 0.9734\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1123 - acc: 0.9600 - val_loss: 0.0931 - val_acc: 0.9714\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0930710075099\n",
      "Test accuracy: 0.9714\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0824 - acc: 0.9700 - val_loss: 0.0911 - val_acc: 0.9717\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0911349523179\n",
      "Test accuracy: 0.9717\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0640 - acc: 0.9900 - val_loss: 0.0923 - val_acc: 0.9714\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0923449648723\n",
      "Test accuracy: 0.9714\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1664 - acc: 0.9400 - val_loss: 0.0914 - val_acc: 0.9710\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0914010935873\n",
      "Test accuracy: 0.971\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1120 - acc: 0.9700 - val_loss: 0.0945 - val_acc: 0.9698\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.094548741255\n",
      "Test accuracy: 0.9698\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2483 - acc: 0.9200 - val_loss: 0.0916 - val_acc: 0.9717\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0915513320297\n",
      "Test accuracy: 0.9717\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0990 - acc: 0.9600 - val_loss: 0.0910 - val_acc: 0.9717\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0910050987281\n",
      "Test accuracy: 0.9717\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1628 - acc: 0.9700 - val_loss: 0.0901 - val_acc: 0.9720\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0901293279938\n",
      "Test accuracy: 0.972\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2775 - acc: 0.8900 - val_loss: 0.0893 - val_acc: 0.9725\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0892525033072\n",
      "Test accuracy: 0.9725\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1399 - acc: 0.9600 - val_loss: 0.0881 - val_acc: 0.9730\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0881243592661\n",
      "Test accuracy: 0.973\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0802 - acc: 0.9700 - val_loss: 0.0865 - val_acc: 0.9746\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0864730714379\n",
      "Test accuracy: 0.9746\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0888 - acc: 0.9700 - val_loss: 0.0873 - val_acc: 0.9743\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0873366575846\n",
      "Test accuracy: 0.9743\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0899 - acc: 0.9800 - val_loss: 0.0935 - val_acc: 0.9711\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0935204542293\n",
      "Test accuracy: 0.9711\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1550 - acc: 0.9400 - val_loss: 0.0920 - val_acc: 0.9712\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0920354149336\n",
      "Test accuracy: 0.9712\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1252 - acc: 0.9400 - val_loss: 0.0863 - val_acc: 0.9730\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0862866008628\n",
      "Test accuracy: 0.973\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1304 - acc: 0.9600 - val_loss: 0.0917 - val_acc: 0.9710\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0917022867709\n",
      "Test accuracy: 0.971\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1926 - acc: 0.9600 - val_loss: 0.0886 - val_acc: 0.9712\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0885703372077\n",
      "Test accuracy: 0.9712\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1305 - acc: 0.9400 - val_loss: 0.0872 - val_acc: 0.9728\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0872045051653\n",
      "Test accuracy: 0.9728\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1102 - acc: 0.9800 - val_loss: 0.0855 - val_acc: 0.9726\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0854905550998\n",
      "Test accuracy: 0.9726\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1899 - acc: 0.9500 - val_loss: 0.0911 - val_acc: 0.9714\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0910687263479\n",
      "Test accuracy: 0.9714\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1955 - acc: 0.9400 - val_loss: 0.0894 - val_acc: 0.9724\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0894293028176\n",
      "Test accuracy: 0.9724\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3443 - acc: 0.8800 - val_loss: 0.0872 - val_acc: 0.9724\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0871897480402\n",
      "Test accuracy: 0.9724\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1593 - acc: 0.9400 - val_loss: 0.0895 - val_acc: 0.9716\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0895176836161\n",
      "Test accuracy: 0.9716\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1303 - acc: 0.9600 - val_loss: 0.0867 - val_acc: 0.9733\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.086699728302\n",
      "Test accuracy: 0.9733\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0858 - acc: 0.9700 - val_loss: 0.0872 - val_acc: 0.9733\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0871866880589\n",
      "Test accuracy: 0.9733\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1036 - acc: 0.9500 - val_loss: 0.0869 - val_acc: 0.9730\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0869238694569\n",
      "Test accuracy: 0.973\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1186 - acc: 0.9400 - val_loss: 0.0832 - val_acc: 0.9735\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0831986326415\n",
      "Test accuracy: 0.9735\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1973 - acc: 0.9500 - val_loss: 0.0816 - val_acc: 0.9746\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0816455520766\n",
      "Test accuracy: 0.9746\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2511 - acc: 0.9400 - val_loss: 0.0820 - val_acc: 0.9736\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0820082624121\n",
      "Test accuracy: 0.9736\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1741 - acc: 0.9600 - val_loss: 0.0816 - val_acc: 0.9743\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0816314215031\n",
      "Test accuracy: 0.9743\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1628 - acc: 0.9700 - val_loss: 0.0807 - val_acc: 0.9748\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0807105753703\n",
      "Test accuracy: 0.9748\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1384 - acc: 0.9500 - val_loss: 0.0810 - val_acc: 0.9746\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0810232747962\n",
      "Test accuracy: 0.9746\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1263 - acc: 0.9600 - val_loss: 0.0799 - val_acc: 0.9747\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.079915963812\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2949 - acc: 0.9600 - val_loss: 0.0817 - val_acc: 0.9751\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0816999197369\n",
      "Test accuracy: 0.9751\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1417 - acc: 0.9500 - val_loss: 0.0817 - val_acc: 0.9751\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0817095264154\n",
      "Test accuracy: 0.9751\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2634 - acc: 0.9100 - val_loss: 0.0809 - val_acc: 0.9743\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0809402313001\n",
      "Test accuracy: 0.9743\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1013 - acc: 0.9700 - val_loss: 0.0832 - val_acc: 0.9738\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0831943857342\n",
      "Test accuracy: 0.9738\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1375 - acc: 0.9500 - val_loss: 0.0837 - val_acc: 0.9735\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0837056957684\n",
      "Test accuracy: 0.9735\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1374 - acc: 0.9500 - val_loss: 0.0822 - val_acc: 0.9744\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0822045791\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2857 - acc: 0.9200 - val_loss: 0.0819 - val_acc: 0.9737\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0819481547259\n",
      "Test accuracy: 0.9737\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1092 - acc: 0.9600 - val_loss: 0.0822 - val_acc: 0.9742\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0821606983973\n",
      "Test accuracy: 0.9742\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0943 - acc: 0.9700 - val_loss: 0.0814 - val_acc: 0.9734\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0813640676333\n",
      "Test accuracy: 0.9734\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2437 - acc: 0.9100 - val_loss: 0.0824 - val_acc: 0.9739\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0824326668307\n",
      "Test accuracy: 0.9739\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1254 - acc: 0.9400 - val_loss: 0.0814 - val_acc: 0.9744\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0814424280357\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1628 - acc: 0.9300 - val_loss: 0.0821 - val_acc: 0.9737\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0820681877866\n",
      "Test accuracy: 0.9737\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1762 - acc: 0.9500 - val_loss: 0.1004 - val_acc: 0.9674\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.100444787383\n",
      "Test accuracy: 0.9674\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2402 - acc: 0.9400 - val_loss: 0.0807 - val_acc: 0.9747\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0807001784733\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1406 - acc: 0.9600 - val_loss: 0.0832 - val_acc: 0.9749\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0831510821989\n",
      "Test accuracy: 0.9749\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1022 - acc: 0.9800 - val_loss: 0.0801 - val_acc: 0.9749\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0801063165467\n",
      "Test accuracy: 0.9749\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1840 - acc: 0.9400 - val_loss: 0.0814 - val_acc: 0.9743\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0814462878428\n",
      "Test accuracy: 0.9743\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1650 - acc: 0.9500 - val_loss: 0.0812 - val_acc: 0.9749\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.081194100092\n",
      "Test accuracy: 0.9749\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0835 - acc: 0.9700 - val_loss: 0.0854 - val_acc: 0.9742\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.085434146126\n",
      "Test accuracy: 0.9742\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.0925 - acc: 0.9800 - val_loss: 0.0839 - val_acc: 0.9741\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0839492549628\n",
      "Test accuracy: 0.9741\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1730 - acc: 0.9500 - val_loss: 0.0831 - val_acc: 0.9747\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0830655800115\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1272 - acc: 0.9600 - val_loss: 0.0857 - val_acc: 0.9744\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0857114887366\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1833 - acc: 0.9200 - val_loss: 0.0878 - val_acc: 0.9731\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0877558588974\n",
      "Test accuracy: 0.9731\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1387 - acc: 0.9400 - val_loss: 0.0858 - val_acc: 0.9734\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0857768847488\n",
      "Test accuracy: 0.9734\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2098 - acc: 0.9100 - val_loss: 0.0903 - val_acc: 0.9728\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0902581637662\n",
      "Test accuracy: 0.9728\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0946 - acc: 0.9800 - val_loss: 0.0842 - val_acc: 0.9749\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0842191899993\n",
      "Test accuracy: 0.9749\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1616 - acc: 0.9600 - val_loss: 0.0853 - val_acc: 0.9737\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0853477076555\n",
      "Test accuracy: 0.9737\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0733 - acc: 0.9800 - val_loss: 0.0850 - val_acc: 0.9731\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0850110141629\n",
      "Test accuracy: 0.9731\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0580 - acc: 0.9900 - val_loss: 0.0832 - val_acc: 0.9737\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.083177283337\n",
      "Test accuracy: 0.9737\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1459 - acc: 0.9700 - val_loss: 0.0848 - val_acc: 0.9729\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0848238822738\n",
      "Test accuracy: 0.9729\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0304 - acc: 0.9900 - val_loss: 0.0840 - val_acc: 0.9734\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0839823519442\n",
      "Test accuracy: 0.9734\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1052 - acc: 0.9600 - val_loss: 0.0822 - val_acc: 0.9734\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.082217416661\n",
      "Test accuracy: 0.9734\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0950 - acc: 0.9600 - val_loss: 0.0806 - val_acc: 0.9744\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0806408337116\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2472 - acc: 0.9600 - val_loss: 0.0805 - val_acc: 0.9749\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0805337463319\n",
      "Test accuracy: 0.9749\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1719 - acc: 0.9600 - val_loss: 0.0789 - val_acc: 0.9753\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0788746668093\n",
      "Test accuracy: 0.9753\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1791 - acc: 0.9400 - val_loss: 0.0799 - val_acc: 0.9744\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0799465723265\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1571 - acc: 0.9500 - val_loss: 0.0813 - val_acc: 0.9752\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0812647558886\n",
      "Test accuracy: 0.9752\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2412 - acc: 0.9300 - val_loss: 0.0827 - val_acc: 0.9754\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0827258393234\n",
      "Test accuracy: 0.9754\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1201 - acc: 0.9700 - val_loss: 0.0800 - val_acc: 0.9763\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0799925900489\n",
      "Test accuracy: 0.9763\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1179 - acc: 0.9600 - val_loss: 0.0807 - val_acc: 0.9753\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0806942213975\n",
      "Test accuracy: 0.9753\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1257 - acc: 0.9700 - val_loss: 0.0825 - val_acc: 0.9741\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0825485415483\n",
      "Test accuracy: 0.9741\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1661 - acc: 0.9600 - val_loss: 0.0806 - val_acc: 0.9756\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0805894556895\n",
      "Test accuracy: 0.9756\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0836 - acc: 0.9800 - val_loss: 0.0836 - val_acc: 0.9748\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0836156468589\n",
      "Test accuracy: 0.9748\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2086 - acc: 0.9600 - val_loss: 0.0835 - val_acc: 0.97320.946\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.08353643809\n",
      "Test accuracy: 0.9732\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0852 - acc: 0.9900 - val_loss: 0.0835 - val_acc: 0.9744\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0835022359917\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1626 - acc: 0.9400 - val_loss: 0.0847 - val_acc: 0.9741\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0847428926155\n",
      "Test accuracy: 0.9741\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0916 - acc: 0.9600 - val_loss: 0.0830 - val_acc: 0.9739\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0830130426561\n",
      "Test accuracy: 0.9739\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1374 - acc: 0.9500 - val_loss: 0.0802 - val_acc: 0.9741\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0802225132337\n",
      "Test accuracy: 0.9741\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0959 - acc: 0.9800 - val_loss: 0.0842 - val_acc: 0.9727\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0841523807773\n",
      "Test accuracy: 0.9727\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1778 - acc: 0.9400 - val_loss: 0.0828 - val_acc: 0.9744\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0828239779567\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1641 - acc: 0.9500 - val_loss: 0.0813 - val_acc: 0.9747\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.081261167713\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1923 - acc: 0.9600 - val_loss: 0.0807 - val_acc: 0.9752\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0806533878569\n",
      "Test accuracy: 0.9752\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0982 - acc: 0.9900 - val_loss: 0.0810 - val_acc: 0.9745\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0810308050731\n",
      "Test accuracy: 0.9745\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3260 - acc: 0.9400 - val_loss: 0.0833 - val_acc: 0.9747\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0833286515806\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0687 - acc: 0.9900 - val_loss: 0.0841 - val_acc: 0.9744\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0841137751121\n",
      "Test accuracy: 0.9744\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0628 - acc: 0.9700 - val_loss: 0.0816 - val_acc: 0.9748\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0815909006355\n",
      "Test accuracy: 0.9748\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1759 - acc: 0.9600 - val_loss: 0.0829 - val_acc: 0.9741\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0828580637183\n",
      "Test accuracy: 0.9741\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1186 - acc: 0.9800 - val_loss: 0.0788 - val_acc: 0.9749\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0788310440755\n",
      "Test accuracy: 0.9749\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0319 - acc: 0.9900 - val_loss: 0.0787 - val_acc: 0.9746\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0786773360709\n",
      "Test accuracy: 0.9746\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2555 - acc: 0.9300 - val_loss: 0.0789 - val_acc: 0.9750\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0788735847296\n",
      "Test accuracy: 0.975\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2465 - acc: 0.9200 - val_loss: 0.0779 - val_acc: 0.9760\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0778940344334\n",
      "Test accuracy: 0.976\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1524 - acc: 0.9600 - val_loss: 0.0773 - val_acc: 0.9758\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0773457115968\n",
      "Test accuracy: 0.9758\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1653 - acc: 0.9500 - val_loss: 0.0774 - val_acc: 0.9759\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0773707858872\n",
      "Test accuracy: 0.9759\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2171 - acc: 0.9500 - val_loss: 0.0786 - val_acc: 0.9754\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0786195428787\n",
      "Test accuracy: 0.9754\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1515 - acc: 0.9500 - val_loss: 0.0771 - val_acc: 0.9770\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0770602717729\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0914 - acc: 0.9600 - val_loss: 0.0756 - val_acc: 0.9773\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0756096964611\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1430 - acc: 0.9700 - val_loss: 0.0764 - val_acc: 0.9768\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0763637862869\n",
      "Test accuracy: 0.9768\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2765 - acc: 0.9300 - val_loss: 0.0774 - val_acc: 0.9758\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0773957169414\n",
      "Test accuracy: 0.9758\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1504 - acc: 0.9500 - val_loss: 0.0785 - val_acc: 0.9766\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.07852704494\n",
      "Test accuracy: 0.9766\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1544 - acc: 0.9600 - val_loss: 0.0829 - val_acc: 0.9740\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0828981159512\n",
      "Test accuracy: 0.974\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0806 - val_acc: 0.9748\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0805681690319\n",
      "Test accuracy: 0.9748\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1151 - acc: 0.9600 - val_loss: 0.0765 - val_acc: 0.9769\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0765276256951\n",
      "Test accuracy: 0.9769\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1708 - acc: 0.9200 - val_loss: 0.0779 - val_acc: 0.9761\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0778543130524\n",
      "Test accuracy: 0.9761\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0968 - acc: 0.9900 - val_loss: 0.0777 - val_acc: 0.9761\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0776513432877\n",
      "Test accuracy: 0.9761\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2091 - acc: 0.9300 - val_loss: 0.0769 - val_acc: 0.9767\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0768727356486\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1573 - acc: 0.9500 - val_loss: 0.0767 - val_acc: 0.9767\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0766790385894\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1300 - acc: 0.9700 - val_loss: 0.0778 - val_acc: 0.9767\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0778101958852\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0439 - acc: 0.9800 - val_loss: 0.0766 - val_acc: 0.9770\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0766271281388\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0647 - acc: 0.9800 - val_loss: 0.0757 - val_acc: 0.9775\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0756810315678\n",
      "Test accuracy: 0.9775\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.0773 - acc: 0.9800 - val_loss: 0.0758 - val_acc: 0.9765\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0758398127826\n",
      "Test accuracy: 0.9765\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0963 - acc: 0.9800 - val_loss: 0.0751 - val_acc: 0.9766\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0751187968816\n",
      "Test accuracy: 0.9766\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1279 - acc: 0.9500 - val_loss: 0.0759 - val_acc: 0.9762\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0759123166006\n",
      "Test accuracy: 0.9762\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2638 - acc: 0.9200 - val_loss: 0.0779 - val_acc: 0.9761\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0778688029069\n",
      "Test accuracy: 0.9761\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1505 - acc: 0.9600 - val_loss: 0.0806 - val_acc: 0.9750\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0805566429565\n",
      "Test accuracy: 0.975\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0808 - acc: 0.9600 - val_loss: 0.0817 - val_acc: 0.9753\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0816881471938\n",
      "Test accuracy: 0.9753\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1498 - acc: 0.9500 - val_loss: 0.0815 - val_acc: 0.9747\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0814853854606\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1452 - acc: 0.9300 - val_loss: 0.0805 - val_acc: 0.9755\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0805275779746\n",
      "Test accuracy: 0.9755\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0691 - acc: 0.9900 - val_loss: 0.0826 - val_acc: 0.9752\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0825657258612\n",
      "Test accuracy: 0.9752\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1550 - acc: 0.9400 - val_loss: 0.0761 - val_acc: 0.9772\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0761327916133\n",
      "Test accuracy: 0.9772\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1587 - acc: 0.9700 - val_loss: 0.0784 - val_acc: 0.9751\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0784471145303\n",
      "Test accuracy: 0.9751\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0823 - acc: 0.9900 - val_loss: 0.0774 - val_acc: 0.9759\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0774148095452\n",
      "Test accuracy: 0.9759\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1148 - acc: 0.9500 - val_loss: 0.0792 - val_acc: 0.9763\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.079200465399\n",
      "Test accuracy: 0.9763\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1106 - acc: 0.9800 - val_loss: 0.0790 - val_acc: 0.9767\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0790239306867\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0805 - acc: 0.9700 - val_loss: 0.0766 - val_acc: 0.9773\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0765600394734\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1385 - acc: 0.9400 - val_loss: 0.0788 - val_acc: 0.9760\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0788040052355\n",
      "Test accuracy: 0.976\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1233 - acc: 0.9700 - val_loss: 0.0781 - val_acc: 0.9765\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0780922669127\n",
      "Test accuracy: 0.9765\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1118 - acc: 0.9800 - val_loss: 0.0779 - val_acc: 0.9771\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.077883637785\n",
      "Test accuracy: 0.9771\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0673 - acc: 0.9800 - val_loss: 0.0792 - val_acc: 0.9764\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0791896678556\n",
      "Test accuracy: 0.9764\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0873 - acc: 0.9800 - val_loss: 0.0828 - val_acc: 0.9749\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0827757356104\n",
      "Test accuracy: 0.9749\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1129 - acc: 0.9700 - val_loss: 0.0782 - val_acc: 0.9760\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0781706139515\n",
      "Test accuracy: 0.976\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0830 - acc: 0.9700 - val_loss: 0.0780 - val_acc: 0.9759\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0779925777643\n",
      "Test accuracy: 0.9759\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2648 - acc: 0.9600 - val_loss: 0.0774 - val_acc: 0.9756\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0774057578125\n",
      "Test accuracy: 0.9756\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2053 - acc: 0.9600 - val_loss: 0.0802 - val_acc: 0.9743\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0801554980403\n",
      "Test accuracy: 0.9743\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1914 - acc: 0.9400 - val_loss: 0.0766 - val_acc: 0.9752\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0765539824002\n",
      "Test accuracy: 0.9752\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1667 - acc: 0.9700 - val_loss: 0.0771 - val_acc: 0.9753\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.07706003852\n",
      "Test accuracy: 0.9753\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0973 - acc: 0.9800 - val_loss: 0.0768 - val_acc: 0.9750\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0767910115355\n",
      "Test accuracy: 0.975\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0855 - acc: 0.9800 - val_loss: 0.0777 - val_acc: 0.9755\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0776787981188\n",
      "Test accuracy: 0.9755\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1087 - acc: 0.9500 - val_loss: 0.0811 - val_acc: 0.9759\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0810778351826\n",
      "Test accuracy: 0.9759\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1146 - acc: 0.9500 - val_loss: 0.0779 - val_acc: 0.9759\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0778605504762\n",
      "Test accuracy: 0.9759\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1231 - acc: 0.9700 - val_loss: 0.0782 - val_acc: 0.9752\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0782468175559\n",
      "Test accuracy: 0.9752\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0708 - acc: 0.9800 - val_loss: 0.0788 - val_acc: 0.9750\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.078764618509\n",
      "Test accuracy: 0.975\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0562 - acc: 0.9900 - val_loss: 0.0795 - val_acc: 0.9746\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0795312161325\n",
      "Test accuracy: 0.9746\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1563 - acc: 0.9600 - val_loss: 0.0765 - val_acc: 0.9756\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0764718635126\n",
      "Test accuracy: 0.9756\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2867 - acc: 0.9300 - val_loss: 0.0745 - val_acc: 0.9766\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0745096377257\n",
      "Test accuracy: 0.9766\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2117 - acc: 0.9600 - val_loss: 0.0736 - val_acc: 0.9765\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0736143697246\n",
      "Test accuracy: 0.9765\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0673 - acc: 0.9800 - val_loss: 0.0755 - val_acc: 0.9768\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0755497158569\n",
      "Test accuracy: 0.9768\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1064 - acc: 0.9700 - val_loss: 0.0762 - val_acc: 0.9766\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0762204141938\n",
      "Test accuracy: 0.9766\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2256 - acc: 0.9400 - val_loss: 0.0775 - val_acc: 0.9755\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0775000897409\n",
      "Test accuracy: 0.9755\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1645 - acc: 0.9600 - val_loss: 0.0752 - val_acc: 0.9762\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0751513528354\n",
      "Test accuracy: 0.9762\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1264 - acc: 0.9200 - val_loss: 0.0818 - val_acc: 0.9737\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0818067516182\n",
      "Test accuracy: 0.9737\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1135 - acc: 0.9700 - val_loss: 0.0889 - val_acc: 0.9708\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0889315435315\n",
      "Test accuracy: 0.9708\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1609 - acc: 0.9600 - val_loss: 0.0781 - val_acc: 0.9754\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0781249266126\n",
      "Test accuracy: 0.9754\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0505 - acc: 0.9800 - val_loss: 0.0751 - val_acc: 0.9767\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.075072559596\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0827 - acc: 0.9700 - val_loss: 0.0756 - val_acc: 0.9764\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0756488924765\n",
      "Test accuracy: 0.9764\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1406 - acc: 0.9500 - val_loss: 0.0764 - val_acc: 0.9753\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0763816891476\n",
      "Test accuracy: 0.9753\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1261 - acc: 0.9700 - val_loss: 0.0772 - val_acc: 0.9747\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0772350365491\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0842 - acc: 0.9900 - val_loss: 0.0758 - val_acc: 0.9754\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0758052854424\n",
      "Test accuracy: 0.9754\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0696 - acc: 0.9700 - val_loss: 0.0746 - val_acc: 0.9761\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0746489978317\n",
      "Test accuracy: 0.9761\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1923 - acc: 0.9600 - val_loss: 0.0733 - val_acc: 0.9752\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.073317379323\n",
      "Test accuracy: 0.9752\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1114 - acc: 0.9700 - val_loss: 0.0720 - val_acc: 0.9771\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0720021670917\n",
      "Test accuracy: 0.9771\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1095 - acc: 0.9700 - val_loss: 0.0756 - val_acc: 0.9758\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0755870318839\n",
      "Test accuracy: 0.9758\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1435 - acc: 0.9500 - val_loss: 0.0757 - val_acc: 0.9767\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0757435063244\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2252 - acc: 0.9300 - val_loss: 0.0741 - val_acc: 0.9773\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0740857723939\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2583 - acc: 0.9500 - val_loss: 0.0723 - val_acc: 0.9781\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0722505641965\n",
      "Test accuracy: 0.9781\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0820 - acc: 0.9800 - val_loss: 0.0703 - val_acc: 0.9779\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0703364617463\n",
      "Test accuracy: 0.9779\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0476 - acc: 0.9800 - val_loss: 0.0716 - val_acc: 0.9770\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0715688807255\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0658 - acc: 0.9900 - val_loss: 0.0708 - val_acc: 0.9776\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0707590522597\n",
      "Test accuracy: 0.9776\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2017 - acc: 0.9600 - val_loss: 0.0720 - val_acc: 0.9770\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0719673081659\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0889 - acc: 0.9700 - val_loss: 0.0740 - val_acc: 0.9774\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0739711791258\n",
      "Test accuracy: 0.9774\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.2094 - acc: 0.9500 - val_loss: 0.0754 - val_acc: 0.9759\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0754356138486\n",
      "Test accuracy: 0.9759\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1674 - acc: 0.9600 - val_loss: 0.0764 - val_acc: 0.9747\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0764405898839\n",
      "Test accuracy: 0.9747\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0536 - acc: 0.9700 - val_loss: 0.0734 - val_acc: 0.9765\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0734313596068\n",
      "Test accuracy: 0.9765\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0957 - acc: 0.9700 - val_loss: 0.0728 - val_acc: 0.9770\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0727809529074\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2032 - acc: 0.9000 - val_loss: 0.0715 - val_acc: 0.9767\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0715293359463\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2087 - acc: 0.9200 - val_loss: 0.0722 - val_acc: 0.9771\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0721851880543\n",
      "Test accuracy: 0.9771\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1256 - acc: 0.9500 - val_loss: 0.0723 - val_acc: 0.9767\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0723366272198\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1572 - acc: 0.9400 - val_loss: 0.0701 - val_acc: 0.9782\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0701335761679\n",
      "Test accuracy: 0.9782\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1719 - acc: 0.9600 - val_loss: 0.0718 - val_acc: 0.9782\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0718064181231\n",
      "Test accuracy: 0.9782\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1732 - acc: 0.9400 - val_loss: 0.0705 - val_acc: 0.9772\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0704989918497\n",
      "Test accuracy: 0.9772\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2119 - acc: 0.9500 - val_loss: 0.0704 - val_acc: 0.9773\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0703856946761\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1084 - acc: 0.9800 - val_loss: 0.0702 - val_acc: 0.9772\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0701911034262\n",
      "Test accuracy: 0.9772\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2219 - acc: 0.9300 - val_loss: 0.0711 - val_acc: 0.9774\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0711413653158\n",
      "Test accuracy: 0.9774\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2345 - acc: 0.9600 - val_loss: 0.0694 - val_acc: 0.9785\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0693937887291\n",
      "Test accuracy: 0.9785\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1597 - acc: 0.9600 - val_loss: 0.0697 - val_acc: 0.9789\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.06972492597\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1035 - acc: 0.9700 - val_loss: 0.0690 - val_acc: 0.9793\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0689703613117\n",
      "Test accuracy: 0.9793\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0691 - acc: 0.9700 - val_loss: 0.0688 - val_acc: 0.9793\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.068785873124\n",
      "Test accuracy: 0.9793\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1443 - acc: 0.9600 - val_loss: 0.0699 - val_acc: 0.9783\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0698534810413\n",
      "Test accuracy: 0.9783\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1265 - acc: 0.9600 - val_loss: 0.0735 - val_acc: 0.9774\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0734673157028\n",
      "Test accuracy: 0.9774\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0818 - acc: 0.9800 - val_loss: 0.0735 - val_acc: 0.9782\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0735102371478\n",
      "Test accuracy: 0.9782\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1848 - acc: 0.9100 - val_loss: 0.0703 - val_acc: 0.9781\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0702620170043\n",
      "Test accuracy: 0.9781\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0954 - acc: 0.9700 - val_loss: 0.0707 - val_acc: 0.9783\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0706583956153\n",
      "Test accuracy: 0.9783\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2420 - acc: 0.9100 - val_loss: 0.0702 - val_acc: 0.9775\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0702309217246\n",
      "Test accuracy: 0.9775\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1797 - acc: 0.9700 - val_loss: 0.0709 - val_acc: 0.9773\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.070873216271\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1918 - acc: 0.9200 - val_loss: 0.0728 - val_acc: 0.9772\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0728331301661\n",
      "Test accuracy: 0.9772\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1578 - acc: 0.9400 - val_loss: 0.0719 - val_acc: 0.9769\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0719344236789\n",
      "Test accuracy: 0.9769\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1508 - acc: 0.9400 - val_loss: 0.0714 - val_acc: 0.9774\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0713739040348\n",
      "Test accuracy: 0.9774\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0899 - acc: 0.9800 - val_loss: 0.0711 - val_acc: 0.9775\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0711283007666\n",
      "Test accuracy: 0.9775\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0678 - acc: 0.9900 - val_loss: 0.0691 - val_acc: 0.9782\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0691399407081\n",
      "Test accuracy: 0.9782\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1399 - acc: 0.9400 - val_loss: 0.0728 - val_acc: 0.9763\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.072842758631\n",
      "Test accuracy: 0.9763\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1350 - acc: 0.9500 - val_loss: 0.0684 - val_acc: 0.9779\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0683651416549\n",
      "Test accuracy: 0.9779\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0606 - acc: 0.9900 - val_loss: 0.0680 - val_acc: 0.9788\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0680030278957\n",
      "Test accuracy: 0.9788\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0838 - acc: 0.9700 - val_loss: 0.0702 - val_acc: 0.9770\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0701992880893\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1648 - acc: 0.9700 - val_loss: 0.0688 - val_acc: 0.9775\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0688150104922\n",
      "Test accuracy: 0.9775\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0470 - acc: 0.9800 - val_loss: 0.0705 - val_acc: 0.9766\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0704872226641\n",
      "Test accuracy: 0.9766\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0682 - acc: 0.9900 - val_loss: 0.0681 - val_acc: 0.9776\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0680711489535\n",
      "Test accuracy: 0.9776\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0554 - acc: 0.9800 - val_loss: 0.0698 - val_acc: 0.9774\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0698044516946\n",
      "Test accuracy: 0.9774\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0273 - acc: 1.0000 - val_loss: 0.0700 - val_acc: 0.9772\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0699958817412\n",
      "Test accuracy: 0.9772\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0953 - acc: 0.9700 - val_loss: 0.0686 - val_acc: 0.9777\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0686193894811\n",
      "Test accuracy: 0.9777\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1672 - acc: 0.9500 - val_loss: 0.0735 - val_acc: 0.9773\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0735156096875\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2780 - acc: 0.8900 - val_loss: 0.0755 - val_acc: 0.9755\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0755472470138\n",
      "Test accuracy: 0.9755\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1165 - acc: 0.9700 - val_loss: 0.0707 - val_acc: 0.9777\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0706958359726\n",
      "Test accuracy: 0.9777\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2347 - acc: 0.9300 - val_loss: 0.0701 - val_acc: 0.9782\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.070079997266\n",
      "Test accuracy: 0.9782\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0439 - acc: 0.9700 - val_loss: 0.0734 - val_acc: 0.9774\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0733530313132\n",
      "Test accuracy: 0.9774\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2254 - acc: 0.9300 - val_loss: 0.0694 - val_acc: 0.9778\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.069360160697\n",
      "Test accuracy: 0.9778\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.4653 - acc: 0.8500 - val_loss: 0.0730 - val_acc: 0.9767\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0729567846186\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1876 - acc: 0.9300 - val_loss: 0.0698 - val_acc: 0.9770\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0698198720352\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0699 - val_acc: 0.9767\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0698933390466\n",
      "Test accuracy: 0.9767\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1461 - acc: 0.9500 - val_loss: 0.0694 - val_acc: 0.9778\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0693833132695\n",
      "Test accuracy: 0.9778\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0766 - acc: 0.9700 - val_loss: 0.0691 - val_acc: 0.9778\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0690518241884\n",
      "Test accuracy: 0.9778\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0622 - acc: 0.9800 - val_loss: 0.0683 - val_acc: 0.9779\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0683164180052\n",
      "Test accuracy: 0.9779\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0562 - acc: 0.9900 - val_loss: 0.0693 - val_acc: 0.9775\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0693126580129\n",
      "Test accuracy: 0.9775\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2558 - acc: 0.9100 - val_loss: 0.0696 - val_acc: 0.9773\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0695721484719\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2502 - acc: 0.9100 - val_loss: 0.0720 - val_acc: 0.9763\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.07201789714\n",
      "Test accuracy: 0.9763\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1626 - acc: 0.9500 - val_loss: 0.0712 - val_acc: 0.9773\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0712459594685\n",
      "Test accuracy: 0.9773\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1152 - acc: 0.9600 - val_loss: 0.0706 - val_acc: 0.9772\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0705606967064\n",
      "Test accuracy: 0.9772\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1351 - acc: 0.9500 - val_loss: 0.0709 - val_acc: 0.9770\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0708881650461\n",
      "Test accuracy: 0.977\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1332 - acc: 0.9700 - val_loss: 0.0661 - val_acc: 0.9792\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0661488585903\n",
      "Test accuracy: 0.9792\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1271 - acc: 0.9500 - val_loss: 0.0665 - val_acc: 0.9790\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0665133677627\n",
      "Test accuracy: 0.979\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0343 - acc: 1.0000 - val_loss: 0.0660 - val_acc: 0.9786\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0660100540832\n",
      "Test accuracy: 0.9786\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.0272 - acc: 0.9900 - val_loss: 0.0658 - val_acc: 0.9791\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0657725519112\n",
      "Test accuracy: 0.9791\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0511 - acc: 0.9900 - val_loss: 0.0653 - val_acc: 0.9795\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.065319921841\n",
      "Test accuracy: 0.9795\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2230 - acc: 0.9300 - val_loss: 0.0660 - val_acc: 0.9786\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0660347246311\n",
      "Test accuracy: 0.9786\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0807 - acc: 0.9600 - val_loss: 0.0664 - val_acc: 0.9786\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0664173985223\n",
      "Test accuracy: 0.9786\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0711 - acc: 0.9600 - val_loss: 0.0659 - val_acc: 0.9787\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0658507315357\n",
      "Test accuracy: 0.9787\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1194 - acc: 0.9700 - val_loss: 0.0645 - val_acc: 0.9795\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0645354215342\n",
      "Test accuracy: 0.9795\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1381 - acc: 0.9700 - val_loss: 0.0638 - val_acc: 0.9802\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0638016678122\n",
      "Test accuracy: 0.9802\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1487 - acc: 0.9500 - val_loss: 0.0653 - val_acc: 0.9797\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0653354130452\n",
      "Test accuracy: 0.9797\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0427 - acc: 0.9800 - val_loss: 0.0654 - val_acc: 0.9796\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0654258642299\n",
      "Test accuracy: 0.9796\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2151 - acc: 0.9300 - val_loss: 0.0667 - val_acc: 0.9789\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0666960427381\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1294 - acc: 0.9400 - val_loss: 0.0667 - val_acc: 0.9783\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.066671293018\n",
      "Test accuracy: 0.9783\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1554 - acc: 0.9600 - val_loss: 0.0662 - val_acc: 0.9783\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0661925135863\n",
      "Test accuracy: 0.9783\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1803 - acc: 0.9400 - val_loss: 0.0716 - val_acc: 0.9777\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0716498300644\n",
      "Test accuracy: 0.9777\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1280 - acc: 0.9800 - val_loss: 0.0691 - val_acc: 0.9785\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0691416936368\n",
      "Test accuracy: 0.9785\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0603 - acc: 0.9900 - val_loss: 0.0674 - val_acc: 0.9778\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0674236259331\n",
      "Test accuracy: 0.9778\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0321 - acc: 1.0000 - val_loss: 0.0686 - val_acc: 0.9777\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0686189355097\n",
      "Test accuracy: 0.9777\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1363 - acc: 0.9700 - val_loss: 0.0654 - val_acc: 0.9789\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0654317528424\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0904 - acc: 0.9800 - val_loss: 0.0649 - val_acc: 0.9789\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0648829103365\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1482 - acc: 0.9300 - val_loss: 0.0647 - val_acc: 0.9796\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0647272721397\n",
      "Test accuracy: 0.9796\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.3650 - acc: 0.9300 - val_loss: 0.0744 - val_acc: 0.9765\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.074377082629\n",
      "Test accuracy: 0.9765\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0851 - acc: 0.9800 - val_loss: 0.0691 - val_acc: 0.9782\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0690915884717\n",
      "Test accuracy: 0.9782\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1957 - acc: 0.9700 - val_loss: 0.0628 - val_acc: 0.9807\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.062755779132\n",
      "Test accuracy: 0.9807\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0943 - acc: 0.9700 - val_loss: 0.0616 - val_acc: 0.9813\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0616038556925\n",
      "Test accuracy: 0.9813\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0974 - acc: 0.9600 - val_loss: 0.0634 - val_acc: 0.9810\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0633566285443\n",
      "Test accuracy: 0.981\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0845 - acc: 0.9800 - val_loss: 0.0625 - val_acc: 0.9808\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0625017549391\n",
      "Test accuracy: 0.9808\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1352 - acc: 0.9700 - val_loss: 0.0635 - val_acc: 0.9801\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0635253640843\n",
      "Test accuracy: 0.9801\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1828 - acc: 0.9400 - val_loss: 0.0649 - val_acc: 0.9796\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0649307907742\n",
      "Test accuracy: 0.9796\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0409 - acc: 0.9900 - val_loss: 0.0652 - val_acc: 0.9789\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0652356863227\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1938 - acc: 0.9500 - val_loss: 0.0636 - val_acc: 0.9803\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0635979506674\n",
      "Test accuracy: 0.9803\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0816 - acc: 0.9700 - val_loss: 0.0641 - val_acc: 0.9793\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0640685862705\n",
      "Test accuracy: 0.9793\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1400 - acc: 0.9600 - val_loss: 0.0632 - val_acc: 0.9803\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.063197676033\n",
      "Test accuracy: 0.9803\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0397 - acc: 0.9900 - val_loss: 0.0654 - val_acc: 0.9789\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0653993649045\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0823 - acc: 0.9800 - val_loss: 0.0633 - val_acc: 0.9793\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0632766236572\n",
      "Test accuracy: 0.9793\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0894 - acc: 0.9800 - val_loss: 0.0633 - val_acc: 0.9795\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0632964257851\n",
      "Test accuracy: 0.9795\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0366 - acc: 0.9800 - val_loss: 0.0635 - val_acc: 0.9792\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0635210609015\n",
      "Test accuracy: 0.9792\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2206 - acc: 0.9500 - val_loss: 0.0669 - val_acc: 0.9785\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0669487149435\n",
      "Test accuracy: 0.9785\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0652 - val_acc: 0.9792\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0652028885167\n",
      "Test accuracy: 0.9792\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0870 - acc: 0.9700 - val_loss: 0.0660 - val_acc: 0.9787\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0660221326299\n",
      "Test accuracy: 0.9787\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1050 - acc: 0.9700 - val_loss: 0.0641 - val_acc: 0.9794\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0641337923215\n",
      "Test accuracy: 0.9794\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1583 - acc: 0.9600 - val_loss: 0.0662 - val_acc: 0.9786\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0661914965378\n",
      "Test accuracy: 0.9786\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0680 - acc: 0.9800 - val_loss: 0.0664 - val_acc: 0.9790\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.066425300915\n",
      "Test accuracy: 0.979\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0352 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9787\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.06723714508\n",
      "Test accuracy: 0.9787\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0922 - acc: 0.9700 - val_loss: 0.0668 - val_acc: 0.9782\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0668116807533\n",
      "Test accuracy: 0.9782\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1515 - acc: 0.9500 - val_loss: 0.0688 - val_acc: 0.9774\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0687959241983\n",
      "Test accuracy: 0.9774\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0975 - acc: 0.9800 - val_loss: 0.0694 - val_acc: 0.9762\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0694493772891\n",
      "Test accuracy: 0.9762\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1501 - acc: 0.9500 - val_loss: 0.0660 - val_acc: 0.9795\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0659749237901\n",
      "Test accuracy: 0.9795\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0539 - acc: 0.9800 - val_loss: 0.0649 - val_acc: 0.9794\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0649440223206\n",
      "Test accuracy: 0.9794\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1135 - acc: 0.9600 - val_loss: 0.0671 - val_acc: 0.9783\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0671322564147\n",
      "Test accuracy: 0.9783\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1336 - acc: 0.9500 - val_loss: 0.0660 - val_acc: 0.9792\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0659839613175\n",
      "Test accuracy: 0.9792\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0473 - acc: 0.9900 - val_loss: 0.0658 - val_acc: 0.9789\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0657584912669\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0516 - acc: 0.9900 - val_loss: 0.0668 - val_acc: 0.9786\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0668192493482\n",
      "Test accuracy: 0.9786\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0547 - acc: 0.9800 - val_loss: 0.0623 - val_acc: 0.9797\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0622830542159\n",
      "Test accuracy: 0.9797\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1567 - acc: 0.9500 - val_loss: 0.0630 - val_acc: 0.9791\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0630369078366\n",
      "Test accuracy: 0.9791\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0774 - acc: 0.9800 - val_loss: 0.0672 - val_acc: 0.9784\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0672288982397\n",
      "Test accuracy: 0.9784\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2189 - acc: 0.9500 - val_loss: 0.0647 - val_acc: 0.9785\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0646904710579\n",
      "Test accuracy: 0.9785\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0817 - acc: 0.9800 - val_loss: 0.0629 - val_acc: 0.9801\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0629468143312\n",
      "Test accuracy: 0.9801\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0931 - acc: 0.9600 - val_loss: 0.0610 - val_acc: 0.9803\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0609698806135\n",
      "Test accuracy: 0.9803\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0797 - acc: 0.9700 - val_loss: 0.0617 - val_acc: 0.9805\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0616845642416\n",
      "Test accuracy: 0.9805\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.2674 - acc: 0.9600 - val_loss: 0.0610 - val_acc: 0.9808\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0609813692593\n",
      "Test accuracy: 0.9808\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0666 - acc: 0.9900 - val_loss: 0.0615 - val_acc: 0.9800\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0614531147995\n",
      "Test accuracy: 0.98\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 0.1148 - acc: 0.9700 - val_loss: 0.0619 - val_acc: 0.9797\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0618827879766\n",
      "Test accuracy: 0.9797\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0237 - acc: 1.0000 - val_loss: 0.0616 - val_acc: 0.9801\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0616274363064\n",
      "Test accuracy: 0.9801\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0909 - acc: 0.9600 - val_loss: 0.0616 - val_acc: 0.9801\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.06155949228\n",
      "Test accuracy: 0.9801\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0726 - acc: 0.9900 - val_loss: 0.0609 - val_acc: 0.9804\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0609231119025\n",
      "Test accuracy: 0.9804\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0590 - acc: 0.9800 - val_loss: 0.0608 - val_acc: 0.9808\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0608112590631\n",
      "Test accuracy: 0.9808\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0610 - acc: 0.9700 - val_loss: 0.0635 - val_acc: 0.9793\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0634964273151\n",
      "Test accuracy: 0.9793\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1786 - acc: 0.9500 - val_loss: 0.0628 - val_acc: 0.9798\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0627808604801\n",
      "Test accuracy: 0.9798\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1651 - acc: 0.9500 - val_loss: 0.0665 - val_acc: 0.9780\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.066484393153\n",
      "Test accuracy: 0.978\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0344 - acc: 0.9900 - val_loss: 0.0685 - val_acc: 0.9777\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0685302120097\n",
      "Test accuracy: 0.9777\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0643 - acc: 0.9600 - val_loss: 0.0655 - val_acc: 0.9788\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0654630112344\n",
      "Test accuracy: 0.9788\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0684 - acc: 0.9800 - val_loss: 0.0677 - val_acc: 0.9776\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0676743985059\n",
      "Test accuracy: 0.9776\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0619 - acc: 0.9900 - val_loss: 0.0658 - val_acc: 0.9787\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0658093908653\n",
      "Test accuracy: 0.9787\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0663 - val_acc: 0.9785\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0663194645726\n",
      "Test accuracy: 0.9785\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0467 - acc: 0.9800 - val_loss: 0.0626 - val_acc: 0.9797\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0626138364149\n",
      "Test accuracy: 0.9797\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0232 - acc: 0.9900 - val_loss: 0.0616 - val_acc: 0.9796\n",
      " 9952/10000 [============================>.] - ETA: 0sTest loss: 0.0616268418712\n",
      "Test accuracy: 0.9796\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 0.9797\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0620959510578\n",
      "Test accuracy: 0.9797\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9796\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0623725123304\n",
      "Test accuracy: 0.9796\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0175 - acc: 0.9900 - val_loss: 0.0628 - val_acc: 0.9789\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0627822434511\n",
      "Test accuracy: 0.9789\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0585 - acc: 0.9700 - val_loss: 0.0634 - val_acc: 0.9786\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0634320487879\n",
      "Test accuracy: 0.9786\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0634 - val_acc: 0.9786\n",
      " 9856/10000 [============================>.] - ETA: 0sTest loss: 0.0633684915433\n",
      "Test accuracy: 0.9786\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0634 - val_acc: 0.9787\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0634483349724\n",
      "Test accuracy: 0.9787\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9792\n",
      " 9888/10000 [============================>.] - ETA: 0sTest loss: 0.0635490868211\n",
      "Test accuracy: 0.9792\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1015 - acc: 0.9700 - val_loss: 0.0629 - val_acc: 0.9798\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0629209940154\n",
      "Test accuracy: 0.9798\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0550 - acc: 0.9800 - val_loss: 0.0617 - val_acc: 0.9801\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0616820555853\n",
      "Test accuracy: 0.9801\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0407 - acc: 0.9800 - val_loss: 0.0637 - val_acc: 0.9795\n",
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 0.0636777122806\n",
      "Test accuracy: 0.9795\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0646 - val_acc: 0.9798\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0646295781054\n",
      "Test accuracy: 0.9798\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0327 - acc: 0.9900 - val_loss: 0.0673 - val_acc: 0.9799\n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.0672553896067\n",
      "Test accuracy: 0.9799\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.6740 - acc: 0.8800 - val_loss: 0.0633 - val_acc: 0.9797\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0633015799489\n",
      "Test accuracy: 0.9797\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.0093 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9796\n",
      " 9824/10000 [============================>.] - ETA: 0sTest loss: 0.0634918130968\n",
      "Test accuracy: 0.9796\n",
      "Train on 100 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 2s - loss: 0.1871 - acc: 0.9900 - val_loss: 0.0634 - val_acc: 0.9802\n",
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 0.0633838577397\n",
      "Test accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(list(x_train.shape) + [1])\n",
    "x_test = x_test.reshape(list(x_test.shape) + [1])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, len(set(y_train)))\n",
    "y_test = keras.utils.to_categorical(y_test, len(set(y_test)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "chunk_size = 100\n",
    "x_chunks = np.split(x_train, len(x_train) // chunk_size)\n",
    "y_chunks = np.split(y_train, len(y_train) // chunk_size)\n",
    "\n",
    "neuron_list = []\n",
    "acc_list = []\n",
    "\n",
    "for img, cls in zip(x_chunks, y_chunks):\n",
    "    model.fit(img, cls,\n",
    "              batch_size=25,\n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=1)\n",
    "    neuron_list.append(get_neurons(model))\n",
    "    acc_list.append(score[1])\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(x_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb1607546d8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJztvXuM7Nt11/nd/ahH97n3XI0zuo7IKDgYxI1GRHNOIFjB\nEw9GChgpCf8ENYmMByEUQlB0JMCKZGETI0UkCtczCXcUjcAhCmnJKGESotg3QwgPExzDPYTJw2A5\nsfPA8cWO7T6Pruqu7t7zR/equ2rVWvu3q7qqf1XV34+09du/Xa/9qz7nW+u39lprp5wzCCGEtMNW\n2xMghJDbDEWYEEJahCJMCCEtQhEmhJAWoQgTQkiLUIQJIaRFKMKEENIiFGFCCGkRijAhhLTITtsT\nSCm9DsDXA/gUgGG7syGEkIXQA/D7Abycc/690hOXJsIppb8G4G8AeD2A/wzgr+ec/4Pz1K8H8E+W\nNQ9CCGmRbwHwY6UnLEWEU0p/HsD3A/grAD4K4AGAl1NKfyjn/Dnz9E8BwI/+6I/ihRdemHjgwYMH\nePHFF5cxxdbZ5GsDNvv6eG3ry01d38c+9jF867d+K3ClbyWWZQk/APBDOecfAYCU0rcB+LMA/hKA\n7zXPHQLACy+8gHv37k08cPfu3amxTWGTrw3Y7Ovjta0vLVxfo4t14QtzKaVdAPcB/JyM5ctSbf8C\nwJsW/XmEELLOLCM64ksAbAN41Yy/ikv/MCGEkCsYokYIIS2yDJ/w5wCcA3jejD8P4DPRix48eIC7\nd+9OjH35l3/5wie3KhwcHLQ9haWyydfHa1tflnF9h4eHODw8nBg7Ojqqfn1axs4aKaWPAPjFnPN3\nXp0nAL8F4P/MOX+fee49AK+88sorG70gQAi5PTx8+BD3798HgPs554el5y4rOuLvA/jhlNIreC1E\nbQ/ADy/p8wghZC1ZigjnnD+QUvoSAN+NSzfELwH4+pzzZ5fxeYQQsq4sLWMu5/wSgJeW9f6EELIJ\nMDqCEEJahCJMCCEtQhEmhJAWoQgTQkiLUIQJIaRFKMKEENIiFGFCCGkRijAhhLQIRZgQQlqEIkwI\nIS1CESaEkBahCBNCSItQhAkhpEUowoQQ0iIUYUIIaRGKMCGEtAhFmBBCWoQiTAghLUIRJoSQFqEI\nE0JIi1CECSGkRSjChBDSIhRhQghpEYowIYS0CEWYEEJahCJMCCEtQhEmhJAWoQgTQkiLUIQJIaRF\nKMKEENIiFGFCCGkRijAhhLQIRZgQQlqEIkwIIS1CESaEkBahCBNCSItQhAkhpEUowoQQ0iI7bU+A\nkJwzUkrIObc9lZnQ85W+Nzbve9aMp5TGR923Rzu/2mPTY7Piza007+h10dg6QhEmrbNoAb4JMc85\nI+eMi4uLcT9qTXOyj9UKZEoJW1tb41Y6l3leXFxM9KOxUl+fN2FF086r1NfPt319vu5QhEnrXEc0\nm16rH7di33Reeo4Il9fs403ztVanJ+De+Pb2Nra3t7G1tTXu2yZcXFzg/Px8fGzqS7PndtwSiaIW\nWDvHaEw/X59LA7ARQkwRJmtJk5jVvKbpPHqOiGyNqIlQRW6KkgA3Wdo7OzvFpi1mmc/Z2dn4KM07\nr236MzTe+dbWVuOcpUU/KvrHZd3FV6AIk7WjJJ435YoQi1CLmAiTHtO37TU+ZM9V4B1zztjd3R23\nTqcz7stzRPj0j8bZ2RlGo9G42XMZ85oV67OzM1eEo/7Ozs7EnKP57+7uuqJsf1jk2tYdijBZK2os\nSu98kehbci1sVqBGoxHOz88n5tN0rHVx5JzR6XTQ7XbR6XRwdnaGbrc7JcBifdq5np6ehq0k0vZc\nPksoLaCJ2Oom87fnWpDPz8+xu7s7JcAXFxcTLpd1ZeEinFJ6N4B3m+H/knP+ykV/Frld1N7S27Fl\nzMOzLCOr0vp2vXlq90PJH6vHer0eRqMRut3ueFzeT3ypWsC1AJ+cnIRNi3GpPxqNcHFx0RjdoEW4\n1+uh2+2i2+26ffkB63Q64x+OTqfjLkZub2/TEi7wKwDeCkB+Ds+W9DnkFlDjfijd8i9jPloURTis\nNSlHeU0kxrpv/cneUfoi8uKb1Raw3L6Le8L+aIgQDwYDDIfDiSZCrI/e2Onp6ZQ7onTsdDro9Xro\n9/tTx9PTU/T7/Qm3jrb65T1SSmO/8CYIMLA8ET7LOX92Se9NbhGl/2hNt/bLnJMIpneLr4VKbtn1\n66Jz6+bwmnWDRBbwzs7OhEDL67QAi+gOBgMcHx9jMBiMRVkeFwGOxqxASt8b63Q62Nvbw97eHvr9\n/rgvP1Z6sS8SYHGv2MfXmWWJ8B9MKf03AEMA/x7Ad+Wcf3tJn0U2lFK0Qu1xWfOyVrB3mz8cDsci\nbJu8j21WYHXzxuxturaAO53OlDtCz1UL8PHxMZ4+fToWYy3Q2kq259YdUep3u13s7+9jf38fe3t7\n4x8rLcA6mkT/DfX12R+edWcZIvwRAO8A8F8BfCmA9wD4Nyml/znn/HQJn0duKZFfVY8t63O9W3zP\nyjw5OWkUXt1sFIKNtrAWsOAJsI7O0EKufyxEhJ8+fYonT55MCLFu3thgMBiLsBVc77zX602IuAiw\nvR4vwUOuzUadbIIQL1yEc84vq9NfSSl9FMBvAvhmAO9f9OeRSW7qH2WNm6BmvCaqwfp5a0VtWZyf\nnxctRCs0TcJrRbgkvtoa9pIcdBhYp9OZiHbQ4ivz1CIsQiyCq4XX6x8fH4/90JHw6nZ2djblVtA/\nGlqUPatfuyA2RYCBGwhRyzkfpZQ+DuCNpec9ePAAd+/enRg7ODjAwcHBMqd3a6j5BzvLc2qs0CbB\ntK+zc/D61l8YhXEt8z/p+fm56yf1jpE7IvpOapMkLi4ucHJyMg7n0vG0Or1XPuP4+BhPnjwZW7ty\nlCYuCe2O0O6CyP98cXExkXJsU4r1mA1Pa2o2Zlhnzlnhb5PDw0McHh5OjB0dHVW/fukinFK6g0sB\n/pHS81588UXcu3dv2dO5dUSi5p3XvFdTk+c11SDwLJroqPtNkQPaTbBMEfaiBmxfFua87y76Pr1Q\ntKgvn1USKUELrifA1g1hQ9U8Eba1Krz6FXpsHhH2suj0+64CnrH48OFD3L9/v+r1y4gT/j4A/xyX\nLojfB+DvABgBOCy9jiyekpjN606w4lkS21KLrNYmC9um30aLVjo+d9GIADY1ndCgryW6Nvsj1WTx\nn5ycYHd31623oK1gsYS18NqmLWEvXtgunAGTEQtRnQd9fh1L2BPgVRHh67IMS/jLAPwYgNcB+CyA\nDwP44znn31vCZ5GAJuuyRozt+0WugKhfE/MaWcRR30udjZoWjEUiMbq62Uwzm6wxq/vGfi/2MQBj\nS1gLlHVDyPMHg0FReLUrQix4fU2RJQxgQmy9ojzSn1V8RYA9Id4UAQaWszBHJ+6KUCNqpddZStlb\n3mOz+Df13KLbdQBVGWrSX5YI55wnPkuHfnkpzPo7rTnWPldu0z3r0P5o2jA0e9SLbaenp+51lGJ3\no0VCfb5IS3iTrGHWjthwIlGTc/286PVCyf/qZXVF4VZeqFWTr1SajnONUmn1LfSyvtOmUDLtHim5\ng2r+BpHQeNavvI923Ug0hxVd7/z4+LgYoRH5hK3wesdZRdhbdIyueZ2hCG8gWtDsWEmI9XO9Mc/K\n9axeSQyIitp4boOS8OrmZaZF/WWKcHTt3vfifa9RP0p28MaiRSrvb2VdDp4rQh7Tgmt/XOXvoKMT\nPCH2WpPYNkVG0CdM1o4aUbPP995DKImOXRirrcJlRbhJkJuKz+i02mW6I2pcMtKi79f7vq24ReeR\nGNm5yQ+dje8t9UVsbSSLvWOJXBFaeLVfdxEhapsoxBThW0JJhGvF2FpXpaQCb8HKugu077YUdaHP\nbUaa1+TxZVrCdo6lH5LoO/XGvR0kvLCvWQR4NBphOBwWkzD0mHY5lHzUXmSEdj/YBTYKsA9FeIPx\nhMAKm31+6dxbeIsiE6zPttRqxEz6OttLZ33pczkuU4Sbjk3iG6GFRkcW2HCv7e1tV4Dt30V+6LyU\n4yglWYusd5S+tso9S9gr3D5Li3bbkM/cFCjCa8As/5G1aDXFmdbeKku/5Ne1ft+S6Gr/rYiwd8vr\njUm9g5q2LBFeJl5UwcXFBXZ2XvuvmtLkPndaeCNLsVS43fqyrc/XNv3+YuHaHTK8sV6vN25SR1iK\nuOti7npBLtp3bpUy5q4LRXgNiRZ1gGm/rfbVen37Pt57WxEuRT3IeVP0gu7X3tZLkoSXUuut3q8b\n+vbe86naYyR2nhDamg36Nbqw+nA4nJhLk4+6tGWRdkPs7u6i3+9jf38fd+7cwf7+Pvr9/rimsHZD\neD9EnvhuggADFOG1ovY22Iuf9RbGSokE3tET8Ujko8/3fMJNC3L6MfFvljK69DWtG9q3GkUNeEkN\nkQBK3xb40YtlIsCyoAlMi3DUr/mhkH6v1xuXsZSjtow9CzjyhW8SFOE1I/Lxyrlkc3m7IEQFx5sW\nYaQ/SwJGTTKDFx3R1Oy1bZIlDLwWeibCpW/bbWsSQH30Fsy63a7rHpJ5NBXkseUzrQ/XjskuGrqo\nu1jCJRG2ljAX5kgrNImTPEcs0toogiZRtxapjYGNxjxfcbSQV3ttYuVHG1J6YWHrhBU2baVKE9Hq\n9XrV28dbYRRL2otYOT09LVq/9qjF3dsdWTe5Fn0N/X5//KNiIyKiKJFNEmCAIryWlG7ZxQq1Oyd4\nMaGyPU2T+HqLfV58rA3wb/JHS7/2hwDAlKvFZqdtkjtCBDOyILXQRVlq0t/d3cVoNMLu7m6Y+i19\nmUckvPropShH85Fr0Rt76kU6awl7IXr0CZNWKYmvFmFtCZeqZ0lR7trmRVpEURdRRp13Hgmv9PVY\nKSxund0RNhNOuwy0COvtgZpqNeh2dna5g3EpqkUv1kYi7MUtR59pm1jgNiKiJMKRK8QLm1tXKMJr\ngmcZRuLnWcJPnjzB48ePx8fHjx/j6dOnVaJbEv3SscZitokBTUfPyo5qG6wLWlAid4QIsEQW3Llz\nJ6xW5o2V/Pf2u4wEN+qXKqdZa9lbXPRC03Z2doqhcfp7W3cowmtErZtALGERYSnk/fjxYxwdHeHR\no0d49OgRHj9+3Ci8NYtmtWId9eXa9HVGY5Hrw4r6ugixFRK7MGfdEfv7+3jmmWfwzDPPTC1a2aYf\na/redL9k9UbZe95n2jGbxFHqS1KGZ/lukisCoAivHZ7wWhHWEQQiwo8fP8ajR49wdHSEL37xi/ji\nF7+IR48ehWIa9fUcIvfBPE1fn71ee+4ld9jjOuAJsGcJd7vdsQjfuXMHzzzzDJ599tmZrNWm70v3\nSyJ8nXHPh+0t4tmawZ71u0lCTBFeEyKhswLsWcLijhAR/sIXvoAvfOELODo6qvpP6bkMavq1R+89\nZv0evPNVJhIQuzCnfcLiD9YiXLpl12NNP5q6P4vlWwpf8/zJ1kVRcmNE39emiK9AEV4zIotTC6ZO\nHfaE+NGjR2NrOHIRrKt1OQ/X/U8963fjfV6Uiea5I+7cuYNnn3126nXeLfs811YSXU+Ea1ut1ayt\n4NsARXjNiP6jibXTdFtqG4AJP6D05bZ03cU3uqUt3dZGAqDHPXdKdB6Jrj33Fru80pCygFXylS5C\nhGv8wZHg2jnYf4+e1X5boQivCfZ2LLI0mm4fPREWRIClrbsAC/Y/u3cOxLe8tl/ym9sFR03pltqz\nFkulITudzlw/MLN8X9GPuR3Tn1USZM9tYZ9zG6EIrxGR8ApiCc8iyPq1pf9I6yjI0a1wZNHJa5qO\n0cKohHeJAFshLlnc3lwjS1hEWL8umus8whYJpvdvy869yTJvEuDbKMQU4TUmut0rLaB4IiziLf1N\nuj30rEtvkajGmtTfiZd8klIaHwHfHVHqWwHW1rAVYC3CNcdZv7PojiG6g6i1ymkJT0MRXjNKFocn\nuk3iLAJsLWF573VHfzelbC7P+tevtz9MuhaGFRVJxRb3jryPnpM3FlnCUaH0mve+jgiXWnT30GSV\nl9pthSK8RqSUpixV/Z/CBtrXNC88yRPkdSQSNa+2wSyLTDnncSrw1tYWzs7OJp4bCXBJIO3freSK\nsO4I/R7R2DzfW42A1lxfk2W87v/OrgtFeE3QAqxvcbUAN7keIhHWAfqb6I6IhFiLW9OttxXh0Wg0\nlVQAvCbA8nklMbZjtXPV7gj7ftH7z/qd6TlF/dofAM/it33v/LZAEV4jrADrvhXiWjG2oVabZp1E\noubV2y2JsD63oYCCCPD5+fnE82UeTUdvrnbO1hK211ozVvu9leZp37v2h6DmruC2QRFeM0r/8Esr\n/00ibMPT9Ges638Qz7qMbu9FhGuiAvR3BUynkkvBnJJwNYlw6UdD4oT166Prv+73Z9+nycquGZvl\n/W4DFOE1RltmNuW01iWhb5+tEG/Cfwotbp5lqSt32e/NE2D7PQGTIixWsLWkZS56Tt6Y/bs1+YSj\na170dzjv5zQ9ZxP+jV0XivAaUfMPtkmAba6+XozTorwJQhwJsLcNuyfCkSjr70f/AHq7Hm9tTSaC\nlIQ4ckVEYWpkM6AIrwnyH16fA6/VLdD/oUulEGVbo9FoNK4zUdtm8W/WFAS6Tk2KmjnYPdrsVuv6\nXEK+SuKrRVg2xez3++O+12SniprCR9vb28WdJmw8M9kMKMJriBZkLcbW8rO76UolLtmdOOc8tQln\nqW9dFd6ttDS7g4aX3ABgfJz1+mta0x5tekyLcNPx4uKiaiNV2bct+jGy/a2tLXcLeNkt2VsMJOsP\nRXjNsALsRUvo21e7O4MIsKTUlvYas+elxSo75om4nMt8vfoKNdcfzcEebSlI3WSfNmlSi6Hp2uQ7\n17sTe01vnlkqQG+LqUeWsK6zSzYLivCaogXYixu2lnC/35/alTilNCG2XtPbyjf5m/Vj+vV2J1/g\ntZ2h5732mkVHexewt7fntv39/alY4aYQNb3bs7djsRXhaCshfQ6guAW8DXsjmwFFeA3xBNi6I7RP\nuNvtTgiwWNJWLPVR+p1OZzymF/aa+tovurOzg5OTkwkBlogM6+uuvX4rxN6+ZnJL722UaZtYwlZ8\nvTFJ1qht0V2BHQMwIcCeT5juiM2DIrxGRK4Im7ThLczZTTDFbeHdQksTAZbt0r2VezumRXgwGEz4\nM2WuYgHOIybWDx1FE4gI60U4Wxhdt2632yi+VoS1iEYuHV1gv3SnIb5j8VPLYqF2R9AS3kwowmtG\njU9YC7Fsc679wNpatr5MvcgkAry7u4vT09OJegu2DoMdGwwGUxs2agEWUZ/3O/DCuWxNCM8dIQIs\nm2VK80Q4atGCptfsD5vX5Du/uLhAv98vbgFPS3jzoAivIVaIgenoCLGExd+oXRD6cbuiL9avHtPW\nWNNGjdoKjyxgEfTriLAVYm9uNSL87LPP4tlnn0Wv15uwMmtFWPt1vbHRaFQMYZPvdzgc4vz8fGwJ\ne9ERXJjbTCjCa4oWX30ubgZZmJNFOHnMLtppMdACvLu7O/bnes0TZpteK4JhLWARFS1qs1x35BO2\nSQ3Rwpzep+3u3bu4e/cuer3e1Pvbc22B2gW2aLHt7OwMw+FwvNffycnJ+Hw4HE5YuqPRqBgdQXfE\nZkIRXnPsf0htCdtFOGshykag1jLTFpgtdGPHov729jaA1xbh9K35dcOtagRYJ2poERYfsFjBd+/e\nxXPPPedawnK0fflR0SFmXhz0xcUFRqMRBoPBVPNcDaPRqBgdwYW5zYQivEZYq9GzIrUIayvZRkzI\nYpBYw8PhcEJ8tfDaKl5WpL2mU6BFgLXAL8Idoa3+qMaCZwlrERYh7vf7rgDbo3znUdyvHT87O8Px\n8fG4lULPTk9PJxbmKMK3A4rwGqD/03mWr8aWoxRLUWdmaeHW4/o9ReC0+8JavSWLWARFp/b2+/2p\nbLLT09Oxz9prck221KaNANFhXdLf398fJ2XYFGVP2GpEWH8/8v2W/m4556kfMC9SZWtrC6enp9jf\n35+yhu0PF0V4s6AIbxgln6knwvo8CvsSF0bkD9bn0hcB1jUWbDicDufyrEltccq5jf7QIqzrQfR6\nPdy5c2dKhHXBnhoBnuX79oTZfodWgOU9JLlErHU9Z89qJpsDRXhD8SIIPGvXK+buVe8ajUbhIp2N\nkJDQNy3EXlqvFuFSnQlgstCNjvDwoiB0nQzPsoxEzRO3kuDpEEH9fUsiisxb/0DpSBV5D/muz87O\nJlKpJUqCIrzZUIQ3EM8yyzmPF8v0c+zz7UKX9iF7sbjRUXzAvV7PFV8dYysuCRtjqwVYl4+MLGFb\nF8JzR3hFcWzEgSfKnhvIJstoAZajfO+yUKojVaxP++zsbMInbKMkKMKbCUV4Q9Hia8ftbXO0yCWZ\nciKKUWaa1yTcyhNgK8ZiaeskDokW0CnOsltFJMJeerLc3s9qCdcInRZgwQowgLFP3d512Os4Pz8f\nz7EpTI1sDhThDUSLil040gKgRU1bwCKM1jr1UpWjvhVhr56CuB3EfWErtdkoBD3/SIR1LLC2hCUT\nrSb2tmnxs+kxEWAtwvrcJplIlMr5+flEoXlbdJ6W8GYyswinlN4M4G8CuA/gSwF8U875p8xzvhvA\nXwbwHIB/B+Cv5pw/cf3pkloiAdYLXVGsbVTxyyvWE52Lxdvr9dxiNvq9tQWtBUbH3GqhbLKEdWGe\nyB2xKJ+wN2ajOSIfsLggJLX84uJiKiZbR1Z43xFZf+axhPcB/BKAfwjgJ+yDKaV3AvgOAG8H8CkA\nfxfAyymlF3LOp/NPldRiBUvGtO9S3+Zvb2+PLVK7KKaPNuJC1zKwY2IJl6qJWXGXOXvbBVnr3oqw\nLtAuscD7+/tThdxnKQ9Zsyinn2fH5DrEF6/9xvq71qnl2q/u+dppCW8eM4twzvlDAD4EAMn/1/Cd\nAN6bc/7pq+e8HcCrAL4JwAfmnyqZBbvopndv0JaaxBDXNC1YWny9Y1TO0VrBdtNMAOPPk+eKSMv1\naMs9WpgTS1jvrFHaMsh+b7N8x9G4Df+zMdteSJ53d2HvNCjCm8VCfcIppTcAeD2An5OxnPOjlNIv\nAngTKMI3grXMtIVmj03b7ugxG3VRarasoyfCNlrAWsASkWFdBiJokU9YuyPsQlcUdzuvsEX+Y1te\n1H6XUfN+6LxzsjksemHu9QAyLi1fzatXj5EbouTn1JSy0+y5FgDbt2NemUcbA6xFGJi2gK21WusT\nFnfE3t7e1ALXsuJuaxfzSsWKbKRF03uRzYDRERvGLP9h9X96azV7lrMnutHR8x3bRUDdIj+zdz3a\nWheftVe/d2dnZ2o+8nleunb0fdqFtetAQSWWRYvwZwAkAM9j0hp+HsB/Kr3wwYMHuHv37sTYwcEB\nDg4OFjxFUoMV5UhwZ3m/pmZvv+1rgelFOx17rMtEHh8fI6U05U7RYqwL3UdYAY6sVXJ7OTw8xOHh\n4cTY0dFR9esXKsI550+mlD4D4K0A/j8ASCk9C+BrAPyD0mtffPFF3Lt3b5HTIZV4ghsd5fn6tXbM\nvrd1V2jRtdav1yzWAtYiLNsqyZ5xdmFMuzJqrWGBAkw8PGPx4cOHuH//ftXr54kT3gfwRlxavADw\nFSmlrwLw+ZzzbwN4H4B3pZQ+gcsQtfcC+B0APznrZ5H2mEWAS+8hx3mah3ZFWBeELclp56oFWBfS\nqRVh+XwKMVkk81jCXw3g53G5AJcBfP/V+D8G8Jdyzt+bUtoD8EO4TNb4twD+DGOE1wNrPdYsFpUs\n42jxzsYXewLsibEIsI4htiIsC2820kAv5tl992b5fghZJPPECf9rAMVq3Dnn9wB4z3xTIm3gia/t\ny7l9Xc17N/mAbfiV7Wu0EFsR1pEP+r11PQwtwLSESdswOoK46IWwJks4GosW2EqLcZ41bOdTsoR1\naU2bji1lL704ZULagiJMxmih88ZKi29N7+styHmuiZIFLGifsAiwlwWX0uSmp1IVbjQaTaQKz2IJ\nE7JoKMJkipIYR+el9yq5I2osYf1ZXnywCLGOOQYwlVUndZFpCZNVgiJMQq7j+6xxRdSIscUr7iMi\nrGtMAK/V8RUBloJCElUxj0+YkEVDEb7F2EW3ZX2GtWprLeHoR8CmN3s1JqQOg+y4rOsa6xRqCjBp\nG4rwLWeZK/02NEzXxu12u1PWrG6np6fjmhDS73Q6ADC1w4RYxlJxTcZ3d3cnNhu1O3rYmhb2+4j6\nhCwSijBZCtrStcV27IKYjnbw9prTWx8BmApDAyaF2BPhaJ87ed+dnR3XYpdzfV2ELBKKMFkKTQkS\n2g1gi5t7IizWcc55qgKaDlvT5zs7O+j3+8WdnrUYR2Fz+poIWTQUYbIUPEu40+lMFTpPKU1VQvME\nWIeW6RhgawkDr1nWOzs7U5awt9Go/DjInHWxHznq6yJkkVCEyVLQcbperQYt0jb5whNgEVG935y2\nVHURehFPEeEmAdY7eOi994DJ3ZPFyqYQk0VCESZLQbsj9Jbvgq3t6/l/PRE+Ozsbv4d+P1uMHrgM\nUfMs4cgdYUPc9Fz1jwchi4QiTJaGiNrOzo6715pETJREWMSz1+uN+972S95RRFhSmqOFOXFHeMXs\nZc4MZSPLgiJMloIWW0+Apaj67u7uhAhLs4tpp6en6Ha7SClN7VBsEzikbW9vuwIcCbHsiqyvQQSY\nSR1kWVCEyVLQPmE539raGoujWMDiK/ZcD7ovYgpgIgxNxNdb3NOWcE10hAithKvZRhEmy4AiTJaC\njSqQBS7xAWu3gd6mSFuqVoB7vd74vYHLxTgvTlhev7W1VfQJW3eEnrsOV4vcFIQsAoowWQo2xEvf\n0tudnG3WnBZevW3RycnJlM9XFupsQZ+Tk5OxCJcsYS3Eeu46fE5+TCjAZBlQhMnS0Bln0Q7OAMap\nzJKmLNvXe4KpxV3eQ3zKWiybdt8YDocYDAY4Pj5Gr9cbh7TpJpl5tmkXi3e9Nf3aJBAdFudtN0XW\nH4owuTEiQbGZdSLCtuwkgPHzbMacFlxdzlIeEyGXjUCPj4/R6XTGdSgkdVmENzraXTui4kTeWE2z\neD9cFOCc+amYAAAgAElEQVTNgiJMbhRPiMW61fUlut3uOENOpzmLYGtr2LoiPIEWX7HejVkL6mg0\nGlvkutCQd263TrI+ZO/c7ipt+7MIK4V4s6AIkxtBRxfYW2qb1CG1f5sEONprTqcz67KXp6en441A\ntZjmnCdEuKnJHDxR9c51+rbu55zHx1mElQK8WVCEyVLxxNfrW3eEFmAdX6wjLqzAnp6eTrgqgEl3\nhN6HTgvwxcXFeIsk+REo9cWFYYU1GtP73Elffz+ziiot4c2CIkxuFCu+wGsWrohwZAGL+HkWsN5p\nWVuk1h0xHA4nRFre5+TkZCyyUrtYC68+ajG1G4p6fXm+VHbzihh56dLkdkARJkvHJjroyAY51z5h\nWYSzFnKn0xmHjen4YsmmGw6HE5awDYHTrgo73uv1JgS41PQCnW12XIoXWfHV1y1zJbcTijC5Eazw\n6jFtMXoWohZhAG7Y2WAwmCr2bpNBrC9ZW9HD4XBCaLvd7pT4ypj+HC3I0Zj+YbHXZmsrk9sHRZjc\nKJEYi2AJWpjPzs7Q6XTGCRVagIfDoSuO2hIWwbXRFPo9rNBKvLIeOzk5Gfe9+GEvxtjbx06LsLgp\nKMS3F4owaQW9sKSjBXTNCe0j1pXObOLF8fExut3ulAjL86UGMXAp/npLIy2aIrxWgPW57HfnxRHr\n/tnZ2XjMlvCUZmssk9sJRZi0jnY7aOtQREq3nPOUAPd6vSlL2NaUsJl1euFMPrvX600JsW3ify6F\nsIkA6yQTbf3qa6QVTCjCZCnMEkKlowOsWNk6wb1eL2z9fn/c9vb2JtKLdSiYrbwGYOyPtpuOenvf\nRTHFOpxNmhSi1z8oUXSEDVezKc42DXxZfw9ys1CEycrgZdPp7YWkaI9eqOv1etjb28P+/v64WI+I\n3nA4nCoW5DVgMutOh7jJa7U1LpZulN5s+7qIUL/fn6jcpl0SUhXOazYTzxNVCu16QhEmK4XNppMx\nYHKnDl1jot/vY39/f8rqHA6HExZv6WgL/ljL2RPgpkU56Ucbi1qfsP7hqcnGs9+Pjb+2j5PVhCJM\nWqdJRHSkg2cJ9/t9nJycTNSaAIDhcOhu6KmbFkEbRQFgSqDtgl5TmJoW4ZIAy/Xb1Gbv6H1H+ru0\n3yMz7FYbijBZGWqEQsRIIhd0yUsdDpZSwmAwcAvE62w5G7YmFrAWZr39kk7GiBI29GO7u7tTPwSR\nbxiA+17ihtHfUykVnIK7XlCEycoR+TutJSwibMtdyi27ZNFJQXgvXdkmdQCT7gddHtO2pnRlmast\nyelFRdjQPG1Re3HGNs3Z86dTkNcDijBZGaLbZ23t2Qw6W21NJ0FIAsZgMJiqrCauBWv1yudrES4V\n54nGdLKJdj9Is9ct/l4JbZNNUK0Al7LsIuGlEK82FGGyEtTcVjdVWwMw8bitegZM70WnhVmLcUpp\nbCnbur+zLJ55IhzVj5AEDklzjmpNiIvCC1nzfshoEa82FGGyUnjCq2+vRdg6nc6UC8IKcKfTKZa1\ntLUkAEyJW6lvw8a8cxHhyP2gXRfb29vugp2X5OG5KOQave+RrC4UYbIyNFluVmitD9j6ind3d6cS\nNESAvUI/mqZkCE+UvfOdnZ1QNK0Aa4H1BFhcEaUsO4rv+kERJitHyacpImwFWO/KIVsjSUEgLcBS\nN9imN9t43VKSR+1c5UdD+6vtnPUCXqnWsDzfs6pn/T7JakERJq0zi0hoa9hai9pKHo1GADCRaiwi\nfHJygl6vN1GBzWau6QU07SuusZr1ufw4yLy04NpwN/EJR4V9rFVsfdXWZ62/W4r26kIRJmuFtSa1\nlWitVZtVJxl13q4dg8Fgok6EV0NCZ9iV0qC9lGivfrHdikm2WZKdpnWWnY011lEb3nZKOqvO27WD\nQrs6UITJ2mEXvzwBTimNF+dE1HSihDxHLFERYS+rzo5bq1lbz7oPvGZFa3+0t82SvE4e1yKs44x1\nixJGJLlDFzCy35+1sEl7UITJ2qEFWCImRIj149o/rOsR63hicRMMBoMJwROr2YqhFXOv/oQgfZ30\nIZawJ8Dn5+cTKc421dmmPEe1K6zAzhLORm4eijBZO7QASxadfkza+fk5ut3uhADLc2w423A4nEht\n9ppYspIiLU37cgUtbFqEZZ87T4A90ffqTcjRls20i3YiwLMkdpCbhyJM1govVjeKvxU/rI5O0AKs\n60/o9OZSE6tbuyds2UubfacTRGSvO3lM+4q14EduES3G4m6RxzqdjhvaVgpnY4pz+8wswimlNwP4\nmwDuA/hSAN+Uc/4p9fj7AfxF87IP5Zzfdp2JEiLYCAA9pmsPi/jpRTgbT9ztdtHv9zEcDifaYDDA\ncDicKEsZpSdLJIb2S8t8tKUradLyXE+Avapv0Q4jWpi10OoFOu2m0d9VU3YiuTnmsYT3AfwSgH8I\n4CeC53wQwDsAyF/2ZI7PIcSlJMLaCtYWaRRP3Ov1xtEKg8Fg3MTK9LZMAl4TOj2mrV49rt0NkYtC\nir97lq8VYC3CthaFtoAlnlp+mOx3yOy61WBmEc45fwjAhwAgxX+9k5zzZ68zMUI8tDsCwETqsfUV\n6+fYmhPWzyv71UWbhtp4W0+AbcEfmZcWZm0Bax+x3iDUcz1YAdaRGfJ9WAH2du3wvkt9DRTkm2dZ\nPuG3pJReBfAFAP8SwLtyzp9f0meRW4gWWy2M9lyHoYkFrBM4dMSCbPTpuSAATAhfJMBWtAFMWKql\nymza19wkwPK5UcKKDaWrgQLcDssQ4Q8C+HEAnwTwBwB8D4CfSSm9Kc+StkNIgE0NBvysNesD1tsX\n2Zjb4XA45X6IFty8TDqxbrUVbP2/Yo1GzWbMReKrr9FLWtHX6xUOIqvFwkU45/wBdfqrKaVfBvDr\nAN4C4OcX/XnkduEJcIT4QiWBIfKvSmSBFU/Biq0W+Kbny7k+RuNWlPV1RtdqMwhtLQoR5CjFWWfW\nUaDbYekhajnnT6aUPgfgjSiI8IMHD3D37t2JsYODAxwcHCx5huS2ULJAbdywJHl4sbfb29vodrs4\nOTkZHyXMTS/0yS7LnvB7DfDjhqXqm10MtL5h/aNiLf5oRxA9BmAi5prUcXh4iMPDw4mxo6Oj6tcv\nXYRTSl8G4HUAfrf0vBdffBH37t1b9nTILcMTXO851m/sFYyX10skgxZf6du4Yr0DdOkIYEJMtd9a\n1z7W0RW2hoV1sWgxLu0MnXOeKHxPZsMzFh8+fIj79+9XvX6eOOF9XFq1cu/yFSmlrwLw+av2blz6\nhD9z9by/B+DjAF6e9bMIuQ42DMtL9ABe8x9rS9gW+pHny3NEYEviK30v9VmSNkaj0ZTrwlrCEh6n\nLWCdgq2tX1t4SMRc7zQife/6yM0zjyX81bh0K+Sr9v1X4/8YwLcD+CMA3g7gOQCfxqX4/u2c8+ja\nsyVkDmxYm820k9Rnu4BnXRA6084KbVPfjtldPcQa9kRYi6POvrMuCP06/fqzs7OxdS+lO7ULRC9g\nMkzt5pknTvhfAyj9ZP7p+adDyGLxkhJEgGXhzoqwrbSmfcZWgCXeOBoT/7DOvvM2HdVWubZi9bh1\nVdjFxagK3NnZ2XhXaltHQ/uHdSgduTlYO4JsPJFlZ4sA2a2IogU7K7y2WSE+Pj4e7+bhWcBalK0l\nrBM+dPrzaDSaEmBPiL1ymJGLhRGk7UARJrcCL1vMJns07dbR6XTGghZVWPME2Qow8JoFrF0TwKQI\n20U4EWBZVIusYF1/IhLgKLGD3DwUYbLRaHHTPmFdPUyLsDxuXRD29j4SX1uMR3bQ8CxgqRlhd9dI\nKU3UIpa95cQ9oXdmtj5gawXr0puej1tSpfVCH7lZKMJk4/HcEVqI9XOsAHuRB1bgoqOIsN3VWSzg\nk5OTKR+xjZQQQdYxwtIvibCNxNA+brsXHy3hdqEIk1uBtYL1mPTlMb1Dso7H1eUx7RZE3pZEUhhI\n3tfuriGLdZ4IazH2wutEnLUQ28/WDYh93DYcj9wsFGGysTSl+mq0VewVydHnFxcXYeyvbd1ud8L9\nIOLr1amwn93ELAtz1r0iC4zWEp7VJcFwtutDESbkCmslW4tZhFone+jaFFbAUrosMq+3ILJb3Xt1\nIuSzarDJGhJtcXp6OjFXL5Ij2kbJfif6aPvk+lCECSlgfcclIfZE+Pz8fEJ8tQCXigDpH4EI7S7R\n/mrvPXWiiBZgG0EhSSC2kFD0Y0GuD0WYEIONnvCEuEl8pWlL2BPgSIT1PCJs/HAkwDnncWq1Zw3b\n3TzsNdim50euD0WYkAKeEGsrWBby5Lly1I9r/68VYmttWtGtEWKddScJHjbJw8vo89wRkj4t89fX\nYudFFgNFmBCFFV1vTAusJ5DayrWWcJM7YhYh9twRUZRF5I7QLgkRYS3iOpFE1x1mjYnFQREmJCAS\nYjnXRYD0mFil4qqIBNgKsfe5Jaw7ovTYLAtz9kdGW8EyTgFeHBRhQgylyAj9HEl88BbqdHyxtYTt\n9kk1/uAma1iLsLaQJbPOs4TtAp2tV+HNh1l1i4ciTIiDJ4IaHddrfcC22LoIcLQ4p0W8RngFm9Ch\nx7SfeGtra6qUphVgbQlHhe/1fCjEi4MiTEiAjTCw4/qW3SZ6WBGujY6odUXoz7IF4bVLRN7fuiNs\nUokW4pIIzzI/UgdFmJAKalwGkZVYGx3hvW9J9ER05Tml3Zy1Fey5IrQQe5EQ+geHLonFQhEmG4uN\nclgk2nLVn2XxNtT0rOCahS7vWrTwezG8cvSsX7sgp3ejlgU6EWBdAJ8sFoow2Wg8S7X0uFc3Inqs\n5nh6eorHjx/jyZMnePr0KQaDwcROzF4Fs2jO3vyjZAo7ZhcGoyy+mh8MeX+yGCjCZOPx3ASeaNrY\n2tJYbdMifHx8PBZhna2mK5jNYmnaqIzSuSe8XovE2BN6shgowmSjicTW9nXSQ03zxNkT7pIlbHe8\niFwNJbTgejWHpW8jNKzw2qON3mhKsSbzQxEmG0/kVrAt2jbeO/cE2Wunp6d49OjR2BI+Pj5udEfI\nPEvnwLQlrJt1J+gIjZJFTHfEzUMRJrcGa6Xq5m0Xb3eu0H271XzU9yxhHaVg935r8ltbrAjrbDzd\nj3zBVnzlPLKAaQkvHoow2Wg8sfXOS9vFe80rpu6NiQg/ffo0dEdoi9qbf4QnwFZ89V5ys/iES+4N\nivBioQiTW0EkxvpodyyOau7KxpmepWzPo4U5646YVYAFzyWhxVf7em10RMkqtot7dEcsD4ow2Vgi\nP7Dnx7V7tem4Wtu3VrEWb297oSdPnkwtzHlbC+k5z0JJgK3oRhXdbIuSPmgJLx6KMNloojhgu4Dm\nWcG26pgthG7Tfb2x09NTPH36dLwopxfmbIjarAJcWpSLBHgWd4SNOdZ9CvHioAiTjceLhPAiGawQ\n6yLoXvEbz01h+6PRaOyGsHHCTe6IGppcEU1JGlGomud2oCtiOVCEycbjiW+NO0KaWK6yU3K0vb2X\nFiyvt83GCdsQtRqsi0CLcMkSbooV1iLc9Pnk+lCESetEVmC0WFWK+9VjXrRCFMkwGo3GQtvUxIJt\nWryT5lnQ19lmXmMtYeueqK1b4bkemsiZxd0XAUWYrBRNdROsBVs6t9EKpRhgLZZabL3jLD5h69YQ\nYbaJH/PgLZh5CRuLLCJEFg9FmKwMTcVwtHVrj9GYJ5Resy4IbzugyJqNIiPk86OthPQPxzyLcrrv\nLdDZpI3IAp5XfCnai4EiTFaCptoOnoshsnD10auZG7kNtBCX+tqSjTLr9LknwvqH47quiJJveJlp\nyHRHLAaKMFkZmvy9ur6DtihL/WjRrGYsWmwTEa7NmvPmZ90RtUJsRc+zgktuiRq/cC0U4MVAESYr\nSVRk5+Liws1qi0TVs2aj85KFbC3o2toR1jK31nqtAHuC1+QTLgnwItwRtIQXA0WYrASR4EaWsOfH\njXy5eizq60y4Jivbq6QWlbvUFq+tOVHrE44EWPcjAS65I2gJrwYUYdI6UVZblGpsRbg2rKz0eM0C\nnmfBloq/ayG2MclWrCMRLgldySfcZA1H4juLsNISXgwUYbIy1NR6iLLadBKETYqIjvq54uetWezz\nLPRSayoCP2uImmcF11rCXrH2eV0SFODFQBEmK8GsxXa031eLqaQFy9H2o3Z6ehomdniLb3qudv7e\n9XjXZscsJTeEZ/3a9OV5EzZKpJTGFjAt4cVAESYrQyRONenFJycnGAwGEzUadLMCbfs2gaIUgzyr\n5TpvCJqlaXGulDE3j/iK0HrnFODFQREmVcwqJJEl6B2bhE/3tdhGgqsL5UR1G7xCPE0RD7OGk2lK\nhXCarNtorNfrodfrodvtNh7v3LmD/f199Pv98es6nQ52d3cnhDqaa3ROrg9FmFyLSJAiv2kU8VDj\ni9Ui3GTZasG1fmArwN5CmV00u441Gy2glRItvIU1e65F1vbt2J07d8ZCvLe3Nx6Xwj61RXvI4qEI\nkyrsrakVJS/CoVTjQY/VhoadnZ1NLbyV/L5R/Qcv9VjKSXrie10Rlu/PLoZ5R1vNzKtwJn0ruFHr\ndDrY398fN7GG5TF5P5kHuVkowqSKSICta0H6TQkM+hiVgPTGm6Ic9HhNcoa1hEtW8LxCHEUweL5a\nXWbSlp60j2mR1Uevv7e3h36/j729vSlL2NYQJjcLRZjMTcnXWyoj6aX1RsVyouSLmrjgUtqxtYKt\nCOujd42z4Lkaol2RxU9bc7Tia8d0X/zA2iesLWH5ESA3z0winFL6LgB/DsAfBjAA8AsA3plz/rh5\n3ncD+MsAngPw7wD81ZzzJxYyY9Iqs4RiaQu4VHHME9ZIbKOsN2+sptSkdUdEvmt9rfNQit/VrobI\nuvWsXBFar9nHIzcFLeH2mdUSfjOAHwDwH69e+z0Afjal9ELOeQAAKaV3AvgOAG8H8CkAfxfAy1fP\nOV3UxMnNYsOT7NGKlRXgSAg9F0Op75WVjMpN1lQ7sxlwpXad787G8NpdkGVbellM86IcbF9bxLrZ\ncX3u9WVhjj7hdphJhHPOb9PnKaV3APjvAO4D+PDV8HcCeG/O+aevnvN2AK8C+CYAH7jmfElLRAtx\npeQDL6438vNG8bz2GLkZ9Jj0vepmpWPT9cyDjX7wdkPWPl8R2X6/P3YdRH1PTKOj52v2trknN891\nfcLPAcgAPg8AKaU3AHg9gJ+TJ+ScH6WUfhHAm0AR3ihmEWCvtKQcbbyv3RhT9+2iWqnSWbTgVlqE\nk+soHefBWsJ2I04RTO27lUU0u6gm51pgreDa82hre5tNR26euUU4Xf5svg/Ah3POv3Y1/HpcivKr\n5umvXj1GNhQvHtgTYs91oIXWbg9vzyNfr+fumKXGg1yDvh57ffPiRUbYjTi171YLrg4t021vb29C\ncJuazaaL+uTmuY4l/BKArwTwtQuaC1lxagXNWsOeJawX1LQAl9qTJ0+mIhu8o7aC7fy9a1o2USqx\n3QlZRFgs4f39/XGShU62kL7dQdkLaZO+zc4r9cnNMpcIp5R+EMDbALw55/y76qHPAEgAnsekNfw8\ngP9Ues8HDx7g7t27E2MHBwc4ODiYZ4q3lnlEpVacSqm8tnkbZkbRD9bq1dav9hXbbeJ188bmDSez\n/Si9WPejsa2trarFMxFgsXRFcLXlKxay+IStb7fk96XILo/Dw0McHh5OjB0dHVW/Ps36D/VKgL8R\nwNflnH/DefzTAL4v5/zi1fmzuBTkt+ec/6nz/HsAXnnllVdw7969meZCpin9PUtiW5OA4YldaV+1\n0o7F1hL2fMHeUWo81FQ6m+Pf9kzpxVGRdGv1RqLrjYkLQvt/9bnu24U1u9imxynCN8vDhw9x//59\nALifc35Yeu6sccIvATgA8A0AnqaUnr966CjnPLzqvw/Au1JKn8BliNp7AfwOgJ+c5bPIYmjybXoL\na1HfbpZZOnqJFl5Mr/iEvVKUuuB6VOWsdneKGqzARqnFXqyvTbqwLgdPfG24mPiDdRPLV0dEaCs3\n2j3Dcz+Q1WRWd8S34XLh7V+Z8f8dwI8AQM75e1NKewB+CJfRE/8WwJ9hjPDNEy002X7TYpU0G9lQ\nu39blPmmHysVXo+K7UQ7XNhrrKFk4XoFdEpWp3URlDLe7JhER9jsNpvl5kU8eGUqyeoza5xw1fJp\nzvk9AN4zx3zIEmhyOTRFEcjjNdsJNQlulFxRKraj93+L/NHeD8g8eOnF1sr1Fr9KbZZU5NqCPBLx\nEO2acZ0dM8jNwtoRG0ptzKsWMM+q1GN2J4uoZq8USbfC6415j0fxxHqX42WVnLRhZF568c7OTjFl\nOFqIs7G7Xr/2faTeQ9MGnmT1oQjfAjwB9ixhT9giEfaKqtsFtBphFSvXK11px2yNB+/HQl/nrNhC\nOzazzatgVmrahztLYoVnYXtjnvvEW0Qkqw1FeIPxfL9Rv6n2rxZhsXhtWJnul8pR2vEo5tcbW1aN\nBy/ywboevMy2qOmavZHwei3yL3vnWnS9jTspwOsBRXjDscJkxaop3tcW4vESLJ48eTI+SovqRHjN\ni3iIxqJFQ+9aZyVKLY6SKmwkgxfVIPG8s2S2eZEWXr8mlI6sPhThDSSKBy5FPnjWrxVA647QIvz4\n8eOJpt0MTXUeSr7oaOHQXut1XRFAXO3MRjnoSmY6nteL7e33+2MLV4eXRVZwFBIXWbt63tL3jmR1\noQhvKE236DXiazfbtO6IwWAwIcJHR0d49OgRHj16NOXrtf5dPV4bSmfnv2iiOGARYp3Zpiua2boO\nOttNkioi0bXnMo/S0fZLY2T1oQi3yCxCUrJi7XlkQXrnWmRL/bOzs7GVq90OuraD3vHYW2iLCu3M\nQym92I7V9CWzzSuWbsd6vV5YWMdmu/X7/eJWRfY4z/WT9YYi3BI1AlxaPCu1UoSD7VvBjc7Pzs7G\nFq8+ivjqoutWdKM5zsssqcXeLb3XnyXszHND6Kw2HctrazhEiRXk9kIRXiFKboOavdpE8GoK7Fgr\n2BNf22xFMx0RIWnGOrHC1pVYRFJFk8B6YVtRWrFdgCvVdGhKL9bN7t/mpRgzu40IFOEVoeTzFMtV\n/LKlrYJKO0mU/L01Tdf1lYU5aXqHY7urxaLrPIhoeYkKeszbRsjbVsguvDVtBaQX5rxmLWGb7OHF\n9pLbC0V4BWhajBJLWBdGj5Igom19PFGsta6lL4tx3rZD1hKOfMuLEmErul4dBW/hq5T8UJtarP3E\nNenF0RwZTkYAivBK4YVZaX+wjlAo1VtoKjPZNBY9LvUhdHEde67Dzjy3iF0gnBWv0I62aL2C6aXy\nkdbajcR41vRi/Z5RISD6hAlAEW6dKM7VphXL4pjeHDPambi024Q3VhJlfR4V5bG1ILzFOLswuKh4\n3ii1WHy8nsXqWbBNGW1N2W5Nsb+lRiG+3VCEV4iSEFt3hC6io9twOCzuv2abJ7ZR39tMMxqzURlR\nmNysNCVUWAvY2yreG6tJKfaiHZqaTi9majHxoAivAE0ZYJ47QouwruEwGAwaC+HoYyS6swq0PS/V\ndrhuooVemPMSKkQsbVKF3TbeRjOUrFrrT27atbjk+71uejGFe7OgCLdIzYKcFmERz2iDzCdPnowT\nJaIUYc+KbRLiUh3fUqyyvRZ9Tfaaa/EsYc8HvLu7G24f720hLyJcs2mmTS+ucTXUJIzUXn/OmUK8\nQVCEV4hSBpx2CZRSh6WCWVTFrLaCmec7rhHUkqW7qFRjb2HO+m91kR27dby3lXyv13Mz2Lwxa93K\nnCILd54UYy22+kg2D4pwA6V/+KXkCv14JFRNTSzK0WhU3AbejtWUj9ThbLVuiWVRqpVgx8TN0LTQ\nJgLsia4nwt1ud6pmcMnP6823dC2L/l7I5kARrqTkt5W+Fc8odXjW54xGo4lykbpspB7TPmHPFeHV\ncahJqFi0BWZFpDa1eGtryxVh8f16Iuy5Hvb29sKkCrugFkUx1BbVIaQJinAF0e23Z9lG2Wo2eWEW\n/6pYwl7xdDumF+aizLomAb5uBINH6Za8aU833ddVzKwA2zFdXN1LL9YiXLN7cWkhjWJM5oUiXEnT\nSr/229b4WT1Rjo6j0chNFdbnXoiaFyNs+96PwnUTKiye5av7UbyvF/8rYWdWfKOxKDxNP2bTiz0h\ntuJLoSWLgiJcSclnq61gL342CherSRuW97QpwjZ9WKcOR+/tjduaEotKqBAi4dK389ECmxcuFglt\nFAPclF4si3i2wpm1gkuxvRRnch0owhV4MbteEkJNbQc9VhseNhqNpjLjbJOQNZ227LlBvL72TXvu\niEWKceRL1WFmXuqvLiPZtK+bV0inKbVYZ7ZF/mBbbIeCSxYBRbiBKKLBS8fVolmq7SA1Fmqz2iRN\nWV5baqenp6F1G/miS4uC1xHgptV96xPWIlwqjKP9vJ7v17oaopRia3F7cb92cVDPO7peQmaBIlyB\nJ76RCOvaDrq4jT3q4ueljDYt6rZwT1TDwbNuox+OJhfLoiiFn0UiXLJ4reh6WXFePV8bBTHr7sXz\nxPwSUoIiPAORNawL7Ig7wiZU2L6uOOY1L8utFO8rj52dnYWCGo1FC42LEOPID2x9wpEIl4qna9HV\nuxvLuBbcUpqxFeDaFGOKL1kEFOEKmsRX+1g9S1gWzXREg7aGo7rA1n/shZp5bg0759K59KPjMhbn\nvCyzkgjrHYxL28vbMfHzej7emvRib772egi5LhThSkpCrBfVtCUsi2WSWizhZE+fPh3vx9ZUFtLu\nVNFUTEfXbZB52+sonUdj18WzIvWtv46OsCJss92s8Np93vQGm5FLYVYLl8JLlgVFuAFPfEuhZFqA\ntQvCFtuRSIYmAdZ+3qZIB13fwVKzmFTTr8X6VqPzlNJUNpuXVlwS4cgaFhHW19B0JOSmoQhXoMXX\nJjvoVgohs/G9utqZ54LQscSlON7IZVBzO13rA/Xeo4mm23+98KVrPIgAR31detJmvHEXY7KOUIQr\niLZvdJ0AAAsmSURBVMLQ7DGK5bULc3LetBhXSisu1XeYVVyjkCx7XsIKnpdyHKUmRxat17cxwF7a\nceReoNVLVhGKcAM2I05bwdaK9bYb8iIjdHqxt7DmZdWVrODIEi6FW0nfZohFR5uoECHPsTsM2/3f\n9LmXbBHFAOsMOJ1wYTPe6Ocl6wJFuILIEtYuBOsHjqxhfayp7RDVdSi5JGqtXGuVeqFcesxSErFS\nSUg7FhXk8YrzeJl0uvaD94PBsDKyylCEG/AW5LwoCIkLLrkjbM2HqJ6DrfdgRbc2q83zw9pwrVn2\nSqtZ3BNKmWnahSD1IOxW8qV+VGPCS7qI5kkxJqsCRbgCawnreGAtxDWWsO7PUvayJqtN+p4lHPlm\na7f02d3dnfhOSgIHYKJeQ9S31qy2ar2+zYArZb2V5kbIKkERbqBkCesaEVaAI4tYW8K19YQvLi4m\n5lLKdBMiIbauhpLo2X6TuOlza8Faq9YurHnbCUXbDXkZb00+4WjOhLQNRbgCffuvBVhbwiK8kUVs\nw9NOTk4ai+dEft+oL3j+YE+soqpl1vcqTb+//Tzb13V/m3bBsPWDS4t63oKh16c1TNYFinAF81jC\nNXV/Z6nfIOfe0faBSSG2rghbJD2quStCKo/J+9rP8fo1NX+lRWUko35TxAfjhMk6QRFuwKYp23KV\nNZERnk/45ORk/P76s5r6pTFgOkY4ioKwrggvEkHH5Ha73Zmy67wKZ15Bnl6vN1708wTViqv+jKZE\nFELWAYrwHER1JGaJ221KfpgXHfFQ42et3Q7Is4Q9wbMi7JWYlI02Zdxarl6WHsWVbCoU4QasNSm3\n8DZqwb6mqSrY6enp0uZbG3YmMbqlpuNza2NvU0pTr9XhZFH5SHkthZbcJijCFYioamG1IqzFxN7y\nWwHe39/HaDRa6lztRpneQpf1CUd9OdrP8fpybhfmamo8MJqB3EYowg1oS9gTYMFay1rgtADLwpzU\n/V0GpSLm9rEojtcb09+J/Y7suU7CiLaWp0+XEIpwFdYS7nQ6U4kR3qKX3hPNbm+0bEu4pnCOJGuU\nstv0uf4M+5mWKBGjxhKmEJPbBEW4AWsJawvYPqZFq9vt4uTkBP1+f2JPOOkvyxL2suSivg1XK9V7\n2NnZGb+/95mWaIPN2kI7TDEmt4WZRDil9F0A/hyAPwxgAOAXALwz5/xx9Zz3A/iL5qUfyjm/7Zpz\nbQVt6V5cXIwtQmsB63CvTqfTuF3R+fn50uZcU8vXuk+iBAl9LH1HllJqsVeLgqJLbiuzWsJvBvAD\nAP7j1Wu/B8DPppReyDkP1PM+COAdAOR/08k159kq2toFfAvYFnpvKlEpqcjLwMbWRgkOpZoS3nnp\n8yw1qcU1JScpyGTTmUmErTWbUnoHgP8O4D6AD6uHTnLOn7327FYAbfEKWoCjbYZsJTQ7tiwRtska\nXvKGF7Nc06LP86h1iUQhaRRfclu4rk/4OQAZwOfN+FtSSq8C+AKAfwngXTln+5y1QQvQ1tYWLi4u\nsL29PZWoMUtBnmVspimUEh288yhTzZ57n1OaQ8kCt3Npej9CNpW5RThd/o95H4AP55x/TT30QQA/\nDuCTAP4ALl0WP5NSelNepvIsCREGsdyaajs01YAo1f9d1txr+k1RCvMIZNOPgZ3LLNdDyKZwHUv4\nJQBfCeBr9WDO+QPq9FdTSr8M4NcBvAXAz1/j81phXgEihJAa5hLhlNIPAngbgDfnnH+39Nyc8ydT\nSp8D8EYURPjBgwe4e/fuxNjBwQEODg7mmSIhhNwIh4eHODw8nBg7Ojqqfn2a9db4SoC/EcDX5Zx/\no+L5XwbgNwF8Y875p53H7wF45ZVXXsG9e/dmmgshhKwiDx8+xP379wHgfs75Yem5M5XySim9BOBb\nAPwFAE9TSs9ftd7V4/sppe9NKX1NSunLU0pvBfD/APg4gJfnuRhCCNlkZq2n+G0AngXwrwB8WrVv\nvnr8HMAfAfCTAP4rgP8bwH8A8L/mnJeTp0sIIWvMrHHCRdHOOQ8B/OlrzYgQQm4Ry6ksTgghpAqK\nMCGEtAhFmBBCWoQiTAghLUIRJoSQFqEIE0JIi1CECSGkRSjChBDSIhRhQghpEYowIYS0CEWYEEJa\nhCJMCCEtQhEmhJAWoQgTQkiLUIQJIaRFKMKEENIiKy3CdvO8TWKTrw3Y7Ovjta0vq3h9FOGW2ORr\nAzb7+nht68sqXt9KizAhhGw6FGFCCGkRijAhhLTITLstL4keAHzsYx+beuDo6AgPHz688QndBJt8\nbcBmXx+vbX25qetTetZrem7KOS93Nk0TSOkvAPgnrU6CEEKWw7fknH+s9IRVEOHXAfh6AJ8CMGx1\nMoQQshh6AH4/gJdzzr9XemLrIkwIIbcZLswRQkiLUIQJIaRFKMKEENIiFGFCCGmRlRThlNJfSyl9\nMqU0SCl9JKX0R9ue0yJIKb07pXRh2q+1Pa95SCm9OaX0Uyml/3Z1Hd/gPOe7U0qfTikdp5T+35TS\nG9uY6zw0XV9K6f3O3/Jn2ppvLSml70opfTSl9Cil9GpK6Z+llP6Q87y1/NvVXN+q/e1WToRTSn8e\nwPcDeDeA/wXAfwbwckrpS1qd2OL4FQDPA3j9VfsT7U5nbvYB/BKAbwcwFWKTUnongO8A8FcA/DEA\nT3H5d+zc5CSvQfH6rvggJv+WBzcztWvxZgA/AOBrAPwpALsAfjal1JcnrPnfrvH6rlidv13OeaUa\ngI8A+D/UeQLwOwD+VttzW8C1vRvAw7bnsYTrugDwDWbs0wAeqPNnAQwAfHPb813Q9b0fwE+0PbcF\nXNuXXF3fn9jQv513fSv1t1spSziltAvgPoCfk7F8+a39CwBvamteC+YPXt3i/npK6UdTSv9T2xNa\nNCmlN+DSutB/x0cAfhGb83cEgLdc3fL+l5TSSyml/6HtCc3Bc7i09D8PbOTfbuL6FCvzt1spEcbl\nr9Y2gFfN+Ku4/Iex7nwEwDtwmSH4bQDeAODfpJT225zUEng9Lv/hb+rfEbi8nX07gD8J4G8B+DoA\nP5NSSq3Oagau5vo+AB/OOcvaxMb87YLrA1bsb7cKBXxuDTnnl9Xpr6SUPgrgNwF8My5vkciakHP+\ngDr91ZTSLwP4dQBvAfDzrUxqdl4C8JUAvrbtiSwJ9/pW7W+3apbw5wCc49JhrnkewGdufjrLJed8\nBODjANZi5XkGPoNLX/6t+DsCQM75k7j897sWf8uU0g8CeBuAt+Scf1c9tBF/u8L1TdH2326lRDjn\nPALwCoC3ytjVLcJbAfxCW/NaFimlO7j8wxf/kawbV/+oP4PJv+OzuFyx3ri/IwCklL4MwOuwBn/L\nK4H6RgD/W875t/Rjm/C3K11f8PxW/3ar6I74+wB+OKX0CoCPAngAYA/AD7c5qUWQUvo+AP8cly6I\n3wfg7wAYAVi9ja8auPJjvxGXVhMAfEVK6asAfD7n/Nu49MW9K6X0CVxWyHsvLqNcfrKF6c5M6fqu\n2rsB/DguBeuNAP4eLu9qXp5+t9UhpfQSLsOxvgHA05SSWLxHOWepYri2f7um67v6u67W367t8Iwg\nrOTbcfnHHwD49wC+uu05Lei6DnH5j3kA4LcA/BiAN7Q9rzmv5etwGfpzbto/Us95Dy7DnY5x+Q/8\njW3PexHXh8syhR/C5X/iIYDfAPB/Afgf2553xXV513QO4O3meWv5t2u6vlX827GUJSGEtMhK+YQJ\nIeS2QREmhJAWoQgTQkiLUIQJIaRFKMKEENIiFGFCCGkRijAhhLQIRZgQQlqEIkwIIS1CESaEkBah\nCBNCSItQhAkhpEX+f5mfadssU0IbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb18075c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neuron_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f783d98b6d8>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuIuIlCCFu4hICVK4i4iUoP8PbnF3JsjqIVMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f783f546550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f982a07a069d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neurons.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "with open('neurons.pickle', 'r+') as f:\n",
    "    pickle.dump(neuron_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphableNeruon(weights=array([[[ 0.04885922,  0.12467428, -0.12467383],\n",
       "         [ 0.02851254,  0.02811397, -0.08050238],\n",
       "         [ 0.13408311, -0.08459224, -0.11106701]]], dtype=float32), bias=0.00034257898, layer=0, neuron_number=0, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.01507164,  0.0660271 ,  0.08590862],\n",
       "         [ 0.08031062,  0.0589522 , -0.10438707],\n",
       "         [-0.10811615, -0.02432563,  0.04790505]]], dtype=float32), bias=-0.0001565725, layer=0, neuron_number=1, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.02261747,  0.06860337, -0.12488568],\n",
       "         [-0.13569264, -0.02418157,  0.06075856],\n",
       "         [ 0.02928413,  0.12057737, -0.00282951]]], dtype=float32), bias=-0.0006562674, layer=0, neuron_number=2, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.08670688,  0.00548047,  0.03822316],\n",
       "         [ 0.03638752, -0.06664986,  0.01881798],\n",
       "         [ 0.05816134,  0.12334223,  0.10224809]]], dtype=float32), bias=0.00033025944, layer=0, neuron_number=3, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.03401228,  0.10844734,  0.03911998],\n",
       "         [ 0.08915877, -0.04122515, -0.00546009],\n",
       "         [ 0.01603308,  0.10670482, -0.03221359]]], dtype=float32), bias=0.00072368537, layer=0, neuron_number=4, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.09422617,  0.07886653,  0.00716214],\n",
       "         [ 0.0126227 , -0.07815487, -0.04373106],\n",
       "         [-0.14090376, -0.04809118,  0.05401329]]], dtype=float32), bias=0.00021821359, layer=0, neuron_number=5, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.03593434,  0.0241116 ,  0.0079205 ],\n",
       "         [ 0.01496261,  0.08690415, -0.07642608],\n",
       "         [-0.07593713,  0.00936745, -0.06016301]]], dtype=float32), bias=-0.00022137768, layer=0, neuron_number=6, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.11839755, -0.08992682,  0.10740984],\n",
       "         [ 0.10898454, -0.13618124, -0.03865692],\n",
       "         [-0.09437135, -0.06263904, -0.12346771]]], dtype=float32), bias=0.00056123745, layer=0, neuron_number=7, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.09470922, -0.08307364,  0.09045283],\n",
       "         [-0.02261148, -0.1250376 , -0.06283762],\n",
       "         [ 0.04151038, -0.12898909,  0.01749008]]], dtype=float32), bias=0.00027624704, layer=0, neuron_number=8, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.11341092, -0.08092086,  0.09036765],\n",
       "         [-0.13825653,  0.07756776, -0.00177725],\n",
       "         [-0.05933958,  0.13054821, -0.11174753]]], dtype=float32), bias=0.0010958931, layer=0, neuron_number=9, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.00163658, -0.09942169, -0.03817872],\n",
       "         [ 0.13497937,  0.02532357, -0.13516062],\n",
       "         [ 0.00259171, -0.05640399, -0.01293492]]], dtype=float32), bias=-0.00066300877, layer=0, neuron_number=10, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.06519754, -0.08074918,  0.03579694],\n",
       "         [-0.12060815,  0.02262459,  0.11122555],\n",
       "         [ 0.03159371, -0.0196292 , -0.05416906]]], dtype=float32), bias=0.0004891227, layer=0, neuron_number=11, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.02146833,  0.02002496, -0.10910771],\n",
       "         [-0.02321792,  0.11597598,  0.03278996],\n",
       "         [ 0.0972603 , -0.03540947,  0.07526086]]], dtype=float32), bias=-0.00038874795, layer=0, neuron_number=12, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.04961428, -0.02146158,  0.04976092],\n",
       "         [-0.05868462,  0.00016212,  0.08104696],\n",
       "         [ 0.10739931,  0.13226928,  0.13492589]]], dtype=float32), bias=9.8502613e-05, layer=0, neuron_number=13, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.04824184,  0.10303477,  0.03306795],\n",
       "         [-0.13797635, -0.03601267,  0.02602677],\n",
       "         [ 0.06097351, -0.03182723,  0.01347174]]], dtype=float32), bias=-0.00046204159, layer=0, neuron_number=14, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.09976528, -0.0352602 ,  0.00930243],\n",
       "         [-0.08263587,  0.01938476,  0.00763963],\n",
       "         [-0.09236038, -0.08783736,  0.05923694]]], dtype=float32), bias=0.0011182553, layer=0, neuron_number=15, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.0726155 ,  0.04847715,  0.08123691],\n",
       "         [ 0.11614378, -0.10040515, -0.03248913],\n",
       "         [-0.04572001,  0.0615749 , -0.0032932 ]]], dtype=float32), bias=0.00020879005, layer=0, neuron_number=16, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.10576193,  0.02833541,  0.09927645],\n",
       "         [ 0.05374307,  0.07566579, -0.13992485],\n",
       "         [ 0.05295023, -0.00158151, -0.08777899]]], dtype=float32), bias=-0.00023423985, layer=0, neuron_number=17, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.03436508, -0.10583371, -0.12350465],\n",
       "         [ 0.06328285,  0.12660609, -0.12984812],\n",
       "         [-0.02683781, -0.05599083, -0.09873972]]], dtype=float32), bias=-0.00046975643, layer=0, neuron_number=18, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.08880267, -0.06633013,  0.10406931],\n",
       "         [ 0.02471365, -0.02148305, -0.10708714],\n",
       "         [ 0.01303947, -0.13705207,  0.08766588]]], dtype=float32), bias=-0.0017241414, layer=0, neuron_number=19, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.02809201, -0.04385575, -0.04062418],\n",
       "         [-0.00632757, -0.08256186,  0.09587613],\n",
       "         [-0.11624467, -0.09942689,  0.09150076]]], dtype=float32), bias=-0.00017427211, layer=0, neuron_number=20, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.11407501,  0.12273315,  0.14191104],\n",
       "         [-0.07774161,  0.1408014 ,  0.03234181],\n",
       "         [-0.05239408,  0.00259094,  0.02935199]]], dtype=float32), bias=0.00033411052, layer=0, neuron_number=21, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.01856494,  0.08011169,  0.01557426],\n",
       "         [-0.13426924, -0.10975287, -0.0974026 ],\n",
       "         [-0.00810835, -0.0690881 , -0.09792074]]], dtype=float32), bias=-9.7996424e-05, layer=0, neuron_number=22, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.06381731,  0.11983477, -0.01347659],\n",
       "         [-0.09591709,  0.13150668,  0.1225578 ],\n",
       "         [-0.01554125, -0.05524668, -0.00210765]]], dtype=float32), bias=-2.2606109e-06, layer=0, neuron_number=23, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.12185017, -0.09283887,  0.01259141],\n",
       "         [-0.00214833, -0.09252465, -0.1026255 ],\n",
       "         [-0.12548411, -0.09861178, -0.06914558]]], dtype=float32), bias=0.0012544271, layer=0, neuron_number=24, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.11772302, -0.13301474, -0.11923912],\n",
       "         [ 0.09610681, -0.07397643,  0.14136584],\n",
       "         [-0.09577103,  0.14352918,  0.05894703]]], dtype=float32), bias=0.00042441016, layer=0, neuron_number=25, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.08298761, -0.11294329,  0.13408068],\n",
       "         [ 0.13938224, -0.07557685, -0.11366856],\n",
       "         [ 0.0817083 ,  0.09808572,  0.12672234]]], dtype=float32), bias=0.00018991478, layer=0, neuron_number=26, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.00979209,  0.0219467 , -0.12282573],\n",
       "         [-0.05467198, -0.06786033,  0.04411578],\n",
       "         [-0.03184821,  0.05206807, -0.12398811]]], dtype=float32), bias=4.3073473e-05, layer=0, neuron_number=27, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.10647488, -0.0215615 , -0.111858  ],\n",
       "         [ 0.11598538, -0.13132645, -0.05582479],\n",
       "         [ 0.0410101 ,  0.10948224, -0.04527203]]], dtype=float32), bias=0.00052748795, layer=0, neuron_number=28, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.10411505, -0.10349209,  0.0677603 ],\n",
       "         [-0.13364547, -0.13592134,  0.00783236],\n",
       "         [-0.10775468, -0.03002691,  0.12583062]]], dtype=float32), bias=-0.00087437523, layer=0, neuron_number=29, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.13995476,  0.11507289,  0.13337903],\n",
       "         [-0.07260299,  0.04723918,  0.10375413],\n",
       "         [-0.04218078, -0.13477375, -0.00193236]]], dtype=float32), bias=-0.00032888411, layer=0, neuron_number=30, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.11994798,  0.13725364, -0.01922129],\n",
       "         [ 0.00331485,  0.1052608 ,  0.00249643],\n",
       "         [-0.05485303, -0.10530301, -0.0433749 ]]], dtype=float32), bias=-0.00041943777, layer=0, neuron_number=31, layer_type={'dtype': 'float32', 'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'batch_input_shape': (None, 28, 28, 1), 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 32, 'kernel_size': (3, 3), 'name': 'conv2d_7', 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.01407393, -0.0107643 , -0.05546533],\n",
       "         [-0.05502502,  0.02664829,  0.05706437],\n",
       "         [ 0.00033796,  0.05201217,  0.04151094]],\n",
       " \n",
       "        [[-0.0349181 ,  0.02612919, -0.06294907],\n",
       "         [ 0.02015115,  0.03290606, -0.03463635],\n",
       "         [-0.00896481,  0.01204591, -0.04708948]],\n",
       " \n",
       "        [[-0.04041793, -0.06780951,  0.0758104 ],\n",
       "         [-0.05281343,  0.011552  , -0.07938241],\n",
       "         [-0.01620841, -0.05498621, -0.03414507]],\n",
       " \n",
       "        [[-0.05867042, -0.05212214, -0.04908245],\n",
       "         [ 0.02899418,  0.04031185, -0.01002046],\n",
       "         [-0.04232142,  0.02200339,  0.01265242]],\n",
       " \n",
       "        [[-0.04377588,  0.00487964, -0.02814939],\n",
       "         [ 0.01842954,  0.03151154, -0.08217017],\n",
       "         [-0.01404725,  0.01857026, -0.01100802]],\n",
       " \n",
       "        [[-0.04060164,  0.07159266, -0.05340628],\n",
       "         [-0.00088234, -0.04984803, -0.07299314],\n",
       "         [ 0.0282422 ,  0.01341806,  0.04499865]],\n",
       " \n",
       "        [[-0.02008965, -0.00854587,  0.03826771],\n",
       "         [-0.02848656,  0.07451779, -0.03096171],\n",
       "         [-0.03099839, -0.01169301,  0.00632885]],\n",
       " \n",
       "        [[-0.07244651, -0.07443539,  0.04723987],\n",
       "         [ 0.04012394,  0.0329181 , -0.04885105],\n",
       "         [ 0.02027351, -0.08011705, -0.00296144]],\n",
       " \n",
       "        [[ 0.06964812, -0.06736981,  0.07067934],\n",
       "         [-0.07083291,  0.00702827,  0.02431456],\n",
       "         [ 0.03196734,  0.01038907,  0.0159425 ]],\n",
       " \n",
       "        [[-0.06370988, -0.0700365 , -0.07650595],\n",
       "         [ 0.0654029 ,  0.02993575, -0.00880761],\n",
       "         [ 0.00317497, -0.04485464,  0.04562888]],\n",
       " \n",
       "        [[ 0.06489807,  0.01307112,  0.03373144],\n",
       "         [ 0.04718288,  0.01425416, -0.08246973],\n",
       "         [-0.05057475,  0.07126977, -0.00509673]],\n",
       " \n",
       "        [[-0.00794273,  0.02571267,  0.01219666],\n",
       "         [ 0.01122907,  0.05686211,  0.08333018],\n",
       "         [ 0.07904056,  0.08017728,  0.04637002]],\n",
       " \n",
       "        [[ 0.0409251 , -0.06850675, -0.05452728],\n",
       "         [-0.07871656, -0.01660801,  0.0454636 ],\n",
       "         [-0.0198935 , -0.06874099, -0.03303956]],\n",
       " \n",
       "        [[ 0.06329325, -0.03264241,  0.08233555],\n",
       "         [-0.04697387, -0.03222617, -0.0160745 ],\n",
       "         [-0.07328247,  0.03064521,  0.06799084]],\n",
       " \n",
       "        [[-0.04060642, -0.04336909,  0.05580019],\n",
       "         [ 0.00126899,  0.05736699,  0.07024974],\n",
       "         [ 0.03004953,  0.05818751,  0.02461858]],\n",
       " \n",
       "        [[-0.03562968,  0.05345152, -0.06638749],\n",
       "         [-0.03179054,  0.00912107, -0.0749089 ],\n",
       "         [-0.02771771,  0.08272247,  0.08326232]],\n",
       " \n",
       "        [[ 0.0633525 , -0.0009982 , -0.04376096],\n",
       "         [ 0.06865463,  0.05909478,  0.07751853],\n",
       "         [-0.07780553,  0.06544746,  0.02414887]],\n",
       " \n",
       "        [[ 0.03115978, -0.01312963,  0.06781623],\n",
       "         [ 0.05805639,  0.08389582, -0.08140232],\n",
       "         [-0.04969247,  0.01213755, -0.02134172]],\n",
       " \n",
       "        [[ 0.00832485,  0.06549213,  0.01945355],\n",
       "         [-0.0378367 ,  0.08417447, -0.0275713 ],\n",
       "         [-0.07276084,  0.00469645,  0.04244856]],\n",
       " \n",
       "        [[-0.01416312,  0.06151266,  0.05525953],\n",
       "         [-0.04101276, -0.02223987, -0.06387664],\n",
       "         [-0.06117802,  0.04241507,  0.08102695]],\n",
       " \n",
       "        [[ 0.01384167,  0.00173365,  0.03396301],\n",
       "         [ 0.02014323, -0.05151793,  0.00238699],\n",
       "         [-0.08071285,  0.07871385,  0.05247864]],\n",
       " \n",
       "        [[ 0.02079562, -0.00723133,  0.05705567],\n",
       "         [-0.00478538,  0.00461273, -0.0523066 ],\n",
       "         [-0.07364986, -0.03678324, -0.03115385]],\n",
       " \n",
       "        [[-0.05908397,  0.00551619,  0.0196087 ],\n",
       "         [-0.0131458 , -0.01981737,  0.06162165],\n",
       "         [ 0.02468959,  0.01061864, -0.07164186]],\n",
       " \n",
       "        [[-0.04455209,  0.0710471 , -0.01541068],\n",
       "         [ 0.04344068,  0.05637599,  0.06538364],\n",
       "         [-0.08128837, -0.0656453 , -0.00976593]],\n",
       " \n",
       "        [[ 0.04259312, -0.06757978,  0.05146531],\n",
       "         [-0.04894873, -0.03221807, -0.0675383 ],\n",
       "         [-0.00461827,  0.05829206,  0.04269614]],\n",
       " \n",
       "        [[ 0.00190791, -0.04875089,  0.0604127 ],\n",
       "         [-0.06459792,  0.02817044, -0.04983204],\n",
       "         [ 0.03925883, -0.02292865, -0.04541495]],\n",
       " \n",
       "        [[ 0.07004254, -0.03486607,  0.02992111],\n",
       "         [ 0.06221705, -0.05799028, -0.04409521],\n",
       "         [ 0.06525074,  0.02268831,  0.07774097]],\n",
       " \n",
       "        [[-0.02353885,  0.02885751, -0.04511775],\n",
       "         [-0.04949827, -0.07283916, -0.02124461],\n",
       "         [ 0.05210083, -0.03330147,  0.03930995]],\n",
       " \n",
       "        [[-0.07921799,  0.04146089, -0.02727694],\n",
       "         [ 0.07064512,  0.0222037 , -0.0574915 ],\n",
       "         [ 0.07257558,  0.03264249, -0.01677117]],\n",
       " \n",
       "        [[ 0.05682645,  0.04239704,  0.05559008],\n",
       "         [ 0.04952374,  0.07210094,  0.00254481],\n",
       "         [-0.06992864,  0.06686318,  0.01452166]],\n",
       " \n",
       "        [[ 0.06725191, -0.02654248, -0.04160038],\n",
       "         [-0.07276566,  0.00704985,  0.04244743],\n",
       "         [-0.06055441,  0.00672432,  0.07729983]],\n",
       " \n",
       "        [[ 0.03026123, -0.00277026, -0.05505896],\n",
       "         [-0.05968278,  0.0597992 , -0.00240569],\n",
       "         [ 0.04262983, -0.06954041,  0.02182362]]], dtype=float32), bias=0.00023837516, layer=1, neuron_number=0, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.08341724, -0.07405888,  0.03875018],\n",
       "         [ 0.01871105, -0.06943914, -0.01211541],\n",
       "         [ 0.00041949, -0.07582168,  0.0034    ]],\n",
       " \n",
       "        [[ 0.07114892,  0.07936974, -0.02272465],\n",
       "         [-0.03209981, -0.01593269, -0.0400585 ],\n",
       "         [ 0.01118493, -0.00937179,  0.05586819]],\n",
       " \n",
       "        [[-0.00249542,  0.07964405, -0.06552491],\n",
       "         [ 0.05252956, -0.02893692,  0.01669136],\n",
       "         [-0.04609104, -0.00314137, -0.07965203]],\n",
       " \n",
       "        [[-0.07089075, -0.01162274, -0.02466673],\n",
       "         [-0.04343124,  0.0545997 , -0.02160851],\n",
       "         [ 0.04775245,  0.08060724, -0.0080913 ]],\n",
       " \n",
       "        [[-0.06693058, -0.02946781, -0.03433333],\n",
       "         [ 0.06680529, -0.06308839, -0.04975094],\n",
       "         [ 0.02426672, -0.05889077, -0.0016609 ]],\n",
       " \n",
       "        [[-0.00686956, -0.06981073,  0.02839023],\n",
       "         [ 0.02994297,  0.07991481, -0.00907572],\n",
       "         [ 0.08156331,  0.02182579, -0.03229137]],\n",
       " \n",
       "        [[-0.03922905, -0.0234413 , -0.03967236],\n",
       "         [ 0.00349645,  0.00122146,  0.00486241],\n",
       "         [ 0.03719916, -0.0248299 ,  0.07338662]],\n",
       " \n",
       "        [[-0.05521675,  0.04962662, -0.00135464],\n",
       "         [ 0.01851544, -0.0131968 ,  0.04174977],\n",
       "         [ 0.07728588,  0.01577048, -0.02922847]],\n",
       " \n",
       "        [[ 0.07919624, -0.0398389 ,  0.03289806],\n",
       "         [-0.04856676,  0.05634383,  0.03703735],\n",
       "         [-0.01378231,  0.03028626,  0.00330138]],\n",
       " \n",
       "        [[-0.07001388, -0.03908138,  0.06661724],\n",
       "         [-0.00060836, -0.04440711, -0.08166118],\n",
       "         [ 0.01949301, -0.0250522 , -0.03763199]],\n",
       " \n",
       "        [[ 0.0098569 ,  0.08007346, -0.06877455],\n",
       "         [ 0.06240941,  0.0634153 , -0.00911   ],\n",
       "         [-0.01208632,  0.02274449, -0.06501708]],\n",
       " \n",
       "        [[-0.05667373,  0.0259449 , -0.0316253 ],\n",
       "         [ 0.06885254,  0.0351527 ,  0.03331924],\n",
       "         [ 0.06632245,  0.03264451, -0.01597963]],\n",
       " \n",
       "        [[ 0.0286066 , -0.04061146, -0.03941661],\n",
       "         [ 0.0109493 , -0.01637042,  0.05363268],\n",
       "         [-0.01934138,  0.07952476,  0.06149835]],\n",
       " \n",
       "        [[-0.02189178,  0.05462958,  0.02077758],\n",
       "         [-0.00534445,  0.04833962, -0.05658   ],\n",
       "         [ 0.04348286,  0.06925048, -0.07012149]],\n",
       " \n",
       "        [[-0.0261205 , -0.03517994,  0.07238051],\n",
       "         [ 0.01181322,  0.0017341 ,  0.06127784],\n",
       "         [ 0.07113376, -0.07716154,  0.04816822]],\n",
       " \n",
       "        [[ 0.07184082,  0.03042567, -0.01306311],\n",
       "         [-0.03381987,  0.00525021, -0.02819619],\n",
       "         [ 0.03241712, -0.05021475,  0.05779665]],\n",
       " \n",
       "        [[-0.0431938 , -0.04360694,  0.019297  ],\n",
       "         [-0.02060154,  0.03237478,  0.02612635],\n",
       "         [-0.0561506 ,  0.0458214 , -0.06670014]],\n",
       " \n",
       "        [[-0.05236325,  0.04491171, -0.03258263],\n",
       "         [ 0.01687339, -0.07862411,  0.0701909 ],\n",
       "         [-0.00520965, -0.02827744,  0.04560655]],\n",
       " \n",
       "        [[-0.0765233 , -0.03335088, -0.00420626],\n",
       "         [-0.04805028, -0.06342676,  0.00383222],\n",
       "         [-0.05398516,  0.061009  , -0.07158908]],\n",
       " \n",
       "        [[-0.08137956, -0.04702649, -0.01426368],\n",
       "         [-0.04223812, -0.03198234,  0.03438267],\n",
       "         [-0.02364093, -0.07110521, -0.04873058]],\n",
       " \n",
       "        [[-0.0285997 ,  0.04580045,  0.06431596],\n",
       "         [-0.05978238, -0.0233965 , -0.06983919],\n",
       "         [-0.03376363,  0.06426439, -0.00723694]],\n",
       " \n",
       "        [[ 0.07708578, -0.05380214, -0.04821499],\n",
       "         [ 0.04178507, -0.05945251,  0.05968416],\n",
       "         [-0.06008223, -0.0381262 , -0.03840042]],\n",
       " \n",
       "        [[-0.06888957, -0.04830204,  0.02493678],\n",
       "         [-0.05816224,  0.06323551, -0.02018782],\n",
       "         [-0.08294147,  0.0741597 , -0.03660533]],\n",
       " \n",
       "        [[-0.02560678,  0.06870846,  0.01578904],\n",
       "         [-0.05345837, -0.00828246, -0.0447957 ],\n",
       "         [ 0.04376333,  0.07484993, -0.03255043]],\n",
       " \n",
       "        [[ 0.01489476, -0.04583932, -0.0737351 ],\n",
       "         [-0.05912455,  0.02561691, -0.02830584],\n",
       "         [ 0.05072546, -0.0491918 ,  0.08035653]],\n",
       " \n",
       "        [[-0.03259329, -0.06743695,  0.03612066],\n",
       "         [ 0.01899898,  0.03166606,  0.01421419],\n",
       "         [-0.07090025,  0.01290895,  0.05505544]],\n",
       " \n",
       "        [[ 0.00459133,  0.01255155,  0.07057243],\n",
       "         [-0.00576393, -0.06837659,  0.00778506],\n",
       "         [-0.07473856,  0.060621  ,  0.03842592]],\n",
       " \n",
       "        [[-0.06243153, -0.05949001, -0.06913766],\n",
       "         [-0.019686  , -0.02451292,  0.07746759],\n",
       "         [ 0.00224876,  0.07129803, -0.06625648]],\n",
       " \n",
       "        [[ 0.03869611,  0.04017606,  0.04030259],\n",
       "         [-0.06953381, -0.01844844, -0.06165981],\n",
       "         [-0.02936589, -0.06272729, -0.02668123]],\n",
       " \n",
       "        [[-0.05216966, -0.06220633,  0.0761277 ],\n",
       "         [-0.06820789, -0.03033724,  0.04960292],\n",
       "         [-0.03854988, -0.04462521, -0.03326848]],\n",
       " \n",
       "        [[ 0.02067475, -0.08206213, -0.06776331],\n",
       "         [ 0.06087831, -0.0278049 ,  0.04511631],\n",
       "         [ 0.03024448,  0.07056317,  0.0716991 ]],\n",
       " \n",
       "        [[ 0.04043184, -0.02021146,  0.01437125],\n",
       "         [ 0.02010776, -0.00671   ,  0.06595849],\n",
       "         [-0.00669971, -0.02107449, -0.01281691]]], dtype=float32), bias=0.0018166883, layer=1, neuron_number=1, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.04758967,  0.02671284, -0.01860731],\n",
       "         [-0.07604957, -0.07113054, -0.04190777],\n",
       "         [-0.03978162,  0.05554759, -0.03959197]],\n",
       " \n",
       "        [[ 0.04147943,  0.05737281, -0.00047666],\n",
       "         [-0.0560974 , -0.03317233,  0.03673235],\n",
       "         [ 0.02743415,  0.04941558, -0.02221454]],\n",
       " \n",
       "        [[ 0.03233167, -0.06127565, -0.02903447],\n",
       "         [-0.0257215 ,  0.02551182, -0.0396103 ],\n",
       "         [ 0.02951259, -0.04595336, -0.0251004 ]],\n",
       " \n",
       "        [[-0.02296169,  0.07400437, -0.05781936],\n",
       "         [-0.06554038,  0.03020501,  0.04359291],\n",
       "         [ 0.00952151,  0.06108761,  0.04513072]],\n",
       " \n",
       "        [[ 0.06209369,  0.07779481,  0.04866156],\n",
       "         [-0.00300976,  0.06279552,  0.05286863],\n",
       "         [ 0.01761463, -0.03049699, -0.0396543 ]],\n",
       " \n",
       "        [[-0.05422893,  0.05017902,  0.01666172],\n",
       "         [-0.06212598, -0.00280307, -0.05075604],\n",
       "         [ 0.05051413,  0.08287808, -0.024957  ]],\n",
       " \n",
       "        [[-0.01628347,  0.04027067,  0.06632733],\n",
       "         [-0.08079299,  0.07926045,  0.06918436],\n",
       "         [-0.01387307, -0.06076542,  0.05000154]],\n",
       " \n",
       "        [[ 0.0345926 ,  0.05508784,  0.05415265],\n",
       "         [ 0.0170566 ,  0.04886763, -0.08092336],\n",
       "         [-0.07331038,  0.0088456 , -0.0137296 ]],\n",
       " \n",
       "        [[-0.04126905, -0.07601094,  0.04827868],\n",
       "         [ 0.00243877,  0.07027302, -0.03732681],\n",
       "         [-0.03408765,  0.02329364,  0.03681795]],\n",
       " \n",
       "        [[ 0.02514602,  0.01835075,  0.04023527],\n",
       "         [ 0.05938931, -0.00838987, -0.00519473],\n",
       "         [-0.06224494, -0.04371786, -0.04064449]],\n",
       " \n",
       "        [[ 0.02967271, -0.04002119,  0.06960282],\n",
       "         [-0.07109284,  0.07432826, -0.06000119],\n",
       "         [ 0.05304962,  0.06676193,  0.00483313]],\n",
       " \n",
       "        [[-0.04129172,  0.02491417, -0.08215067],\n",
       "         [-0.05956133, -0.07682633,  0.03122002],\n",
       "         [ 0.04292314,  0.03967657, -0.04205445]],\n",
       " \n",
       "        [[ 0.01531225, -0.06126783,  0.05948822],\n",
       "         [-0.05401168, -0.08042803,  0.00946022],\n",
       "         [ 0.034463  ,  0.04766597, -0.04230696]],\n",
       " \n",
       "        [[-0.0357386 , -0.05315805,  0.03501653],\n",
       "         [-0.07899607, -0.03190435,  0.07381105],\n",
       "         [-0.07404663, -0.02694131,  0.01567582]],\n",
       " \n",
       "        [[-0.05735072,  0.00162367,  0.05435799],\n",
       "         [ 0.0685738 ,  0.06810986, -0.02422982],\n",
       "         [ 0.05311377,  0.06061054,  0.04255629]],\n",
       " \n",
       "        [[-0.0444902 ,  0.00880353,  0.03907747],\n",
       "         [-0.01148509, -0.00059624,  0.0354594 ],\n",
       "         [ 0.01734024,  0.06100202, -0.01304375]],\n",
       " \n",
       "        [[-0.0446777 ,  0.03133743,  0.01290348],\n",
       "         [ 0.00773584,  0.06361085,  0.02392082],\n",
       "         [-0.0480802 ,  0.03358138,  0.0008329 ]],\n",
       " \n",
       "        [[-0.08066748,  0.07700647,  0.05326228],\n",
       "         [-0.06059756, -0.01149392,  0.06883312],\n",
       "         [ 0.07955485, -0.00891227, -0.03372769]],\n",
       " \n",
       "        [[ 0.04617179, -0.05448413, -0.0770167 ],\n",
       "         [ 0.06254535, -0.01644247, -0.01213529],\n",
       "         [-0.05384081,  0.07257503,  0.03084468]],\n",
       " \n",
       "        [[ 0.05955507,  0.01717628,  0.05356215],\n",
       "         [ 0.01691837, -0.05645933,  0.07237108],\n",
       "         [-0.04118602, -0.01230703,  0.05399341]],\n",
       " \n",
       "        [[ 0.05352256,  0.03910646, -0.04907137],\n",
       "         [-0.07671789, -0.00516234, -0.02990187],\n",
       "         [-0.00635351, -0.0127447 , -0.03621124]],\n",
       " \n",
       "        [[-0.03777837,  0.02502368, -0.0086016 ],\n",
       "         [ 0.03815497,  0.0124338 ,  0.07727248],\n",
       "         [-0.04602475, -0.0377461 , -0.01459166]],\n",
       " \n",
       "        [[-0.01877732,  0.00904578,  0.04091369],\n",
       "         [ 0.06477871,  0.00018106,  0.04362707],\n",
       "         [ 0.071794  , -0.05858451,  0.07749975]],\n",
       " \n",
       "        [[ 0.03250452,  0.04334139, -0.03215666],\n",
       "         [-0.01096528, -0.07346314, -0.00219816],\n",
       "         [-0.03786331, -0.04219395, -0.00647747]],\n",
       " \n",
       "        [[-0.04558611, -0.07991435,  0.01055965],\n",
       "         [ 0.04204069, -0.07628575, -0.00295791],\n",
       "         [-0.0330259 ,  0.04419915, -0.04721508]],\n",
       " \n",
       "        [[ 0.05569672,  0.06001391,  0.01757645],\n",
       "         [ 0.07279937, -0.0192739 , -0.05952866],\n",
       "         [ 0.03506254, -0.07739489,  0.01140236]],\n",
       " \n",
       "        [[-0.02463208, -0.048553  ,  0.00314595],\n",
       "         [ 0.02879337, -0.07891062, -0.08307697],\n",
       "         [ 0.08036465,  0.07371305,  0.00954061]],\n",
       " \n",
       "        [[ 0.06228995,  0.02940559,  0.02661195],\n",
       "         [-0.00278608,  0.01554078, -0.04087089],\n",
       "         [ 0.05798139,  0.02639557,  0.0340588 ]],\n",
       " \n",
       "        [[-0.07470695,  0.04279096,  0.03154243],\n",
       "         [-0.01705003, -0.00356155,  0.06894106],\n",
       "         [-0.07375436, -0.0623032 , -0.07295199]],\n",
       " \n",
       "        [[ 0.06456966, -0.00481274, -0.06490851],\n",
       "         [-0.05693621,  0.04154525, -0.01634373],\n",
       "         [ 0.07410508,  0.01531911,  0.00642256]],\n",
       " \n",
       "        [[ 0.00460478, -0.07578171, -0.03004234],\n",
       "         [-0.01128749, -0.06139329, -0.03862793],\n",
       "         [-0.02983109, -0.00258705,  0.06612409]],\n",
       " \n",
       "        [[ 0.0638785 , -0.01153597, -0.02489459],\n",
       "         [ 0.06555779,  0.01809844, -0.0102406 ],\n",
       "         [ 0.06594317, -0.02844621, -0.03969034]]], dtype=float32), bias=8.8549277e-05, layer=1, neuron_number=2, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.03223502, -0.0698775 , -0.03862591],\n",
       "         [ 0.07628719, -0.02257577,  0.01175626],\n",
       "         [-0.02357433,  0.03801386, -0.00035922]],\n",
       " \n",
       "        [[-0.04056541,  0.06235528,  0.02064167],\n",
       "         [ 0.06364628,  0.0364765 ,  0.00716819],\n",
       "         [ 0.05627497, -0.06770083, -0.02762313]],\n",
       " \n",
       "        [[-0.05357543,  0.02811523, -0.05007204],\n",
       "         [ 0.05083047,  0.02977644,  0.0531017 ],\n",
       "         [ 0.05644258,  0.00790271,  0.04551748]],\n",
       " \n",
       "        [[ 0.04785541,  0.00789936,  0.03006739],\n",
       "         [-0.01644525, -0.07575924,  0.00866678],\n",
       "         [ 0.06027833, -0.03278948, -0.0216901 ]],\n",
       " \n",
       "        [[ 0.05565578, -0.02194595, -0.05484838],\n",
       "         [ 0.072601  , -0.04422402,  0.06961957],\n",
       "         [ 0.07782404,  0.0755951 , -0.05382825]],\n",
       " \n",
       "        [[-0.05935981, -0.07014205,  0.06594532],\n",
       "         [-0.00090099, -0.03688596,  0.0507585 ],\n",
       "         [ 0.00412933, -0.07337102, -0.06462052]],\n",
       " \n",
       "        [[-0.01231361,  0.01248967, -0.04228158],\n",
       "         [ 0.06372804,  0.07134517, -0.06075345],\n",
       "         [ 0.0803916 , -0.03170342,  0.00438297]],\n",
       " \n",
       "        [[ 0.08043608, -0.03317649, -0.01909985],\n",
       "         [ 0.04608376,  0.0534139 ,  0.05184526],\n",
       "         [-0.04137917, -0.02985906, -0.05997809]],\n",
       " \n",
       "        [[ 0.02452038, -0.07640423,  0.06879961],\n",
       "         [ 0.07103946, -0.08163932, -0.03659216],\n",
       "         [-0.04297508,  0.02248346,  0.0828011 ]],\n",
       " \n",
       "        [[ 0.0464074 , -0.07262765, -0.07718432],\n",
       "         [-0.06045584,  0.04879614,  0.05201755],\n",
       "         [ 0.07223228, -0.0188255 , -0.02915397]],\n",
       " \n",
       "        [[-0.06926268,  0.05708336, -0.03483779],\n",
       "         [-0.04134245, -0.01929579,  0.04909511],\n",
       "         [-0.0591642 , -0.02283095,  0.00068255]],\n",
       " \n",
       "        [[ 0.0328057 ,  0.01681769,  0.04338787],\n",
       "         [ 0.04748332,  0.06911434, -0.07443235],\n",
       "         [ 0.01126927, -0.08352612, -0.03160358]],\n",
       " \n",
       "        [[ 0.06173187,  0.05024087, -0.07919727],\n",
       "         [ 0.04764912,  0.06347781,  0.08108761],\n",
       "         [-0.02908257, -0.06987176,  0.04472365]],\n",
       " \n",
       "        [[ 0.0674631 ,  0.0016429 , -0.07472659],\n",
       "         [ 0.01437343, -0.00910161,  0.0457998 ],\n",
       "         [ 0.06027851,  0.04437675,  0.04354737]],\n",
       " \n",
       "        [[ 0.06253215,  0.07596026, -0.08359271],\n",
       "         [-0.06291652,  0.04182676, -0.08315976],\n",
       "         [-0.02715498,  0.06394663, -0.04069005]],\n",
       " \n",
       "        [[ 0.05685175,  0.06897078,  0.07439747],\n",
       "         [ 0.06091168, -0.00518271, -0.00340904],\n",
       "         [ 0.05773469,  0.02496458,  0.05086291]],\n",
       " \n",
       "        [[-0.07025439, -0.07261705,  0.04976896],\n",
       "         [ 0.04515549, -0.0618187 , -0.04344746],\n",
       "         [-0.07056723,  0.01771996,  0.07725906]],\n",
       " \n",
       "        [[-0.06506468, -0.02514243,  0.01836133],\n",
       "         [ 0.03881058,  0.06746758, -0.03383435],\n",
       "         [ 0.06886418, -0.02763492, -0.0265844 ]],\n",
       " \n",
       "        [[-0.02599437, -0.0669551 , -0.07541519],\n",
       "         [-0.04112652, -0.04407959, -0.04145361],\n",
       "         [-0.07462239,  0.053122  ,  0.04941736]],\n",
       " \n",
       "        [[-0.00162281, -0.01423337, -0.07830796],\n",
       "         [ 0.07409427,  0.07519487, -0.0646669 ],\n",
       "         [-0.03587937,  0.01478542,  0.07400336]],\n",
       " \n",
       "        [[ 0.01399364,  0.0055967 , -0.06925291],\n",
       "         [-0.03459316,  0.04952066, -0.0140183 ],\n",
       "         [ 0.03980337,  0.04207029, -0.02761443]],\n",
       " \n",
       "        [[-0.05071343, -0.06656564, -0.06319954],\n",
       "         [-0.07332642, -0.02837264, -0.02382684],\n",
       "         [-0.02853735, -0.0697502 ,  0.02218686]],\n",
       " \n",
       "        [[-0.03326518,  0.02053423, -0.07407381],\n",
       "         [-0.05735674, -0.06204389,  0.07004608],\n",
       "         [ 0.02019917, -0.0036861 , -0.04114513]],\n",
       " \n",
       "        [[-0.07888842,  0.03893666, -0.01897881],\n",
       "         [ 0.0003921 ,  0.06409254,  0.04455426],\n",
       "         [-0.01236227, -0.07118152, -0.03798353]],\n",
       " \n",
       "        [[ 0.0557789 ,  0.0054799 , -0.00604492],\n",
       "         [-0.02335617,  0.00283077,  0.01186882],\n",
       "         [ 0.06450583, -0.0807866 ,  0.07300217]],\n",
       " \n",
       "        [[ 0.01590107, -0.04442432, -0.04790583],\n",
       "         [ 0.00357425, -0.06648702, -0.02259276],\n",
       "         [ 0.02413482, -0.04941269,  0.07159534]],\n",
       " \n",
       "        [[ 0.07199154,  0.06108004,  0.04808689],\n",
       "         [ 0.07487327,  0.00278418, -0.06348576],\n",
       "         [ 0.04973773, -0.05977691, -0.0148877 ]],\n",
       " \n",
       "        [[-0.06111084,  0.00302723, -0.00909199],\n",
       "         [-0.07775006,  0.07894334,  0.04150342],\n",
       "         [-0.01578144,  0.03314536,  0.01822392]],\n",
       " \n",
       "        [[-0.07505818, -0.02251037, -0.03907448],\n",
       "         [ 0.05893758,  0.07248487, -0.00791892],\n",
       "         [ 0.07966948,  0.08463044,  0.04972101]],\n",
       " \n",
       "        [[-0.05128812, -0.03290041,  0.05875409],\n",
       "         [ 0.04866754, -0.01451235,  0.05410667],\n",
       "         [ 0.06702551, -0.07761557,  0.07115239]],\n",
       " \n",
       "        [[-0.07579455,  0.03790979, -0.06469046],\n",
       "         [-0.00795168, -0.05558692, -0.05734866],\n",
       "         [ 0.0759877 ,  0.00212131, -0.08068818]],\n",
       " \n",
       "        [[ 0.05712239, -0.01859089, -0.0210588 ],\n",
       "         [ 0.06032452,  0.02599634,  0.06022328],\n",
       "         [-0.01349146, -0.05159932, -0.05493637]]], dtype=float32), bias=0.00027961633, layer=1, neuron_number=3, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.05634762, -0.07284738, -0.02025983],\n",
       "         [-0.05438886, -0.05851586, -0.07444122],\n",
       "         [ 0.00268836,  0.01675983, -0.00100533]],\n",
       " \n",
       "        [[-0.03687698,  0.03016302,  0.03997339],\n",
       "         [ 0.04405075,  0.00318386, -0.06917919],\n",
       "         [-0.00208123,  0.03477299,  0.0681681 ]],\n",
       " \n",
       "        [[ 0.05067278, -0.04349975,  0.00394765],\n",
       "         [ 0.03566932,  0.06439991, -0.00554376],\n",
       "         [-0.00360927, -0.01267552,  0.06362842]],\n",
       " \n",
       "        [[ 0.03126509, -0.08046395, -0.05279448],\n",
       "         [ 0.07884999,  0.02436291, -0.07869453],\n",
       "         [ 0.04746328,  0.00433925,  0.00909413]],\n",
       " \n",
       "        [[-0.06461974, -0.01800809, -0.02942656],\n",
       "         [ 0.07046831, -0.06921075,  0.04313613],\n",
       "         [ 0.05389578,  0.06065509,  0.04751632]],\n",
       " \n",
       "        [[-0.04568364,  0.06110302,  0.080164  ],\n",
       "         [-0.00024662,  0.04487232, -0.07300153],\n",
       "         [ 0.03832925,  0.0271286 , -0.05130436]],\n",
       " \n",
       "        [[-0.04613052,  0.00807822, -0.0686533 ],\n",
       "         [-0.005765  , -0.05603009,  0.02352207],\n",
       "         [-0.00956262, -0.02347377, -0.0165592 ]],\n",
       " \n",
       "        [[-0.02191502, -0.05167958,  0.04376627],\n",
       "         [ 0.042098  ,  0.03499639, -0.04142645],\n",
       "         [ 0.07044073,  0.02165771,  0.06685162]],\n",
       " \n",
       "        [[ 0.02068099, -0.07903182, -0.04754764],\n",
       "         [ 0.03101757,  0.04689323, -0.07939599],\n",
       "         [ 0.03145338, -0.01406392,  0.02725521]],\n",
       " \n",
       "        [[ 0.05912055,  0.07889185, -0.01454446],\n",
       "         [-0.04574756,  0.0579995 ,  0.01083911],\n",
       "         [ 0.0436332 ,  0.04724914,  0.01211559]],\n",
       " \n",
       "        [[ 0.02220687,  0.06982346,  0.02748168],\n",
       "         [-0.02261113, -0.00366935, -0.05850004],\n",
       "         [ 0.01905487,  0.07689848, -0.01759348]],\n",
       " \n",
       "        [[ 0.03537372,  0.01582108,  0.05979962],\n",
       "         [ 0.02390109, -0.02894493, -0.07748953],\n",
       "         [-0.05095917, -0.04642729,  0.03617575]],\n",
       " \n",
       "        [[-0.04259725, -0.04262893,  0.03948765],\n",
       "         [ 0.00732486,  0.00177659,  0.02729576],\n",
       "         [-0.02269046, -0.08073179,  0.04841091]],\n",
       " \n",
       "        [[-0.060748  ,  0.02731827, -0.04147928],\n",
       "         [-0.03429028,  0.00160805, -0.05439037],\n",
       "         [ 0.0543773 ,  0.06058767, -0.0434073 ]],\n",
       " \n",
       "        [[-0.02283533, -0.03028924,  0.05056694],\n",
       "         [-0.02436915,  0.02522804, -0.04398285],\n",
       "         [ 0.06241358,  0.07740851, -0.05749054]],\n",
       " \n",
       "        [[ 0.05679565, -0.03857522,  0.04020407],\n",
       "         [ 0.0812664 , -0.04557406, -0.03634417],\n",
       "         [ 0.07711542, -0.00914636, -0.03941315]],\n",
       " \n",
       "        [[ 0.05907247,  0.07437944,  0.03673745],\n",
       "         [-0.06276388,  0.07028813,  0.06857979],\n",
       "         [-0.01967162,  0.06220478, -0.0377298 ]],\n",
       " \n",
       "        [[ 0.06480096,  0.011132  ,  0.03671933],\n",
       "         [ 0.04899509, -0.06086654, -0.00542877],\n",
       "         [ 0.06308365,  0.0030175 ,  0.079314  ]],\n",
       " \n",
       "        [[-0.04874504,  0.03281553, -0.02576262],\n",
       "         [ 0.04691852, -0.0255142 ,  0.03706939],\n",
       "         [ 0.00055545, -0.01844296,  0.08035973]],\n",
       " \n",
       "        [[-0.00086279, -0.06090698,  0.04862944],\n",
       "         [ 0.00437387, -0.06670085, -0.03675992],\n",
       "         [-0.00186087, -0.07007469,  0.03014943]],\n",
       " \n",
       "        [[ 0.0777632 ,  0.04128054, -0.0330008 ],\n",
       "         [-0.08079044,  0.03756199,  0.00904973],\n",
       "         [-0.01319633,  0.05303298, -0.05605783]],\n",
       " \n",
       "        [[-0.06297927,  0.07552685,  0.06853265],\n",
       "         [-0.03808613,  0.06508344, -0.03053579],\n",
       "         [-0.00413285, -0.00984705, -0.04007903]],\n",
       " \n",
       "        [[-0.08262175,  0.07626834,  0.04652162],\n",
       "         [ 0.01013875,  0.04689861, -0.00884671],\n",
       "         [ 0.06065926,  0.0009054 , -0.03787518]],\n",
       " \n",
       "        [[ 0.04322611,  0.07528284, -0.02799691],\n",
       "         [ 0.05113123, -0.04832811,  0.06613584],\n",
       "         [-0.01739378,  0.08225565,  0.07327615]],\n",
       " \n",
       "        [[-0.03632271,  0.0693344 , -0.0292613 ],\n",
       "         [-0.02409781,  0.01414212,  0.01779465],\n",
       "         [-0.0533543 , -0.06288739, -0.07636899]],\n",
       " \n",
       "        [[ 0.01425693,  0.00528087,  0.00958589],\n",
       "         [ 0.07378475,  0.044181  , -0.06142037],\n",
       "         [-0.03946894, -0.07234991,  0.04596701]],\n",
       " \n",
       "        [[ 0.05576884,  0.03364731,  0.01085074],\n",
       "         [-0.04538003,  0.01671902, -0.05507531],\n",
       "         [ 0.07297587, -0.07384183,  0.06478734]],\n",
       " \n",
       "        [[ 0.0361795 ,  0.0267707 , -0.01963918],\n",
       "         [ 0.04029922, -0.00862299, -0.02774351],\n",
       "         [-0.00546209,  0.00401702, -0.07291327]],\n",
       " \n",
       "        [[ 0.02397479, -0.06620472, -0.08339277],\n",
       "         [-0.04191262, -0.04093001, -0.07091629],\n",
       "         [ 0.051063  , -0.03575806,  0.0754972 ]],\n",
       " \n",
       "        [[ 0.01104173,  0.08279633,  0.05160623],\n",
       "         [-0.05387252, -0.00746703, -0.00904579],\n",
       "         [-0.04676349,  0.03613266,  0.0723828 ]],\n",
       " \n",
       "        [[ 0.04504501,  0.01428708, -0.03018619],\n",
       "         [-0.07815575,  0.01898319, -0.05574637],\n",
       "         [-0.0651156 , -0.01771332,  0.00426563]],\n",
       " \n",
       "        [[ 0.0051723 ,  0.00136374, -0.04401501],\n",
       "         [-0.01786645,  0.03837781,  0.04141932],\n",
       "         [-0.00597327, -0.0015694 , -0.04104776]]], dtype=float32), bias=9.5461408e-05, layer=1, neuron_number=4, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.00642024, -0.04694281, -0.05223019],\n",
       "         [ 0.05027759,  0.06746627, -0.06834303],\n",
       "         [-0.05145498, -0.01639559, -0.03271421]],\n",
       " \n",
       "        [[-0.01360341, -0.02959916,  0.01314735],\n",
       "         [ 0.04735473,  0.02084234,  0.05508541],\n",
       "         [-0.06831446,  0.02995624, -0.06193553]],\n",
       " \n",
       "        [[ 0.00115129,  0.06891427, -0.00347308],\n",
       "         [ 0.04083812, -0.01135203, -0.01490993],\n",
       "         [ 0.02788062, -0.05509651, -0.03696006]],\n",
       " \n",
       "        [[-0.07103463,  0.08163672,  0.05761083],\n",
       "         [ 0.01041373, -0.07932173, -0.00819637],\n",
       "         [ 0.06943319,  0.07431048,  0.06848653]],\n",
       " \n",
       "        [[ 0.08204678, -0.04235712,  0.07670023],\n",
       "         [-0.0086997 , -0.03737439,  0.0657698 ],\n",
       "         [ 0.00936615,  0.04499431, -0.07188513]],\n",
       " \n",
       "        [[ 0.02583219, -0.00161   , -0.05647898],\n",
       "         [-0.04039516,  0.06804287,  0.04137225],\n",
       "         [-0.01206886, -0.00924368,  0.00772284]],\n",
       " \n",
       "        [[ 0.07774775,  0.05411468,  0.03521424],\n",
       "         [-0.01466345, -0.0089328 ,  0.02495603],\n",
       "         [-0.05885495, -0.08031839, -0.07564687]],\n",
       " \n",
       "        [[ 0.00691772, -0.07240828,  0.04118471],\n",
       "         [-0.07040139,  0.00681396, -0.02625592],\n",
       "         [-0.06212203, -0.06403563,  0.03765882]],\n",
       " \n",
       "        [[ 0.05572842,  0.03305873,  0.04068319],\n",
       "         [ 0.02083235, -0.01311086, -0.03436205],\n",
       "         [ 0.07566902, -0.08292291, -0.07365559]],\n",
       " \n",
       "        [[ 0.01668171, -0.02977584,  0.0568828 ],\n",
       "         [-0.04113187,  0.06099538,  0.00139844],\n",
       "         [-0.03609502,  0.08026133,  0.07491965]],\n",
       " \n",
       "        [[-0.01475321,  0.04905382, -0.069377  ],\n",
       "         [-0.06938679, -0.01933536, -0.05611971],\n",
       "         [ 0.05014769,  0.05203782,  0.04313228]],\n",
       " \n",
       "        [[-0.0815859 , -0.02876595, -0.06807466],\n",
       "         [-0.0567009 , -0.06806363,  0.0513128 ],\n",
       "         [ 0.08059561,  0.02312271, -0.05316012]],\n",
       " \n",
       "        [[-0.05359026, -0.0019487 , -0.01176674],\n",
       "         [ 0.00436641, -0.08136098,  0.01021019],\n",
       "         [ 0.00467005,  0.05103295, -0.05233816]],\n",
       " \n",
       "        [[-0.06174983,  0.04545421,  0.04416708],\n",
       "         [ 0.07169136,  0.02438612, -0.04612552],\n",
       "         [ 0.00299983,  0.018781  ,  0.06528233]],\n",
       " \n",
       "        [[-0.02843624, -0.05718777, -0.07841618],\n",
       "         [-0.0325366 ,  0.02455902, -0.04543066],\n",
       "         [ 0.07043467,  0.034454  , -0.06637375]],\n",
       " \n",
       "        [[ 0.07679613,  0.00741365,  0.05198428],\n",
       "         [-0.0333834 ,  0.03854809, -0.06716216],\n",
       "         [-0.01131023, -0.0121395 , -0.02706387]],\n",
       " \n",
       "        [[ 0.00810785,  0.04830796, -0.00118027],\n",
       "         [ 0.04350791, -0.06756754, -0.05273435],\n",
       "         [ 0.01286762,  0.05993294, -0.02315168]],\n",
       " \n",
       "        [[-0.00274889, -0.07769483, -0.03385104],\n",
       "         [ 0.03794217, -0.02077392,  0.01290336],\n",
       "         [-0.03809895, -0.00324883, -0.05937039]],\n",
       " \n",
       "        [[-0.0247797 , -0.00877402, -0.03287951],\n",
       "         [-0.01048747,  0.0619736 , -0.01394982],\n",
       "         [-0.0453295 ,  0.05828382,  0.08317542]],\n",
       " \n",
       "        [[ 0.03672995,  0.08329219, -0.0701998 ],\n",
       "         [-0.05858403, -0.05302393,  0.04645335],\n",
       "         [ 0.07045791, -0.07405794,  0.0359956 ]],\n",
       " \n",
       "        [[-0.00987071, -0.05268295,  0.02708367],\n",
       "         [ 0.0411733 , -0.00365605, -0.04194124],\n",
       "         [ 0.03132259, -0.01863808, -0.04634325]],\n",
       " \n",
       "        [[ 0.02878508,  0.04093999,  0.07594851],\n",
       "         [ 0.02115207, -0.00853982,  0.03160898],\n",
       "         [ 0.06275441, -0.05786851, -0.02559355]],\n",
       " \n",
       "        [[ 0.0032549 ,  0.00932082, -0.00549121],\n",
       "         [-0.04176202, -0.05343098,  0.03965269],\n",
       "         [-0.07979249,  0.07650855,  0.07517551]],\n",
       " \n",
       "        [[ 0.07511506, -0.07204276, -0.06760408],\n",
       "         [-0.04713209, -0.05765157, -0.00985507],\n",
       "         [-0.07360236,  0.03403656,  0.01715559]],\n",
       " \n",
       "        [[ 0.03343923, -0.01307658,  0.00919084],\n",
       "         [ 0.04094331,  0.06968944, -0.07655461],\n",
       "         [-0.00705262,  0.01165988, -0.02513309]],\n",
       " \n",
       "        [[ 0.02043173, -0.01534235, -0.03816336],\n",
       "         [-0.04790649, -0.05037753, -0.07253893],\n",
       "         [ 0.08030313,  0.0225587 , -0.00993692]],\n",
       " \n",
       "        [[-0.03249612,  0.05374552,  0.01250298],\n",
       "         [-0.01681132,  0.08189327,  0.04149309],\n",
       "         [ 0.05302934, -0.0082959 ,  0.05075061]],\n",
       " \n",
       "        [[-0.0721522 , -0.03137847, -0.05463809],\n",
       "         [-0.01814612, -0.07386829,  0.06818759],\n",
       "         [-0.07891321,  0.0506753 , -0.02935319]],\n",
       " \n",
       "        [[ 0.0595229 , -0.06404467,  0.04424136],\n",
       "         [-0.02745162,  0.03699319,  0.06132064],\n",
       "         [ 0.05123061, -0.02864167, -0.05093607]],\n",
       " \n",
       "        [[ 0.0117935 , -0.08249139, -0.0345667 ],\n",
       "         [ 0.07079599,  0.02845239, -0.07433525],\n",
       "         [-0.07598829, -0.06593534, -0.06687633]],\n",
       " \n",
       "        [[ 0.00239191, -0.04357735, -0.03992542],\n",
       "         [ 0.06162003, -0.0042902 ,  0.08028701],\n",
       "         [ 0.07347476,  0.00927376,  0.00797627]],\n",
       " \n",
       "        [[-0.04043532, -0.0079431 ,  0.03156611],\n",
       "         [ 0.06157795, -0.03978621,  0.05513015],\n",
       "         [-0.05034323,  0.03911274,  0.05471206]]], dtype=float32), bias=0.0011878726, layer=1, neuron_number=5, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ -8.25430676e-02,   3.94601263e-02,  -2.97509357e-02],\n",
       "         [ -1.18233217e-02,  -3.29716578e-02,   4.55920994e-02],\n",
       "         [ -1.64851863e-02,  -6.51141331e-02,   1.09885754e-02]],\n",
       " \n",
       "        [[ -5.87094203e-02,   6.16398901e-02,   5.56244738e-02],\n",
       "         [ -5.21400683e-02,  -4.94426340e-02,  -5.90594970e-02],\n",
       "         [ -3.77736799e-03,  -1.32120969e-02,   1.19093899e-02]],\n",
       " \n",
       "        [[  5.69804721e-02,  -6.33033067e-02,  -4.22792472e-02],\n",
       "         [ -6.17664382e-02,   2.48662755e-03,  -1.16037643e-02],\n",
       "         [  6.94088042e-02,   5.43405823e-02,  -1.92708001e-02]],\n",
       " \n",
       "        [[ -1.17849251e-02,   6.77333996e-02,   3.20030116e-02],\n",
       "         [ -1.72843318e-02,  -2.87684966e-02,  -7.57731050e-02],\n",
       "         [  6.59661666e-02,  -9.15541127e-03,  -4.64484654e-02]],\n",
       " \n",
       "        [[ -9.49321315e-03,   6.57482147e-02,  -3.07601877e-02],\n",
       "         [ -1.05913857e-03,  -1.56429727e-02,   3.96978334e-02],\n",
       "         [  5.80256134e-02,   1.41068781e-02,   7.32178858e-04]],\n",
       " \n",
       "        [[ -5.66024333e-02,   2.27345638e-02,  -2.16273055e-03],\n",
       "         [ -5.86765297e-02,  -3.70979011e-02,  -3.27608176e-02],\n",
       "         [ -1.21520599e-02,   7.94409737e-02,  -7.22567886e-02]],\n",
       " \n",
       "        [[ -6.75165001e-03,   5.79285026e-02,  -4.56259493e-03],\n",
       "         [  1.82109140e-02,  -1.22757712e-02,   1.39640942e-02],\n",
       "         [  6.69884384e-02,   3.15361209e-02,  -4.22124863e-02]],\n",
       " \n",
       "        [[  4.98717837e-02,   9.60276579e-04,   6.23425655e-02],\n",
       "         [  1.16550252e-02,  -1.26923772e-03,  -7.34939203e-02],\n",
       "         [ -6.30742498e-03,   6.13566600e-02,   1.81365870e-02]],\n",
       " \n",
       "        [[  8.21606442e-02,   2.89140232e-02,  -8.47440585e-02],\n",
       "         [  4.35254648e-02,   5.01264855e-02,   3.65819708e-02],\n",
       "         [ -4.43133451e-02,  -2.93104500e-02,   2.88160797e-02]],\n",
       " \n",
       "        [[  7.73541406e-02,  -8.01675320e-02,   4.92334776e-02],\n",
       "         [  9.90401168e-05,   2.34911982e-02,   6.12804741e-02],\n",
       "         [ -5.82418218e-02,  -3.22095118e-02,   1.36369430e-02]],\n",
       " \n",
       "        [[ -2.66381055e-02,  -5.07587194e-02,   5.57215214e-02],\n",
       "         [  4.17312002e-03,   5.82911074e-02,   2.29482725e-02],\n",
       "         [  1.98553279e-02,   4.90755402e-02,   5.20558618e-02]],\n",
       " \n",
       "        [[  3.65161076e-02,  -6.57641934e-03,  -4.23405878e-02],\n",
       "         [  4.07145321e-02,   4.52991612e-02,   2.58132517e-02],\n",
       "         [  6.59120083e-02,   3.90094109e-02,   2.46165358e-02]],\n",
       " \n",
       "        [[  5.93036897e-02,  -5.92561662e-02,   5.71508519e-02],\n",
       "         [ -2.06077900e-02,  -1.30507266e-02,   5.63975014e-02],\n",
       "         [ -5.01657389e-02,   7.86705688e-02,  -3.59372161e-02]],\n",
       " \n",
       "        [[ -4.64480780e-02,  -5.03491377e-03,  -8.03638101e-02],\n",
       "         [ -7.91864470e-02,   2.50872187e-02,   3.79662551e-02],\n",
       "         [ -7.60831833e-02,   6.13989821e-03,  -6.40882924e-02]],\n",
       " \n",
       "        [[ -4.42132540e-02,   1.55367155e-03,   8.26502033e-03],\n",
       "         [ -5.33042066e-02,   3.37142088e-02,   4.45154533e-02],\n",
       "         [  2.63558738e-02,   6.54415637e-02,   2.21957900e-02]],\n",
       " \n",
       "        [[  3.56132053e-02,  -6.33650720e-02,   4.36298400e-02],\n",
       "         [ -3.28445062e-02,  -8.00139606e-02,   6.40264377e-02],\n",
       "         [  4.50666957e-02,   8.15601498e-02,  -3.13686207e-04]],\n",
       " \n",
       "        [[ -5.55720665e-02,  -2.99722794e-02,   1.63545553e-02],\n",
       "         [ -3.24437860e-03,  -4.72621806e-02,   5.52907176e-02],\n",
       "         [  7.83875287e-02,   7.07957447e-02,  -4.02364917e-02]],\n",
       " \n",
       "        [[ -5.99688217e-02,   7.56260827e-02,  -7.67806917e-02],\n",
       "         [ -7.17279315e-02,   1.65806506e-02,  -2.16338746e-02],\n",
       "         [ -8.30619112e-02,  -2.74558179e-02,  -7.63368309e-02]],\n",
       " \n",
       "        [[ -2.02207342e-02,  -2.08305698e-02,  -5.81713347e-03],\n",
       "         [ -1.05875768e-02,   3.51153016e-02,  -5.00235222e-02],\n",
       "         [ -2.28205565e-02,  -2.86710206e-02,   2.13357788e-02]],\n",
       " \n",
       "        [[ -6.51107058e-02,   1.95826832e-02,  -2.94305701e-02],\n",
       "         [  8.63509532e-03,   6.36204258e-02,   4.58699502e-02],\n",
       "         [ -1.35373994e-04,   4.23826315e-02,   1.02235321e-02]],\n",
       " \n",
       "        [[ -4.78602014e-02,  -7.86580220e-02,  -9.67244711e-03],\n",
       "         [ -7.82859623e-02,   6.61760494e-02,   3.87610942e-02],\n",
       "         [ -6.44757375e-02,  -5.61288707e-02,   3.93777117e-02]],\n",
       " \n",
       "        [[  4.73016687e-02,   5.00018708e-03,  -5.14426678e-02],\n",
       "         [  6.19538948e-02,  -6.21885881e-02,   1.57680716e-02],\n",
       "         [  7.14349300e-02,  -7.10374266e-02,   6.15409836e-02]],\n",
       " \n",
       "        [[  3.59109528e-02,  -7.20102787e-02,   2.33893469e-02],\n",
       "         [ -8.34969729e-02,  -7.51435906e-02,  -3.20360661e-02],\n",
       "         [  8.04581214e-03,  -4.99914284e-04,  -3.61116696e-03]],\n",
       " \n",
       "        [[  7.96791837e-02,  -5.61802313e-02,   4.63939086e-02],\n",
       "         [  4.64746058e-02,   5.96354343e-02,  -2.07225420e-02],\n",
       "         [ -5.41075580e-02,   4.02984247e-02,  -4.15826589e-02]],\n",
       " \n",
       "        [[ -5.71775809e-02,  -6.85566366e-02,  -4.88989390e-02],\n",
       "         [ -3.30829509e-02,  -4.66919206e-02,  -2.12870128e-02],\n",
       "         [ -2.90865097e-02,   7.68135488e-02,   3.98110189e-02]],\n",
       " \n",
       "        [[ -6.39851168e-02,   4.14539054e-02,   5.58032282e-02],\n",
       "         [ -4.04569656e-02,  -2.80346186e-03,   2.18459480e-02],\n",
       "         [  2.48340331e-03,  -1.03487996e-02,  -6.47432357e-02]],\n",
       " \n",
       "        [[  1.61449276e-02,   5.66316471e-02,   4.69370931e-02],\n",
       "         [ -4.54190113e-02,   6.82245493e-02,   5.02680950e-02],\n",
       "         [ -5.60639650e-02,   6.81005046e-02,   4.81215827e-02]],\n",
       " \n",
       "        [[ -6.92387968e-02,   3.77344005e-02,   3.80875096e-02],\n",
       "         [  2.37637367e-02,   2.97409054e-02,  -4.60143164e-02],\n",
       "         [ -5.06073460e-02,   4.03572023e-02,   3.60089205e-02]],\n",
       " \n",
       "        [[  2.73055135e-04,  -5.20238541e-02,  -2.35205162e-02],\n",
       "         [  2.92117894e-02,  -1.62531137e-02,   3.64024416e-02],\n",
       "         [ -6.70754761e-02,  -6.01727627e-02,   4.69291508e-02]],\n",
       " \n",
       "        [[ -2.48931888e-02,  -1.73755130e-03,   2.24712323e-02],\n",
       "         [ -4.01767567e-02,  -6.05927520e-02,  -8.33965242e-02],\n",
       "         [  6.93077669e-02,  -2.88655218e-02,   2.95218937e-02]],\n",
       " \n",
       "        [[  7.64243007e-02,  -7.84811303e-02,   4.87368670e-05],\n",
       "         [  3.00349146e-02,   6.97346032e-02,  -9.34028346e-03],\n",
       "         [ -2.70110276e-02,  -1.63977481e-02,  -3.50303054e-02]],\n",
       " \n",
       "        [[  4.26156633e-02,   3.36300433e-02,  -2.57430729e-02],\n",
       "         [ -2.14717556e-02,  -6.40602410e-02,  -7.13919401e-02],\n",
       "         [  6.69626147e-02,   8.86942353e-03,   3.20336828e-03]]], dtype=float32), bias=-1.9195082e-05, layer=1, neuron_number=6, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.05453932, -0.05810631,  0.00574245],\n",
       "         [ 0.05183752,  0.06151648, -0.06482285],\n",
       "         [-0.07227746,  0.01343816,  0.04494709]],\n",
       " \n",
       "        [[ 0.00423435, -0.03954884,  0.02858847],\n",
       "         [-0.05069864, -0.00885594,  0.01586121],\n",
       "         [ 0.01632343, -0.00598538,  0.0044906 ]],\n",
       " \n",
       "        [[ 0.06726187,  0.00239911,  0.02005362],\n",
       "         [ 0.02727293, -0.02926273,  0.06585557],\n",
       "         [ 0.00939505,  0.02454219, -0.0662313 ]],\n",
       " \n",
       "        [[-0.02030959, -0.06971868,  0.05980706],\n",
       "         [ 0.00612884, -0.0303515 , -0.0218474 ],\n",
       "         [-0.04458305, -0.00241411, -0.05654234]],\n",
       " \n",
       "        [[ 0.08353293,  0.0807531 ,  0.02651608],\n",
       "         [-0.00713581,  0.04236612, -0.07234291],\n",
       "         [-0.03799415, -0.07664275, -0.02028452]],\n",
       " \n",
       "        [[-0.03060171,  0.06358328,  0.06591424],\n",
       "         [-0.00564011, -0.05000325, -0.01902599],\n",
       "         [ 0.05971703,  0.06902178,  0.05953576]],\n",
       " \n",
       "        [[ 0.01490856, -0.07684905,  0.05874983],\n",
       "         [-0.01623308,  0.01386791,  0.00633731],\n",
       "         [ 0.06391508,  0.0011772 ,  0.00283475]],\n",
       " \n",
       "        [[-0.04520075,  0.00688026,  0.05124471],\n",
       "         [ 0.00486057,  0.05074117,  0.03382311],\n",
       "         [ 0.02497962,  0.02176163,  0.00446838]],\n",
       " \n",
       "        [[-0.0148234 , -0.00654904, -0.04259623],\n",
       "         [-0.00955125,  0.01602161,  0.0201913 ],\n",
       "         [-0.07770561, -0.04665482, -0.05788871]],\n",
       " \n",
       "        [[ 0.03787488, -0.07711835, -0.00874703],\n",
       "         [ 0.01924906,  0.07464204,  0.02982079],\n",
       "         [ 0.0543629 , -0.0648685 ,  0.05258837]],\n",
       " \n",
       "        [[-0.03511599,  0.03503729, -0.02179577],\n",
       "         [-0.00173063,  0.03186519, -0.07952955],\n",
       "         [ 0.06934406,  0.05851103, -0.01260841]],\n",
       " \n",
       "        [[ 0.0511555 , -0.05244593, -0.05170515],\n",
       "         [-0.07786563,  0.07843735,  0.04104879],\n",
       "         [ 0.06252069, -0.04622118,  0.02140727]],\n",
       " \n",
       "        [[ 0.04989181, -0.00848451, -0.00688357],\n",
       "         [ 0.07174481,  0.08199015, -0.00491677],\n",
       "         [-0.01668101,  0.04999584,  0.03872003]],\n",
       " \n",
       "        [[-0.05462334, -0.00662808,  0.02068103],\n",
       "         [-0.06916796,  0.012803  , -0.05123914],\n",
       "         [ 0.00013633,  0.03419227, -0.06887583]],\n",
       " \n",
       "        [[ 0.03705249,  0.02041855, -0.060417  ],\n",
       "         [-0.01047804,  0.00625882, -0.07873002],\n",
       "         [-0.00327228, -0.08051963,  0.01233128]],\n",
       " \n",
       "        [[ 0.06747834, -0.02444518, -0.02247746],\n",
       "         [ 0.06762001,  0.05037432, -0.06479722],\n",
       "         [-0.0096561 , -0.05859428,  0.05819306]],\n",
       " \n",
       "        [[-0.01935661, -0.03933014, -0.06400458],\n",
       "         [-0.02125701, -0.0652895 , -0.03306884],\n",
       "         [ 0.04792865, -0.02730933,  0.03412187]],\n",
       " \n",
       "        [[ 0.0523307 ,  0.02025748,  0.08147685],\n",
       "         [ 0.08106978, -0.03196774,  0.02009755],\n",
       "         [-0.06501081,  0.0320832 ,  0.05206056]],\n",
       " \n",
       "        [[ 0.0014612 , -0.02946536,  0.01606517],\n",
       "         [ 0.07014108, -0.08224235,  0.07968091],\n",
       "         [ 0.03823524, -0.06129191, -0.02366566]],\n",
       " \n",
       "        [[ 0.05680547, -0.00453397, -0.07715782],\n",
       "         [ 0.05669545, -0.00494806, -0.01116014],\n",
       "         [-0.03370386,  0.07338872, -0.01917323]],\n",
       " \n",
       "        [[ 0.00658142, -0.03595244,  0.03026608],\n",
       "         [-0.07650123, -0.05258002,  0.06655943],\n",
       "         [-0.03917611,  0.07263788,  0.01836375]],\n",
       " \n",
       "        [[-0.06128997, -0.02272408,  0.02923413],\n",
       "         [ 0.03273741,  0.05934053, -0.04808423],\n",
       "         [ 0.0204027 , -0.03455007,  0.07612003]],\n",
       " \n",
       "        [[-0.02138618,  0.08158797, -0.03658489],\n",
       "         [ 0.02587147, -0.05897532, -0.04856219],\n",
       "         [-0.04129757,  0.06022776,  0.00340086]],\n",
       " \n",
       "        [[-0.0094622 , -0.07626221,  0.07898837],\n",
       "         [ 0.02091698,  0.07440612, -0.07787254],\n",
       "         [-0.06002381,  0.04936362, -0.06283287]],\n",
       " \n",
       "        [[ 0.08289915, -0.02312466, -0.06099536],\n",
       "         [ 0.01059643, -0.05803708, -0.07386303],\n",
       "         [-0.04160319,  0.0774501 ,  0.07944731]],\n",
       " \n",
       "        [[-0.08201556, -0.04827101,  0.08134996],\n",
       "         [ 0.07048093,  0.00674483, -0.06725013],\n",
       "         [ 0.0129095 , -0.06847855, -0.05402004]],\n",
       " \n",
       "        [[ 0.00794062, -0.06482963, -0.03971343],\n",
       "         [ 0.0553889 , -0.0037206 , -0.0476071 ],\n",
       "         [-0.07931741, -0.00324524, -0.01061494]],\n",
       " \n",
       "        [[-0.00647743,  0.07743144, -0.03762033],\n",
       "         [ 0.06773247, -0.03682096, -0.00570981],\n",
       "         [-0.08235558, -0.01732174,  0.076485  ]],\n",
       " \n",
       "        [[-0.04744136, -0.03185995, -0.07892624],\n",
       "         [-0.01225104, -0.07844362, -0.07823328],\n",
       "         [-0.07332452, -0.000675  , -0.07259265]],\n",
       " \n",
       "        [[-0.05453054,  0.00988074,  0.05799434],\n",
       "         [-0.02076714, -0.00145532,  0.0444803 ],\n",
       "         [ 0.02316996,  0.00030571,  0.05842984]],\n",
       " \n",
       "        [[-0.06269247,  0.01257706, -0.06951405],\n",
       "         [ 0.01621941, -0.07964657, -0.07593555],\n",
       "         [-0.03242306, -0.0383103 ,  0.0325168 ]],\n",
       " \n",
       "        [[-0.05727005, -0.00918712, -0.03770123],\n",
       "         [ 0.03515266,  0.02784033,  0.00799882],\n",
       "         [-0.08063851, -0.06845161, -0.03878399]]], dtype=float32), bias=0.00025714721, layer=1, neuron_number=7, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  6.61123171e-02,   7.48699456e-02,   6.64853379e-02],\n",
       "         [ -1.51099246e-02,  -3.36599611e-02,  -4.56678011e-02],\n",
       "         [ -1.50331073e-02,  -1.70306992e-02,   4.67476882e-02]],\n",
       " \n",
       "        [[ -6.58182502e-02,  -2.40259953e-02,   7.76043162e-03],\n",
       "         [ -5.74564040e-02,  -1.62095465e-02,   5.91942258e-02],\n",
       "         [  4.05148193e-02,  -1.65816322e-02,  -6.69859722e-02]],\n",
       " \n",
       "        [[  7.58288503e-02,   8.14174190e-02,  -3.14961895e-02],\n",
       "         [ -6.75470978e-02,  -4.33620885e-02,   7.04183802e-02],\n",
       "         [ -3.41965705e-02,  -4.13741432e-02,   3.18877213e-02]],\n",
       " \n",
       "        [[ -3.35576236e-02,  -6.06738590e-02,   6.87429830e-02],\n",
       "         [  2.97080856e-02,  -4.84281704e-02,   4.39156927e-02],\n",
       "         [ -6.45604804e-02,   2.92027500e-02,   8.42379220e-03]],\n",
       " \n",
       "        [[ -3.93064693e-02,   2.37505119e-02,  -1.71152700e-03],\n",
       "         [ -1.15090320e-02,  -6.09976873e-02,  -7.47730508e-02],\n",
       "         [ -4.21685986e-02,   7.97586143e-02,   3.68476696e-02]],\n",
       " \n",
       "        [[ -2.51125153e-02,  -6.59507811e-02,   7.38272294e-02],\n",
       "         [  6.28157631e-02,  -8.21694639e-03,  -2.67683305e-02],\n",
       "         [  6.35171384e-02,   1.69278379e-03,  -4.74098995e-02]],\n",
       " \n",
       "        [[  1.17405942e-02,   6.11609779e-02,   6.30729720e-02],\n",
       "         [  7.66836256e-02,   5.63044548e-02,   5.09611703e-02],\n",
       "         [ -5.86171262e-02,  -4.00425494e-02,   3.09384316e-02]],\n",
       " \n",
       "        [[ -6.55311495e-02,   1.84675772e-02,  -2.05362104e-02],\n",
       "         [  8.19705948e-02,  -3.77545878e-02,   1.08856726e-02],\n",
       "         [  8.17573890e-02,   6.23508766e-02,   5.16114943e-02]],\n",
       " \n",
       "        [[  1.39231086e-02,  -2.86768638e-02,   4.40019630e-02],\n",
       "         [ -6.64883032e-02,   5.53894229e-02,   3.94830890e-02],\n",
       "         [  5.92913218e-02,  -7.61959329e-02,   6.97165802e-02]],\n",
       " \n",
       "        [[ -1.37862435e-03,   7.32863024e-02,   4.53778496e-03],\n",
       "         [ -2.11700425e-02,  -1.08919879e-02,   3.78405093e-03],\n",
       "         [ -8.29682052e-02,  -5.47869205e-02,  -5.74675538e-02]],\n",
       " \n",
       "        [[  2.85271499e-02,  -7.61228651e-02,  -6.87770247e-02],\n",
       "         [ -1.21117895e-02,  -7.04306141e-02,   1.37446346e-02],\n",
       "         [ -1.20694134e-02,  -6.71054274e-02,   6.07458986e-02]],\n",
       " \n",
       "        [[ -6.40896708e-02,   1.18533690e-02,   4.24834639e-02],\n",
       "         [ -7.57227615e-02,  -7.14469403e-02,  -7.16250017e-02],\n",
       "         [  5.38672060e-02,   7.00431392e-02,   8.16100612e-02]],\n",
       " \n",
       "        [[  5.76804318e-02,  -5.69497421e-02,   8.08406174e-02],\n",
       "         [  1.18608894e-02,   1.87119301e-02,   2.73793135e-02],\n",
       "         [  6.33293092e-02,   6.03046343e-02,  -5.89728765e-02]],\n",
       " \n",
       "        [[  8.29507411e-02,  -5.68350106e-02,   5.77797741e-02],\n",
       "         [  1.91685874e-02,   5.96377708e-04,   7.45644048e-02],\n",
       "         [ -6.58631623e-02,   2.18861643e-02,  -7.49366730e-02]],\n",
       " \n",
       "        [[  1.83073673e-02,  -6.23738319e-02,  -8.19897205e-02],\n",
       "         [  2.27483269e-02,  -2.06416529e-02,  -4.09003980e-02],\n",
       "         [  1.93225965e-02,   3.99594121e-02,   2.83093266e-02]],\n",
       " \n",
       "        [[  7.89831951e-02,   5.67658022e-02,  -2.00419091e-02],\n",
       "         [  2.68006716e-02,  -4.08266932e-02,  -1.52932210e-02],\n",
       "         [  1.83643550e-02,   3.25790904e-02,  -1.88202746e-02]],\n",
       " \n",
       "        [[  5.19744381e-02,  -4.06706966e-02,  -5.54687344e-02],\n",
       "         [  5.82194068e-02,   1.39914698e-03,  -8.00996274e-02],\n",
       "         [ -9.69188381e-03,   8.03766102e-02,   3.42887118e-02]],\n",
       " \n",
       "        [[  4.80131581e-02,   7.28972182e-02,   7.01382803e-03],\n",
       "         [ -8.12472031e-02,  -5.14158159e-02,   1.61795560e-02],\n",
       "         [ -7.62310475e-02,   7.92053938e-02,   5.91008589e-02]],\n",
       " \n",
       "        [[  6.35470147e-04,   9.89626162e-03,   3.19986865e-02],\n",
       "         [ -2.41549965e-02,  -3.36746685e-02,   6.71492592e-02],\n",
       "         [ -7.27900416e-02,  -4.68155369e-02,   1.42693594e-02]],\n",
       " \n",
       "        [[ -4.19830382e-02,   6.34517372e-02,  -5.49688116e-02],\n",
       "         [ -6.49481313e-03,   7.38004372e-02,  -5.44708073e-02],\n",
       "         [ -5.76206483e-02,  -4.65058647e-02,  -5.07156178e-03]],\n",
       " \n",
       "        [[ -1.30954646e-02,   1.83646344e-02,  -5.53659312e-02],\n",
       "         [ -1.56517085e-02,   1.21381516e-02,  -1.34110218e-02],\n",
       "         [ -4.41421941e-03,   7.90946558e-02,   4.50905748e-02]],\n",
       " \n",
       "        [[ -7.50962198e-02,  -2.52081584e-02,   6.91143125e-02],\n",
       "         [  3.65106054e-02,   7.44765475e-02,  -5.86835742e-02],\n",
       "         [ -2.37828083e-02,   5.48773073e-02,   7.92316571e-02]],\n",
       " \n",
       "        [[  1.33154849e-02,  -6.36704043e-02,   2.39998661e-02],\n",
       "         [  8.04164708e-02,  -3.42447832e-02,  -5.70773426e-03],\n",
       "         [  4.55273800e-02,   4.13791789e-03,  -4.84468900e-02]],\n",
       " \n",
       "        [[  6.65421262e-02,   2.70286128e-02,   5.06895259e-02],\n",
       "         [ -5.22668473e-03,  -4.27601300e-02,   1.33900056e-02],\n",
       "         [  3.83912176e-02,   2.30626054e-02,  -8.22566301e-02]],\n",
       " \n",
       "        [[  7.95565844e-02,   4.86334562e-02,   1.94272436e-02],\n",
       "         [ -1.36633962e-02,  -3.94149683e-02,   6.76768422e-02],\n",
       "         [ -2.07671579e-02,   2.03770027e-02,  -7.81434923e-02]],\n",
       " \n",
       "        [[  1.55761065e-02,  -2.35865396e-02,  -3.47867161e-02],\n",
       "         [ -6.81908503e-02,   6.81734011e-02,   3.66137177e-02],\n",
       "         [  7.38960505e-02,   7.36771151e-02,  -9.65986121e-03]],\n",
       " \n",
       "        [[ -7.89945647e-02,  -5.11448011e-02,  -5.04962988e-02],\n",
       "         [  3.50162241e-04,  -3.04664783e-02,   1.35989767e-02],\n",
       "         [  2.86963675e-02,   2.23100036e-02,   2.01429729e-03]],\n",
       " \n",
       "        [[  2.07777433e-02,  -7.20394775e-02,   7.29478989e-03],\n",
       "         [  8.00063014e-02,   4.52816933e-02,  -7.91444629e-02],\n",
       "         [ -2.34486889e-02,   4.26435843e-02,  -7.02342317e-02]],\n",
       " \n",
       "        [[ -3.94061347e-03,  -1.76219102e-02,   7.53463954e-02],\n",
       "         [ -9.28402599e-03,  -2.42415424e-02,  -2.25943290e-02],\n",
       "         [  4.76791710e-02,  -5.50019904e-05,   6.90791979e-02]],\n",
       " \n",
       "        [[  7.58789806e-03,   6.39104564e-03,   7.19921663e-02],\n",
       "         [ -2.33183000e-02,  -8.01485553e-02,   4.67864238e-02],\n",
       "         [ -6.23852648e-02,  -5.93152158e-02,   2.29170080e-02]],\n",
       " \n",
       "        [[  1.89962294e-02,  -3.30570969e-03,   5.94405495e-02],\n",
       "         [ -5.30313589e-02,  -9.75638255e-03,   3.81957181e-02],\n",
       "         [ -3.23535465e-02,  -2.01560650e-02,   4.66077663e-02]],\n",
       " \n",
       "        [[  6.87560439e-03,   1.52966632e-02,   4.44154032e-02],\n",
       "         [ -2.23600380e-02,  -2.64103785e-02,   2.55414397e-02],\n",
       "         [ -5.44422641e-02,  -5.06705046e-02,  -4.80068587e-02]]], dtype=float32), bias=-0.00016800105, layer=1, neuron_number=8, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.04719753,  0.05120803,  0.06021059],\n",
       "         [-0.04402971, -0.04483275,  0.0419223 ],\n",
       "         [-0.01467256,  0.02379362, -0.02309269]],\n",
       " \n",
       "        [[ 0.01159752, -0.07301421,  0.06571256],\n",
       "         [ 0.06744252,  0.01568045,  0.02512379],\n",
       "         [-0.02735918,  0.01295901,  0.00156958]],\n",
       " \n",
       "        [[-0.06735528,  0.05259131, -0.0366606 ],\n",
       "         [ 0.02566655, -0.04793081, -0.01864926],\n",
       "         [-0.01211252,  0.03624197, -0.0692452 ]],\n",
       " \n",
       "        [[-0.05648357, -0.0042531 , -0.0330066 ],\n",
       "         [-0.03976525, -0.05279935, -0.04256793],\n",
       "         [ 0.04880938, -0.01043674, -0.07368556]],\n",
       " \n",
       "        [[-0.00044807,  0.01256729,  0.04063543],\n",
       "         [ 0.05422046,  0.0792124 ,  0.07233627],\n",
       "         [-0.05804697, -0.05506467, -0.02628376]],\n",
       " \n",
       "        [[ 0.06168853, -0.0151041 , -0.0122769 ],\n",
       "         [ 0.05102428,  0.06682516, -0.02048738],\n",
       "         [ 0.0786745 ,  0.02227134,  0.06490559]],\n",
       " \n",
       "        [[ 0.04400425,  0.07775169, -0.00104976],\n",
       "         [ 0.02045465,  0.00075734, -0.00567873],\n",
       "         [-0.02774213,  0.06492916, -0.05747517]],\n",
       " \n",
       "        [[ 0.01354152, -0.00370704,  0.0656967 ],\n",
       "         [ 0.05946404, -0.05907841,  0.07370695],\n",
       "         [-0.01432636,  0.06522311, -0.07692654]],\n",
       " \n",
       "        [[ 0.01063705,  0.08030809, -0.0646184 ],\n",
       "         [ 0.06923182,  0.08165061, -0.00015971],\n",
       "         [-0.0072349 ,  0.00303447,  0.01399497]],\n",
       " \n",
       "        [[ 0.07728231, -0.02163703, -0.01838671],\n",
       "         [-0.04919787, -0.04049826, -0.0303374 ],\n",
       "         [ 0.0466416 ,  0.01503284,  0.04932353]],\n",
       " \n",
       "        [[ 0.02611873,  0.04718933, -0.06402589],\n",
       "         [ 0.01459806,  0.03344292, -0.04327615],\n",
       "         [-0.08397226, -0.08082451, -0.05737352]],\n",
       " \n",
       "        [[-0.03829122,  0.07177655, -0.01348683],\n",
       "         [-0.03310801,  0.06399743,  0.02478899],\n",
       "         [-0.00901806, -0.06544012,  0.03976324]],\n",
       " \n",
       "        [[-0.04713047, -0.0449018 ,  0.07562562],\n",
       "         [ 0.00327692, -0.0516245 , -0.06972504],\n",
       "         [-0.01196481,  0.05941263,  0.03811487]],\n",
       " \n",
       "        [[ 0.01609393, -0.05984221, -0.08031524],\n",
       "         [-0.00512627, -0.04779524, -0.05363281],\n",
       "         [-0.00898508, -0.02891615, -0.0412067 ]],\n",
       " \n",
       "        [[-0.05120453, -0.04682452, -0.06045789],\n",
       "         [-0.06032892, -0.04841544, -0.05282116],\n",
       "         [ 0.01688902,  0.00883328,  0.03274228]],\n",
       " \n",
       "        [[-0.03583828, -0.03141876, -0.032999  ],\n",
       "         [ 0.026087  ,  0.00700578, -0.032306  ],\n",
       "         [-0.05293343, -0.02710837,  0.05754253]],\n",
       " \n",
       "        [[-0.02565597, -0.07795039, -0.0341882 ],\n",
       "         [ 0.0452021 ,  0.02739807, -0.07261482],\n",
       "         [ 0.08048278, -0.02490804,  0.07996939]],\n",
       " \n",
       "        [[ 0.01720087, -0.05818442,  0.05584435],\n",
       "         [-0.00100855,  0.04330315,  0.0547005 ],\n",
       "         [-0.01781109,  0.02208588,  0.02571412]],\n",
       " \n",
       "        [[ 0.08006997,  0.04522943,  0.07006296],\n",
       "         [-0.08325092,  0.00022042,  0.07425974],\n",
       "         [-0.07648789,  0.07349049, -0.0181369 ]],\n",
       " \n",
       "        [[-0.07563277, -0.02627163, -0.07134971],\n",
       "         [ 0.07284404,  0.00694424, -0.08034244],\n",
       "         [ 0.07943258, -0.03083879, -0.02673274]],\n",
       " \n",
       "        [[ 0.0765152 , -0.00037496, -0.04600319],\n",
       "         [-0.02660793,  0.08158219,  0.00072487],\n",
       "         [-0.07585286, -0.06965899, -0.03917214]],\n",
       " \n",
       "        [[ 0.00891237, -0.0149919 ,  0.00926069],\n",
       "         [ 0.08423433,  0.06532958,  0.02365561],\n",
       "         [ 0.02195659,  0.08411243, -0.07917392]],\n",
       " \n",
       "        [[-0.05681576, -0.00020865,  0.03475398],\n",
       "         [-0.07100321,  0.0226987 ,  0.01751216],\n",
       "         [-0.07954574, -0.01130487, -0.02203614]],\n",
       " \n",
       "        [[ 0.01130378,  0.0791944 ,  0.06595952],\n",
       "         [ 0.06650641,  0.01941059,  0.07711265],\n",
       "         [-0.01830541, -0.02785354, -0.06116289]],\n",
       " \n",
       "        [[ 0.01685075,  0.05627257,  0.00713453],\n",
       "         [-0.04272492,  0.0700403 ,  0.0154469 ],\n",
       "         [-0.08024102, -0.0444791 ,  0.03168093]],\n",
       " \n",
       "        [[-0.04982794, -0.00692417, -0.05781164],\n",
       "         [ 0.03375538,  0.01486414, -0.05182398],\n",
       "         [-0.06419764,  0.05781714, -0.03176851]],\n",
       " \n",
       "        [[ 0.04467744, -0.04390962, -0.06607021],\n",
       "         [-0.06846956, -0.02927263, -0.07723662],\n",
       "         [ 0.07595077,  0.0713698 ,  0.04022323]],\n",
       " \n",
       "        [[-0.04652857, -0.02507049, -0.08176883],\n",
       "         [-0.01726287,  0.01770221,  0.07612734],\n",
       "         [ 0.01751946,  0.06515297, -0.00396581]],\n",
       " \n",
       "        [[-0.07073075, -0.0696192 ,  0.05666695],\n",
       "         [ 0.06464589, -0.07113784,  0.06472433],\n",
       "         [-0.02981702,  0.02977197,  0.01873588]],\n",
       " \n",
       "        [[-0.060867  , -0.03057336, -0.06340566],\n",
       "         [ 0.043606  ,  0.0560125 ,  0.07534599],\n",
       "         [-0.08179869,  0.04374941,  0.01818774]],\n",
       " \n",
       "        [[-0.06479214, -0.06151111, -0.05912976],\n",
       "         [ 0.04436732,  0.06218131, -0.07786458],\n",
       "         [ 0.01773353,  0.04417656, -0.07600699]],\n",
       " \n",
       "        [[ 0.08291812, -0.01642734,  0.03587116],\n",
       "         [-0.01588872,  0.05741946,  0.02064385],\n",
       "         [-0.0305779 ,  0.00455222, -0.00615955]]], dtype=float32), bias=0.0013262115, layer=1, neuron_number=9, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.03404484, -0.06157285, -0.01601264],\n",
       "         [-0.08202334,  0.04586107, -0.02935852],\n",
       "         [ 0.00627099,  0.02784065,  0.01521499]],\n",
       " \n",
       "        [[ 0.06004924, -0.00647128,  0.00497298],\n",
       "         [ 0.02332049, -0.00554251,  0.00052766],\n",
       "         [ 0.06371533,  0.0079285 , -0.05060945]],\n",
       " \n",
       "        [[-0.04992899,  0.07903874,  0.00175154],\n",
       "         [-0.07580645, -0.07373668, -0.02422806],\n",
       "         [ 0.0325437 ,  0.04651292,  0.07343461]],\n",
       " \n",
       "        [[-0.03382485,  0.04913385,  0.055218  ],\n",
       "         [-0.02527502, -0.07605904, -0.04351971],\n",
       "         [ 0.03934319, -0.0196959 , -0.01719305]],\n",
       " \n",
       "        [[ 0.04108455, -0.03244039,  0.02883521],\n",
       "         [ 0.03504021, -0.07358824,  0.04569932],\n",
       "         [ 0.00947964,  0.05753662,  0.02156274]],\n",
       " \n",
       "        [[ 0.0719992 ,  0.07659435, -0.03412252],\n",
       "         [ 0.01545922,  0.0531901 ,  0.02362425],\n",
       "         [-0.0497408 ,  0.08225352, -0.05727318]],\n",
       " \n",
       "        [[ 0.0072886 ,  0.0637208 , -0.02917043],\n",
       "         [-0.01749728,  0.05209313, -0.00014332],\n",
       "         [ 0.01508516,  0.07488512, -0.00643782]],\n",
       " \n",
       "        [[ 0.04109222, -0.00074719,  0.02517881],\n",
       "         [-0.04040156, -0.01419798,  0.00297131],\n",
       "         [ 0.01165032, -0.05690773, -0.06725413]],\n",
       " \n",
       "        [[ 0.08366139,  0.04073719, -0.04722799],\n",
       "         [ 0.0594197 ,  0.03986231, -0.05566622],\n",
       "         [ 0.02064896,  0.04162452, -0.0211647 ]],\n",
       " \n",
       "        [[-0.01824385, -0.04219538, -0.00027507],\n",
       "         [-0.02195647, -0.0645282 , -0.04584919],\n",
       "         [-0.03544397,  0.02536971, -0.05592046]],\n",
       " \n",
       "        [[ 0.02442627,  0.0452885 , -0.04855285],\n",
       "         [-0.0518281 , -0.03369103,  0.01753157],\n",
       "         [-0.01528248, -0.02825995, -0.06226892]],\n",
       " \n",
       "        [[ 0.00132531, -0.00200359, -0.04797565],\n",
       "         [ 0.07425741,  0.03681998, -0.06936697],\n",
       "         [-0.02698522,  0.06158619,  0.04335519]],\n",
       " \n",
       "        [[-0.01592529, -0.07456926, -0.01176999],\n",
       "         [-0.03891879, -0.07821964, -0.03942253],\n",
       "         [-0.07371375,  0.02791819,  0.05932372]],\n",
       " \n",
       "        [[-0.05094938, -0.08167164, -0.06723543],\n",
       "         [-0.06493727, -0.04317566, -0.04213878],\n",
       "         [ 0.07247321, -0.03824874, -0.0675116 ]],\n",
       " \n",
       "        [[ 0.07089034, -0.02485025, -0.07000521],\n",
       "         [-0.06696396,  0.00345421,  0.03470125],\n",
       "         [-0.0354197 , -0.02414935,  0.05238446]],\n",
       " \n",
       "        [[-0.07578805, -0.0518427 ,  0.06178404],\n",
       "         [-0.03541569,  0.0721265 , -0.03820112],\n",
       "         [ 0.05455381,  0.05132147,  0.069131  ]],\n",
       " \n",
       "        [[ 0.03006635, -0.07076123,  0.03397683],\n",
       "         [ 0.0504984 , -0.00159647,  0.01337365],\n",
       "         [ 0.02342099, -0.0052785 , -0.06598579]],\n",
       " \n",
       "        [[-0.06221379,  0.00949205,  0.04358531],\n",
       "         [-0.02381764, -0.05751815,  0.00097429],\n",
       "         [-0.02344501,  0.0140847 , -0.06840147]],\n",
       " \n",
       "        [[ 0.01834261,  0.00123127, -0.05408166],\n",
       "         [ 0.0481072 , -0.03506802, -0.06629899],\n",
       "         [ 0.01033936,  0.03051651, -0.01836633]],\n",
       " \n",
       "        [[-0.00679058, -0.04416409,  0.07120683],\n",
       "         [ 0.0381567 , -0.01663821,  0.07063346],\n",
       "         [ 0.0572648 ,  0.02687031,  0.01724185]],\n",
       " \n",
       "        [[ 0.03503485,  0.01635525,  0.04759525],\n",
       "         [ 0.0628629 , -0.07329355, -0.05844672],\n",
       "         [ 0.00369912,  0.03291416,  0.06048395]],\n",
       " \n",
       "        [[-0.01746033,  0.05200111, -0.03793335],\n",
       "         [-0.07408747,  0.02135723,  0.00776321],\n",
       "         [ 0.0524165 , -0.01020632, -0.03241785]],\n",
       " \n",
       "        [[ 0.04546659, -0.04225957,  0.00459389],\n",
       "         [-0.0071973 ,  0.02368613, -0.05462991],\n",
       "         [-0.04685818,  0.05851184,  0.02904883]],\n",
       " \n",
       "        [[ 0.01065879,  0.06702627, -0.00411868],\n",
       "         [-0.06347957, -0.01292951, -0.03699134],\n",
       "         [ 0.03244029, -0.06670947,  0.00253951]],\n",
       " \n",
       "        [[ 0.03446725, -0.01060875, -0.02666609],\n",
       "         [-0.00845276, -0.01843975, -0.05517577],\n",
       "         [ 0.02650262, -0.05972   , -0.03196503]],\n",
       " \n",
       "        [[-0.0261228 ,  0.01356337, -0.0360995 ],\n",
       "         [-0.03387163, -0.07279423,  0.03610751],\n",
       "         [ 0.01778433, -0.01474609,  0.00154049]],\n",
       " \n",
       "        [[-0.02965846, -0.02976627,  0.06290133],\n",
       "         [ 0.03693986, -0.00837544,  0.01560008],\n",
       "         [-0.04102219, -0.02812993, -0.04102458]],\n",
       " \n",
       "        [[-0.02576546,  0.01382392, -0.07479107],\n",
       "         [ 0.07658917, -0.06556894,  0.08114085],\n",
       "         [ 0.06671757, -0.00342098,  0.04635821]],\n",
       " \n",
       "        [[-0.01137717, -0.03618054, -0.03124472],\n",
       "         [-0.01915039, -0.07752687, -0.04843462],\n",
       "         [-0.03422609,  0.00750089,  0.02300068]],\n",
       " \n",
       "        [[-0.04123103,  0.01421103,  0.06245477],\n",
       "         [ 0.01700072,  0.0692487 , -0.06629969],\n",
       "         [ 0.03546482, -0.05004586, -0.01063455]],\n",
       " \n",
       "        [[ 0.05428232, -0.0082733 , -0.00510689],\n",
       "         [-0.00549378, -0.02771787, -0.06460068],\n",
       "         [-0.03689533,  0.00864776,  0.02715657]],\n",
       " \n",
       "        [[-0.00624311,  0.0430468 ,  0.06859007],\n",
       "         [ 0.01436272,  0.06306363,  0.07656074],\n",
       "         [-0.02962836, -0.02607272, -0.0547873 ]]], dtype=float32), bias=0.00068171648, layer=1, neuron_number=10, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.0818528 , -0.04071414, -0.04495805],\n",
       "         [ 0.00438845, -0.04975331,  0.06631126],\n",
       "         [-0.03332017,  0.00850954,  0.00576154]],\n",
       " \n",
       "        [[-0.05544645,  0.08316676, -0.06601211],\n",
       "         [-0.0799822 ,  0.02559526,  0.0221178 ],\n",
       "         [-0.02674972,  0.06938055,  0.02699635]],\n",
       " \n",
       "        [[ 0.02066732, -0.02368414,  0.07595435],\n",
       "         [-0.00386747,  0.02744586,  0.03701444],\n",
       "         [ 0.0212397 , -0.03952052,  0.05197594]],\n",
       " \n",
       "        [[ 0.00519114, -0.01450672, -0.05448576],\n",
       "         [-0.06847375,  0.05500533, -0.05103179],\n",
       "         [ 0.03715523,  0.00557909, -0.0683043 ]],\n",
       " \n",
       "        [[ 0.07194851,  0.08301102, -0.02608054],\n",
       "         [ 0.01161245,  0.08043879,  0.00450186],\n",
       "         [ 0.05280301,  0.01673891, -0.01968005]],\n",
       " \n",
       "        [[ 0.03641345,  0.03604481,  0.02705326],\n",
       "         [ 0.04616968, -0.04631697, -0.04840921],\n",
       "         [-0.07624084, -0.05776153, -0.00503174]],\n",
       " \n",
       "        [[-0.07647685,  0.05101885, -0.06387249],\n",
       "         [-0.08239382, -0.04289644,  0.05444332],\n",
       "         [-0.01413749, -0.00190597, -0.00367631]],\n",
       " \n",
       "        [[ 0.07970743,  0.03020307, -0.01788111],\n",
       "         [ 0.00574412,  0.00765954, -0.03929387],\n",
       "         [-0.07814066, -0.0400336 , -0.01855705]],\n",
       " \n",
       "        [[ 0.00848541,  0.02551759,  0.02788925],\n",
       "         [ 0.00820455, -0.02148088,  0.05071406],\n",
       "         [ 0.00288803, -0.02041792, -0.07830159]],\n",
       " \n",
       "        [[ 0.04214939,  0.01886619,  0.07195522],\n",
       "         [-0.05459347, -0.00731602,  0.00270004],\n",
       "         [ 0.07623415,  0.01809206,  0.08118661]],\n",
       " \n",
       "        [[-0.05002584, -0.02099462,  0.0579687 ],\n",
       "         [-0.00275773,  0.04158038,  0.0762786 ],\n",
       "         [ 0.06357753, -0.01550556, -0.04677432]],\n",
       " \n",
       "        [[-0.04304912,  0.01113947, -0.06931397],\n",
       "         [-0.03540263,  0.01618067, -0.0617495 ],\n",
       "         [-0.07180673,  0.07299524,  0.05387584]],\n",
       " \n",
       "        [[ 0.06834371, -0.08090384, -0.07589635],\n",
       "         [ 0.06222291,  0.00978603,  0.07033671],\n",
       "         [-0.03979488, -0.01227737,  0.0374721 ]],\n",
       " \n",
       "        [[ 0.08061551, -0.00997934, -0.03848162],\n",
       "         [-0.04637434,  0.03525542, -0.05639039],\n",
       "         [-0.06652801,  0.0484619 , -0.05386293]],\n",
       " \n",
       "        [[-0.05745605, -0.02096535, -0.0040794 ],\n",
       "         [ 0.04238573, -0.03165433,  0.06468089],\n",
       "         [ 0.08306007, -0.02774483, -0.00927241]],\n",
       " \n",
       "        [[ 0.00114153, -0.00184245, -0.05127339],\n",
       "         [ 0.00259777, -0.08144021,  0.04462117],\n",
       "         [ 0.03768506,  0.05696007,  0.0332651 ]],\n",
       " \n",
       "        [[ 0.0633676 ,  0.06994675, -0.07268555],\n",
       "         [ 0.02065882,  0.06389357,  0.00248466],\n",
       "         [ 0.00316355, -0.06586087,  0.03746715]],\n",
       " \n",
       "        [[-0.01810598,  0.00946779, -0.02016454],\n",
       "         [ 0.0444949 ,  0.08137641,  0.03208905],\n",
       "         [-0.00158108, -0.04883141, -0.01300652]],\n",
       " \n",
       "        [[-0.04521526, -0.06224248, -0.02519829],\n",
       "         [ 0.00362876, -0.04062035,  0.05774928],\n",
       "         [ 0.00597137,  0.03493474,  0.04620164]],\n",
       " \n",
       "        [[-0.01005316,  0.07238489, -0.06682808],\n",
       "         [-0.05081657, -0.00357312,  0.07685961],\n",
       "         [ 0.05613735, -0.05169402, -0.01804163]],\n",
       " \n",
       "        [[ 0.04193853, -0.02383095,  0.04769124],\n",
       "         [-0.07741623, -0.02014115, -0.00178744],\n",
       "         [ 0.01638322, -0.00222749, -0.06558298]],\n",
       " \n",
       "        [[-0.01917148, -0.01999985,  0.07002551],\n",
       "         [-0.00134938, -0.05475491, -0.07364685],\n",
       "         [-0.03307426, -0.0499189 , -0.05124009]],\n",
       " \n",
       "        [[ 0.05518196, -0.0062659 , -0.07983056],\n",
       "         [-0.04600671, -0.06690137, -0.05420091],\n",
       "         [ 0.05760722, -0.04627341,  0.02270423]],\n",
       " \n",
       "        [[-0.05283479,  0.00497693,  0.01955141],\n",
       "         [ 0.07231307,  0.04203817, -0.05417043],\n",
       "         [ 0.00678633, -0.01927723, -0.06006694]],\n",
       " \n",
       "        [[ 0.00821838,  0.00958422, -0.00201314],\n",
       "         [ 0.051228  , -0.06385266,  0.04807625],\n",
       "         [ 0.06899577, -0.03818981, -0.05192282]],\n",
       " \n",
       "        [[-0.01642697,  0.00449978,  0.00899636],\n",
       "         [ 0.01667177,  0.01787099,  0.00574159],\n",
       "         [ 0.0132387 , -0.02660725, -0.036271  ]],\n",
       " \n",
       "        [[ 0.00813694, -0.02527801, -0.07060499],\n",
       "         [ 0.0650264 , -0.03376077,  0.01412445],\n",
       "         [ 0.05285327, -0.05978124, -0.08065123]],\n",
       " \n",
       "        [[ 0.00054997,  0.01119574, -0.02986089],\n",
       "         [-0.06710187,  0.02173021, -0.07365447],\n",
       "         [ 0.02084787,  0.05615631, -0.06339062]],\n",
       " \n",
       "        [[-0.00494883, -0.02570671,  0.06005305],\n",
       "         [-0.05464106,  0.07100882,  0.01788923],\n",
       "         [-0.00255097, -0.07191683, -0.04170838]],\n",
       " \n",
       "        [[-0.01946915, -0.04075562,  0.00359295],\n",
       "         [ 0.03822423, -0.01470731,  0.00631489],\n",
       "         [ 0.05662971,  0.0422383 , -0.08018436]],\n",
       " \n",
       "        [[ 0.00985502,  0.02057246, -0.01813646],\n",
       "         [ 0.0342715 ,  0.07766739,  0.00478519],\n",
       "         [ 0.00662502,  0.02859174,  0.07718755]],\n",
       " \n",
       "        [[ 0.07980313,  0.02502202, -0.08004306],\n",
       "         [ 0.07934005, -0.05574068, -0.05926111],\n",
       "         [ 0.0569246 , -0.04365966, -0.02198832]]], dtype=float32), bias=0.00082720071, layer=1, neuron_number=11, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.05663579,  0.06575078, -0.00508177],\n",
       "         [ 0.07255326, -0.00595291,  0.00654815],\n",
       "         [ 0.00738305, -0.07287914, -0.00337882]],\n",
       " \n",
       "        [[-0.07641234,  0.074475  , -0.00033815],\n",
       "         [-0.07837231,  0.03466862,  0.00922324],\n",
       "         [ 0.03304483, -0.02440695,  0.00067544]],\n",
       " \n",
       "        [[ 0.06522202, -0.03875242, -0.03390263],\n",
       "         [-0.06343389, -0.01296281,  0.00509743],\n",
       "         [-0.05145502, -0.00130724, -0.02923615]],\n",
       " \n",
       "        [[ 0.00344654,  0.07501359,  0.03148746],\n",
       "         [-0.04669988,  0.05023233,  0.08379365],\n",
       "         [ 0.0433133 ,  0.07560671,  0.02994036]],\n",
       " \n",
       "        [[-0.04607918,  0.02621309,  0.06560462],\n",
       "         [-0.03192379, -0.0041249 , -0.01096914],\n",
       "         [ 0.06360713, -0.05405098,  0.01738308]],\n",
       " \n",
       "        [[-0.08133523,  0.0215691 , -0.01516589],\n",
       "         [ 0.01257211,  0.00937241, -0.067431  ],\n",
       "         [-0.02472384, -0.02933684,  0.05675506]],\n",
       " \n",
       "        [[ 0.04087723,  0.03549366, -0.01972021],\n",
       "         [-0.0673134 ,  0.00467486, -0.06800266],\n",
       "         [ 0.07412928, -0.00413154,  0.05313713]],\n",
       " \n",
       "        [[ 0.04004116, -0.0125115 ,  0.00412956],\n",
       "         [ 0.05281757,  0.0596551 ,  0.08018075],\n",
       "         [-0.05048894, -0.02802966, -0.05380694]],\n",
       " \n",
       "        [[ 0.05865322,  0.08304361, -0.00519758],\n",
       "         [ 0.05729081,  0.00620884,  0.07432798],\n",
       "         [-0.04435845,  0.01930804, -0.00663439]],\n",
       " \n",
       "        [[-0.04404558, -0.03530769,  0.01335617],\n",
       "         [-0.02606718,  0.04858649,  0.05802138],\n",
       "         [-0.00112018,  0.07587706,  0.04298289]],\n",
       " \n",
       "        [[-0.08088429,  0.00826473, -0.00874376],\n",
       "         [ 0.06347297, -0.0009688 , -0.0302577 ],\n",
       "         [-0.03554589, -0.08317062, -0.02926633]],\n",
       " \n",
       "        [[-0.0766093 , -0.03599719,  0.04537732],\n",
       "         [ 0.0468168 ,  0.04349711,  0.03681079],\n",
       "         [-0.07317446,  0.01889984, -0.02175135]],\n",
       " \n",
       "        [[ 0.01687736,  0.00664942, -0.04465871],\n",
       "         [-0.03242427, -0.00646109, -0.02504324],\n",
       "         [ 0.04240201, -0.0690672 , -0.00580461]],\n",
       " \n",
       "        [[ 0.03138807,  0.0015973 , -0.02965274],\n",
       "         [-0.00050551, -0.00753817,  0.05520066],\n",
       "         [-0.06723092, -0.0480328 , -0.0020421 ]],\n",
       " \n",
       "        [[-0.00549312,  0.0779205 ,  0.07214104],\n",
       "         [-0.02497168,  0.05422378, -0.01586894],\n",
       "         [ 0.08031449,  0.00683186, -0.05762171]],\n",
       " \n",
       "        [[ 0.05039988,  0.04334787, -0.05574887],\n",
       "         [ 0.05478511, -0.02393619, -0.04606146],\n",
       "         [ 0.05478918, -0.07831306, -0.03511916]],\n",
       " \n",
       "        [[ 0.05234921,  0.05390336,  0.00985392],\n",
       "         [ 0.0166021 , -0.06225754, -0.06775218],\n",
       "         [-0.06574786,  0.0608004 , -0.04325871]],\n",
       " \n",
       "        [[-0.06964483, -0.03369436, -0.07976599],\n",
       "         [-0.04463177, -0.06925657, -0.02828176],\n",
       "         [-0.03586394,  0.04490347,  0.07417997]],\n",
       " \n",
       "        [[-0.06980366, -0.05254101, -0.04575615],\n",
       "         [-0.07848731,  0.00036322, -0.0724727 ],\n",
       "         [ 0.04231081,  0.05825562,  0.06207861]],\n",
       " \n",
       "        [[-0.04356326,  0.0217408 ,  0.0563749 ],\n",
       "         [ 0.0453771 , -0.02161765,  0.01454069],\n",
       "         [ 0.01524578, -0.07591589, -0.05730642]],\n",
       " \n",
       "        [[-0.03588302, -0.00537761,  0.01259856],\n",
       "         [-0.07516373,  0.04135701, -0.03450057],\n",
       "         [ 0.05103811, -0.06943712,  0.05513437]],\n",
       " \n",
       "        [[-0.01393207, -0.06858061,  0.01060754],\n",
       "         [ 0.02938355,  0.01471891,  0.01144124],\n",
       "         [-0.07848379, -0.04659732, -0.03529036]],\n",
       " \n",
       "        [[ 0.04429036, -0.04476724, -0.0495345 ],\n",
       "         [ 0.03103535, -0.05640582,  0.07783715],\n",
       "         [ 0.07358116,  0.06188675, -0.07396124]],\n",
       " \n",
       "        [[ 0.05511674, -0.02142659, -0.04544618],\n",
       "         [ 0.00199299,  0.06744803,  0.05144186],\n",
       "         [ 0.06531171,  0.0344877 ,  0.05138412]],\n",
       " \n",
       "        [[ 0.07438342, -0.00402219,  0.06815757],\n",
       "         [ 0.03560981,  0.05415475,  0.03914562],\n",
       "         [-0.0094263 ,  0.0682456 , -0.00430824]],\n",
       " \n",
       "        [[-0.07095792, -0.07122145, -0.0441664 ],\n",
       "         [-0.01015774,  0.0448102 ,  0.04239801],\n",
       "         [-0.03172817,  0.06554246, -0.0813134 ]],\n",
       " \n",
       "        [[ 0.02896133, -0.00526549, -0.04961167],\n",
       "         [-0.00414775, -0.06557804,  0.08422558],\n",
       "         [-0.03924786,  0.08025558, -0.02364273]],\n",
       " \n",
       "        [[ 0.02611273,  0.07529583,  0.01509875],\n",
       "         [ 0.02026274, -0.00388913,  0.08012512],\n",
       "         [-0.00405506, -0.03913389,  0.06392206]],\n",
       " \n",
       "        [[-0.06238766, -0.0315119 ,  0.05433477],\n",
       "         [-0.01402807, -0.00141601,  0.07904108],\n",
       "         [-0.0381768 , -0.07951731,  0.05709627]],\n",
       " \n",
       "        [[-0.01654152,  0.03019694,  0.01564478],\n",
       "         [-0.07279303, -0.04055601,  0.0751199 ],\n",
       "         [ 0.04662367, -0.02537373, -0.02910125]],\n",
       " \n",
       "        [[-0.00177581, -0.02787755,  0.02011988],\n",
       "         [ 0.07459124, -0.00575945,  0.01535689],\n",
       "         [ 0.05964943, -0.03912795,  0.04053556]],\n",
       " \n",
       "        [[-0.07002746,  0.0517028 , -0.01433353],\n",
       "         [ 0.06484339,  0.04361015,  0.03213078],\n",
       "         [ 0.07804023, -0.07565054, -0.05763597]]], dtype=float32), bias=0.001801257, layer=1, neuron_number=12, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.08071021, -0.07860094,  0.01006371],\n",
       "         [ 0.00416164, -0.04485893, -0.03138638],\n",
       "         [-0.04596716,  0.05914738, -0.06624098]],\n",
       " \n",
       "        [[ 0.03175942, -0.02695278,  0.01535809],\n",
       "         [ 0.04371018,  0.04237171, -0.04016235],\n",
       "         [-0.07809447,  0.04712775, -0.00562627]],\n",
       " \n",
       "        [[ 0.01597249, -0.05328208, -0.06288584],\n",
       "         [-0.06731633, -0.06586583, -0.00412742],\n",
       "         [-0.01992399, -0.06258533, -0.01407149]],\n",
       " \n",
       "        [[ 0.0054346 ,  0.07861757,  0.05440101],\n",
       "         [-0.01222657,  0.03853972,  0.04330455],\n",
       "         [ 0.05595492, -0.04244773,  0.01870257]],\n",
       " \n",
       "        [[ 0.02926399, -0.01159225, -0.05683682],\n",
       "         [ 0.01230857,  0.0321265 , -0.01250424],\n",
       "         [ 0.06422121, -0.06090663,  0.01534902]],\n",
       " \n",
       "        [[ 0.05313436, -0.03967564, -0.0294889 ],\n",
       "         [ 0.07891146, -0.02268318, -0.04987832],\n",
       "         [ 0.06396794, -0.03639555,  0.0367723 ]],\n",
       " \n",
       "        [[-0.00127836,  0.00447286, -0.00073887],\n",
       "         [ 0.01967689,  0.07417426,  0.07126499],\n",
       "         [-0.00952612,  0.02088667,  0.03325671]],\n",
       " \n",
       "        [[-0.01038131,  0.05427798,  0.04627304],\n",
       "         [ 0.0637038 ,  0.03400279,  0.03260023],\n",
       "         [-0.03912423,  0.06214482,  0.02343755]],\n",
       " \n",
       "        [[ 0.00927612,  0.05093496, -0.06289034],\n",
       "         [ 0.04965556,  0.00745166,  0.0473212 ],\n",
       "         [-0.05820044, -0.00253443, -0.05163217]],\n",
       " \n",
       "        [[ 0.05839719,  0.0797696 , -0.05121078],\n",
       "         [-0.07720086,  0.01821979,  0.0709323 ],\n",
       "         [-0.04737305,  0.01249814, -0.00161728]],\n",
       " \n",
       "        [[ 0.03417866, -0.07472111,  0.04362652],\n",
       "         [ 0.0145825 , -0.01140726,  0.04770942],\n",
       "         [ 0.01496045, -0.03917623,  0.03579883]],\n",
       " \n",
       "        [[ 0.0136916 ,  0.03571231,  0.05739576],\n",
       "         [-0.02364478, -0.02383164, -0.00151646],\n",
       "         [-0.05194602, -0.00557974,  0.03838037]],\n",
       " \n",
       "        [[-0.0449955 ,  0.01867795,  0.06437252],\n",
       "         [-0.06335877,  0.02253618, -0.06509305],\n",
       "         [-0.0153068 , -0.07101468,  0.06965125]],\n",
       " \n",
       "        [[ 0.0523961 ,  0.06090606, -0.02073221],\n",
       "         [-0.06244441, -0.00205042,  0.06278642],\n",
       "         [-0.07878868,  0.02854139,  0.0084712 ]],\n",
       " \n",
       "        [[-0.01179354, -0.0393769 , -0.06869335],\n",
       "         [-0.05989774,  0.07447182, -0.08233168],\n",
       "         [-0.05347026,  0.07594787,  0.02965503]],\n",
       " \n",
       "        [[-0.04402243, -0.05000993,  0.04491328],\n",
       "         [-0.01142382,  0.03547843, -0.04804728],\n",
       "         [ 0.0206754 ,  0.07201272,  0.05104477]],\n",
       " \n",
       "        [[ 0.07256153, -0.0346617 , -0.00358747],\n",
       "         [ 0.02463963,  0.05909254,  0.02433573],\n",
       "         [ 0.06889882,  0.08022339, -0.05844275]],\n",
       " \n",
       "        [[ 0.03359948,  0.03870005, -0.07267097],\n",
       "         [ 0.02763642, -0.00461324,  0.02122892],\n",
       "         [ 0.04964302, -0.02844464, -0.08019995]],\n",
       " \n",
       "        [[ 0.07376685,  0.01223199,  0.01608224],\n",
       "         [ 0.03986521, -0.02673792, -0.00916206],\n",
       "         [-0.03597865, -0.07047292, -0.05598322]],\n",
       " \n",
       "        [[ 0.03995338,  0.05757136,  0.00331099],\n",
       "         [ 0.03585254,  0.03086599,  0.0147439 ],\n",
       "         [-0.05839266,  0.02647183, -0.03798877]],\n",
       " \n",
       "        [[-0.0390449 , -0.01438122, -0.07718691],\n",
       "         [ 0.01016307, -0.05613341,  0.07809216],\n",
       "         [-0.02389387, -0.03948716,  0.03254505]],\n",
       " \n",
       "        [[-0.04489472, -0.06080626,  0.02311818],\n",
       "         [ 0.03943298,  0.05914535, -0.02605739],\n",
       "         [ 0.04983187,  0.07337404,  0.01542006]],\n",
       " \n",
       "        [[-0.02516432, -0.03687289,  0.05167555],\n",
       "         [ 0.05288091,  0.05029748, -0.05325264],\n",
       "         [ 0.03601637, -0.07518034, -0.06350016]],\n",
       " \n",
       "        [[ 0.07443891,  0.01361353,  0.00231698],\n",
       "         [-0.02608263, -0.02961171, -0.00025437],\n",
       "         [-0.05394654, -0.0238541 , -0.05682736]],\n",
       " \n",
       "        [[-0.06473758,  0.01251425,  0.02893452],\n",
       "         [-0.06871508,  0.08232716,  0.06172707],\n",
       "         [-0.00369338, -0.00038337,  0.0720856 ]],\n",
       " \n",
       "        [[ 0.08165485,  0.05313494,  0.03537209],\n",
       "         [-0.0398102 , -0.06362373,  0.00284514],\n",
       "         [ 0.0483066 ,  0.04105047,  0.03813034]],\n",
       " \n",
       "        [[ 0.04992572, -0.0083744 , -0.03642641],\n",
       "         [-0.05462795,  0.01869793, -0.03727291],\n",
       "         [-0.03967226, -0.02103005, -0.05969333]],\n",
       " \n",
       "        [[-0.0581988 , -0.01446914,  0.0074608 ],\n",
       "         [ 0.07758795,  0.01600471,  0.03709785],\n",
       "         [ 0.06074919, -0.03638172, -0.05285832]],\n",
       " \n",
       "        [[-0.0208621 ,  0.00294984, -0.04914105],\n",
       "         [-0.06634455, -0.07438064,  0.07233679],\n",
       "         [ 0.03911564,  0.05844941, -0.04943911]],\n",
       " \n",
       "        [[-0.04754104,  0.05913525,  0.06700324],\n",
       "         [-0.07650987, -0.00658648,  0.08390394],\n",
       "         [-0.03944113,  0.07630586,  0.0134961 ]],\n",
       " \n",
       "        [[-0.05605413,  0.05870789, -0.055375  ],\n",
       "         [ 0.05839219, -0.01377858, -0.03724857],\n",
       "         [ 0.02907938, -0.04074613, -0.01448931]],\n",
       " \n",
       "        [[ 0.08217059,  0.03081875, -0.07607486],\n",
       "         [-0.02827908, -0.02029006,  0.08107159],\n",
       "         [-0.0355603 , -0.02184035, -0.01353045]]], dtype=float32), bias=-0.00042318052, layer=1, neuron_number=13, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  3.26962117e-03,   7.60718957e-02,   7.08231106e-02],\n",
       "         [ -3.08212843e-02,   1.55650370e-03,  -3.82578634e-02],\n",
       "         [  1.73458923e-02,  -6.06913585e-03,   4.43087257e-02]],\n",
       " \n",
       "        [[  4.59972955e-02,   6.79595172e-02,   4.52036895e-02],\n",
       "         [ -8.28521624e-02,  -6.94286972e-02,   2.33159494e-02],\n",
       "         [  3.50662060e-02,  -8.06158688e-03,   1.24149462e-02]],\n",
       " \n",
       "        [[  2.03990005e-02,   7.42570907e-02,   8.13743751e-03],\n",
       "         [ -6.76193610e-02,   2.95592584e-02,  -7.17412680e-02],\n",
       "         [  6.01115823e-02,  -5.99159934e-02,   3.82157490e-02]],\n",
       " \n",
       "        [[ -3.35791102e-03,  -1.15001220e-02,   7.95120932e-03],\n",
       "         [ -7.97012523e-02,  -3.75870876e-02,  -2.86418237e-02],\n",
       "         [  1.33430306e-02,  -3.99178341e-02,  -7.90002570e-02]],\n",
       " \n",
       "        [[ -2.79970118e-03,  -1.92082394e-02,   2.29395181e-02],\n",
       "         [ -7.40631074e-02,   1.72314961e-02,   8.06135833e-02],\n",
       "         [ -2.79194098e-02,   1.60091266e-03,  -3.22598815e-02]],\n",
       " \n",
       "        [[  7.96607807e-02,  -6.29433291e-03,   7.15747923e-02],\n",
       "         [ -5.35377786e-02,  -5.88940457e-02,   5.66416197e-02],\n",
       "         [ -6.24763332e-02,  -6.47473410e-02,  -6.11568131e-02]],\n",
       " \n",
       "        [[  1.21531328e-02,  -7.37319291e-02,   7.13259429e-02],\n",
       "         [ -3.85210663e-02,   7.22920522e-02,   3.53298010e-03],\n",
       "         [ -6.70754015e-02,  -5.81979528e-02,   7.76964501e-02]],\n",
       " \n",
       "        [[ -3.39142643e-02,  -7.98829943e-02,   1.70280144e-03],\n",
       "         [  7.76373819e-02,  -2.04169136e-02,  -5.41312136e-02],\n",
       "         [  7.57340863e-02,  -2.47581322e-02,  -5.91306500e-02]],\n",
       " \n",
       "        [[  5.97336031e-02,   2.34612264e-02,   7.46755153e-02],\n",
       "         [ -4.11405079e-02,   6.42843321e-02,   5.08149713e-02],\n",
       "         [ -4.36217450e-02,   7.52461255e-02,  -4.72777197e-03]],\n",
       " \n",
       "        [[  7.01679960e-02,   7.09957108e-02,  -3.65204960e-02],\n",
       "         [ -2.06791572e-02,  -7.47424923e-03,  -6.41005412e-02],\n",
       "         [  7.29928017e-02,   2.94667482e-02,   2.02273820e-02]],\n",
       " \n",
       "        [[  7.29914606e-02,  -3.46634649e-02,   8.29817802e-02],\n",
       "         [  5.44788241e-02,   2.95069963e-02,  -7.82177784e-03],\n",
       "         [  1.85047928e-02,  -1.27373810e-03,   6.95122778e-03]],\n",
       " \n",
       "        [[ -4.81831469e-02,  -6.63359556e-03,  -7.03400746e-02],\n",
       "         [ -1.03539722e-02,   3.51260975e-02,   6.27794266e-02],\n",
       "         [  2.89752726e-02,  -3.75999399e-02,   2.05235602e-03]],\n",
       " \n",
       "        [[  6.59316033e-02,  -1.11376531e-02,  -2.68611442e-02],\n",
       "         [  6.97352597e-03,  -7.53988624e-02,   6.52354350e-03],\n",
       "         [ -5.06200409e-03,   8.63707438e-03,   5.16073070e-02]],\n",
       " \n",
       "        [[  2.66045146e-02,  -6.33210242e-02,   4.66984324e-02],\n",
       "         [  2.39503570e-02,  -1.11145033e-02,   5.99013343e-02],\n",
       "         [ -1.65822543e-02,  -9.18112043e-03,   6.75124750e-02]],\n",
       " \n",
       "        [[  4.34509441e-02,   6.69189245e-02,   7.69060701e-02],\n",
       "         [ -2.45915353e-02,  -3.66922803e-02,   2.95740608e-02],\n",
       "         [  4.20629941e-02,  -7.06346184e-02,   3.22191082e-02]],\n",
       " \n",
       "        [[ -3.51128243e-02,   5.30019589e-02,   7.86989853e-02],\n",
       "         [ -1.36142690e-03,  -8.10158029e-02,   7.05538541e-02],\n",
       "         [  7.83166736e-02,  -1.80133488e-02,   1.22512830e-03]],\n",
       " \n",
       "        [[  6.37250096e-02,   5.49745001e-03,   3.53179947e-02],\n",
       "         [ -3.16594467e-02,  -5.03329448e-02,   6.83529079e-02],\n",
       "         [ -6.24098405e-02,   7.38752782e-02,  -2.08003130e-02]],\n",
       " \n",
       "        [[  5.01387604e-02,  -4.05724905e-02,   5.00863940e-02],\n",
       "         [ -6.95120990e-02,  -5.27567193e-02,  -4.75946106e-02],\n",
       "         [ -4.39049788e-02,   8.07847604e-02,   5.55312261e-04]],\n",
       " \n",
       "        [[  6.69760047e-04,   2.42861547e-02,   1.20139448e-02],\n",
       "         [ -6.37509301e-02,  -2.99487952e-02,  -8.01635757e-02],\n",
       "         [  1.59110464e-02,  -3.19315642e-02,   1.91074200e-02]],\n",
       " \n",
       "        [[  7.64899850e-02,  -6.35332838e-02,   4.05073389e-02],\n",
       "         [  8.05445537e-02,   5.82918599e-02,   1.30639793e-02],\n",
       "         [ -8.06367248e-02,  -3.40027064e-02,   4.30043265e-02]],\n",
       " \n",
       "        [[ -6.01804145e-02,  -3.28279436e-02,   7.64166117e-02],\n",
       "         [ -1.54084265e-02,   5.69210611e-02,   7.97315985e-02],\n",
       "         [  2.13828776e-02,  -8.12198780e-03,  -3.28493156e-02]],\n",
       " \n",
       "        [[  5.37553281e-02,  -1.01610729e-02,   5.81584685e-02],\n",
       "         [  3.13131772e-02,  -2.38061789e-02,  -7.58457482e-02],\n",
       "         [  7.80683404e-05,   7.51557574e-02,  -3.56161632e-02]],\n",
       " \n",
       "        [[ -1.03056838e-04,  -3.05416957e-02,   7.35580698e-02],\n",
       "         [  1.75254350e-03,  -4.75240462e-02,  -7.45075941e-02],\n",
       "         [ -8.04712698e-02,   4.41178717e-02,  -7.64871389e-02]],\n",
       " \n",
       "        [[  4.19055745e-02,   2.21308190e-02,   1.36786569e-02],\n",
       "         [ -1.36251245e-02,   7.98662007e-02,   7.68965036e-02],\n",
       "         [  3.27109322e-02,   7.02032074e-02,  -4.39903624e-02]],\n",
       " \n",
       "        [[ -4.17226665e-02,   2.43013259e-02,   4.96998839e-02],\n",
       "         [  4.05417979e-02,   8.20795670e-02,   2.42815949e-02],\n",
       "         [ -5.59296459e-02,   7.58911818e-02,   4.74866182e-02]],\n",
       " \n",
       "        [[ -7.52141178e-02,  -4.38025780e-02,   7.40594193e-02],\n",
       "         [  3.33835930e-02,  -1.73957422e-02,   7.52377734e-02],\n",
       "         [ -7.47342706e-02,   1.55456215e-02,   1.51375756e-02]],\n",
       " \n",
       "        [[ -4.55039367e-02,   5.23111671e-02,   5.41958325e-02],\n",
       "         [  7.51740932e-02,  -6.31270930e-02,  -7.44578317e-02],\n",
       "         [ -2.94707064e-02,  -8.02663490e-02,  -7.22185150e-03]],\n",
       " \n",
       "        [[ -3.53300348e-02,  -3.21085379e-02,  -1.49292150e-03],\n",
       "         [  2.72788312e-02,  -4.80815060e-02,   6.05543517e-02],\n",
       "         [ -7.33772442e-02,  -2.71829721e-02,   2.13265978e-02]],\n",
       " \n",
       "        [[ -5.52225076e-02,   3.41097564e-02,   7.94710070e-02],\n",
       "         [  4.02987450e-02,  -6.73410818e-02,   7.02222288e-02],\n",
       "         [  7.39181340e-02,  -3.12769599e-02,   7.99336657e-02]],\n",
       " \n",
       "        [[  1.78928711e-02,   3.36962529e-02,   2.16628127e-02],\n",
       "         [ -5.81723228e-02,  -2.65404079e-02,  -6.18130565e-02],\n",
       "         [  1.13102887e-02,   2.92919967e-02,  -5.71762919e-02]],\n",
       " \n",
       "        [[ -4.64875326e-02,   3.68861319e-03,  -1.57470349e-02],\n",
       "         [  1.93407815e-02,  -7.69044533e-02,   6.80635637e-03],\n",
       "         [  2.37056985e-02,   1.53337894e-02,   6.24696277e-02]],\n",
       " \n",
       "        [[  3.02612334e-02,   8.43367260e-03,  -1.77443698e-02],\n",
       "         [  3.82450819e-02,   2.77736485e-02,  -4.44127135e-02],\n",
       "         [  6.93044066e-03,   5.48253618e-02,  -5.33780903e-02]]], dtype=float32), bias=-0.00099905743, layer=1, neuron_number=14, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.02058093, -0.04001101, -0.03998917],\n",
       "         [-0.06732406,  0.03760098, -0.06206624],\n",
       "         [-0.02175052, -0.01584757,  0.04207393]],\n",
       " \n",
       "        [[ 0.03797964, -0.03732137, -0.00305885],\n",
       "         [ 0.05933172, -0.0389571 , -0.04120152],\n",
       "         [-0.05049883, -0.02280933, -0.07573125]],\n",
       " \n",
       "        [[-0.04710829,  0.02208819, -0.06271075],\n",
       "         [ 0.05646475,  0.02364516, -0.0682824 ],\n",
       "         [-0.06945048, -0.04431503,  0.04285197]],\n",
       " \n",
       "        [[ 0.07719304,  0.07934055,  0.06406445],\n",
       "         [-0.03588887, -0.06931771, -0.06235007],\n",
       "         [ 0.08283903,  0.01955662,  0.07579782]],\n",
       " \n",
       "        [[ 0.01966123, -0.06168354,  0.01364886],\n",
       "         [ 0.04298224,  0.08427815, -0.05697642],\n",
       "         [-0.01974421, -0.03918461,  0.0296913 ]],\n",
       " \n",
       "        [[ 0.0029277 , -0.01657601, -0.04113405],\n",
       "         [-0.07057809, -0.0475689 , -0.01449912],\n",
       "         [ 0.05090401, -0.08066221, -0.05112598]],\n",
       " \n",
       "        [[ 0.06354718,  0.04335355, -0.06600521],\n",
       "         [-0.05053244, -0.02472044, -0.03341728],\n",
       "         [ 0.0587901 , -0.06083484,  0.04891809]],\n",
       " \n",
       "        [[-0.06354745,  0.00713975, -0.03802843],\n",
       "         [-0.04377662,  0.00397949,  0.02548765],\n",
       "         [ 0.04558146,  0.01573701,  0.05233738]],\n",
       " \n",
       "        [[ 0.00422182, -0.05104471, -0.02311878],\n",
       "         [-0.06266899,  0.06695188, -0.05011932],\n",
       "         [ 0.00079749, -0.05958164, -0.03436162]],\n",
       " \n",
       "        [[ 0.058798  ,  0.05553401,  0.06222294],\n",
       "         [ 0.07755811, -0.07962326, -0.00175754],\n",
       "         [-0.04660111, -0.00718086, -0.00470473]],\n",
       " \n",
       "        [[ 0.02647496, -0.01799186, -0.08213334],\n",
       "         [-0.07175215,  0.08003347,  0.06236154],\n",
       "         [-0.04219931, -0.08153351,  0.05383805]],\n",
       " \n",
       "        [[-0.0540214 ,  0.01559681,  0.00899338],\n",
       "         [ 0.04686126,  0.01474174,  0.04926955],\n",
       "         [-0.03527669, -0.0559909 , -0.08234505]],\n",
       " \n",
       "        [[ 0.05200903, -0.03337673,  0.03453645],\n",
       "         [-0.01002112, -0.04645158, -0.04111456],\n",
       "         [-0.01316783, -0.0411238 , -0.0467934 ]],\n",
       " \n",
       "        [[ 0.00714906, -0.05648872,  0.0508588 ],\n",
       "         [-0.05959927,  0.0233247 , -0.02677206],\n",
       "         [ 0.07898118, -0.04265612,  0.02199638]],\n",
       " \n",
       "        [[ 0.02581387,  0.07977089,  0.07385961],\n",
       "         [-0.05734474,  0.02666543,  0.05212723],\n",
       "         [-0.06408764,  0.07370111,  0.05366573]],\n",
       " \n",
       "        [[-0.0230748 , -0.0441667 ,  0.01399942],\n",
       "         [ 0.02042657, -0.03904448,  0.0455059 ],\n",
       "         [ 0.0077218 ,  0.07498001,  0.04406564]],\n",
       " \n",
       "        [[ 0.03646346, -0.04133679, -0.00581687],\n",
       "         [-0.00935987, -0.07143781, -0.00845841],\n",
       "         [-0.06234385,  0.04742686, -0.0485359 ]],\n",
       " \n",
       "        [[-0.03179737, -0.03666993,  0.0793409 ],\n",
       "         [ 0.00796932, -0.02436346, -0.00551995],\n",
       "         [ 0.02837429,  0.06663451, -0.07643303]],\n",
       " \n",
       "        [[-0.07520426, -0.05417899,  0.01308506],\n",
       "         [-0.03166521, -0.04152834,  0.02506809],\n",
       "         [ 0.01836134,  0.04175418,  0.02027983]],\n",
       " \n",
       "        [[-0.02439935, -0.0056424 ,  0.01937766],\n",
       "         [-0.01093915, -0.06974851, -0.01044425],\n",
       "         [ 0.06218215, -0.03676299, -0.00704792]],\n",
       " \n",
       "        [[-0.02402744,  0.05038825,  0.00465164],\n",
       "         [ 0.06749282, -0.06517287,  0.00935107],\n",
       "         [ 0.02092658, -0.04754677,  0.03196525]],\n",
       " \n",
       "        [[ 0.03217022, -0.01087912, -0.01056232],\n",
       "         [ 0.07233467, -0.01736483, -0.01540154],\n",
       "         [ 0.05577596, -0.07551878, -0.02339103]],\n",
       " \n",
       "        [[-0.04097031,  0.01511127, -0.05357561],\n",
       "         [ 0.00438894, -0.02067768, -0.00617966],\n",
       "         [ 0.0024006 , -0.07781019,  0.02290012]],\n",
       " \n",
       "        [[-0.03689594, -0.01075581,  0.03592463],\n",
       "         [ 0.0341613 ,  0.06029253, -0.07417788],\n",
       "         [-0.02631637, -0.07388409,  0.01509076]],\n",
       " \n",
       "        [[-0.06555314,  0.01206272,  0.04139817],\n",
       "         [ 0.02227418,  0.08137696, -0.02715542],\n",
       "         [-0.04124021,  0.05128036,  0.01073127]],\n",
       " \n",
       "        [[-0.05022899, -0.01574363,  0.07356215],\n",
       "         [ 0.03115082, -0.0607494 ,  0.05119564],\n",
       "         [ 0.04995065,  0.01844539,  0.06740692]],\n",
       " \n",
       "        [[-0.04468822, -0.01729582, -0.07356092],\n",
       "         [-0.01069333, -0.0401058 , -0.06250304],\n",
       "         [-0.04596719,  0.01449527,  0.08290633]],\n",
       " \n",
       "        [[-0.05026522, -0.01422009,  0.07037535],\n",
       "         [-0.02757942,  0.02493945,  0.04823017],\n",
       "         [-0.00789925,  0.06100804,  0.02422786]],\n",
       " \n",
       "        [[-0.04644129,  0.0172522 ,  0.00345404],\n",
       "         [-0.05951357, -0.01840871, -0.03344234],\n",
       "         [-0.02042459, -0.02931107, -0.01526892]],\n",
       " \n",
       "        [[-0.03792388,  0.03785493,  0.07937141],\n",
       "         [ 0.07392534, -0.07827847,  0.02378824],\n",
       "         [-0.03669083, -0.05777467, -0.0772501 ]],\n",
       " \n",
       "        [[ 0.07586383,  0.02953451,  0.00664018],\n",
       "         [ 0.04251258, -0.05457744, -0.06336395],\n",
       "         [ 0.02451782, -0.02383121, -0.08059485]],\n",
       " \n",
       "        [[-0.04745661,  0.01545199, -0.02938531],\n",
       "         [-0.02347108,  0.0411644 ,  0.00489176],\n",
       "         [ 0.05482009,  0.05100268, -0.05525262]]], dtype=float32), bias=0.0017031096, layer=1, neuron_number=15, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.05018582, -0.02276272, -0.00226164],\n",
       "         [-0.02514798, -0.05861344, -0.03372433],\n",
       "         [-0.03540534, -0.00159988, -0.02815367]],\n",
       " \n",
       "        [[-0.04895193, -0.01598471,  0.02472124],\n",
       "         [ 0.02462223,  0.0600255 ,  0.00390872],\n",
       "         [ 0.00573567, -0.07439812, -0.02475357]],\n",
       " \n",
       "        [[ 0.00718387,  0.07758197, -0.04122643],\n",
       "         [-0.01549616,  0.03316803,  0.0224277 ],\n",
       "         [-0.03637695,  0.06714489,  0.08081554]],\n",
       " \n",
       "        [[-0.01294556, -0.05600215,  0.02466878],\n",
       "         [-0.0006044 , -0.05987935, -0.06707098],\n",
       "         [ 0.02262903, -0.07310982,  0.06857567]],\n",
       " \n",
       "        [[ 0.08217041,  0.06682062,  0.00235842],\n",
       "         [-0.05326457, -0.06214864, -0.06928092],\n",
       "         [-0.07818603,  0.03607638,  0.01722598]],\n",
       " \n",
       "        [[ 0.00324199, -0.03100309,  0.04771153],\n",
       "         [ 0.0299254 ,  0.06821759, -0.01178099],\n",
       "         [-0.04346093,  0.00572352,  0.04307086]],\n",
       " \n",
       "        [[ 0.04828028, -0.08298497,  0.07871502],\n",
       "         [-0.00360725, -0.0481946 ,  0.06542239],\n",
       "         [ 0.06750973, -0.07338955,  0.00747231]],\n",
       " \n",
       "        [[-0.06730824, -0.00501032, -0.06559785],\n",
       "         [ 0.03073775, -0.04728512,  0.07293919],\n",
       "         [-0.02292521,  0.02499472,  0.04184107]],\n",
       " \n",
       "        [[-0.0230065 , -0.00529069,  0.02022678],\n",
       "         [-0.04127379, -0.07842164, -0.06568918],\n",
       "         [ 0.04174977, -0.04244163,  0.05774818]],\n",
       " \n",
       "        [[ 0.03085725, -0.0423466 ,  0.08028924],\n",
       "         [ 0.06409456,  0.01293094, -0.00698179],\n",
       "         [ 0.05291956,  0.05702056, -0.04356979]],\n",
       " \n",
       "        [[ 0.01261556,  0.0123025 , -0.01390509],\n",
       "         [ 0.00068581, -0.04611541,  0.07239736],\n",
       "         [ 0.00665865, -0.04149897,  0.07156788]],\n",
       " \n",
       "        [[ 0.01376795,  0.05303734, -0.02609113],\n",
       "         [-0.06212925, -0.02915987,  0.01872891],\n",
       "         [ 0.04794267, -0.0351662 , -0.04066626]],\n",
       " \n",
       "        [[-0.06884988,  0.08027686, -0.05903352],\n",
       "         [-0.05331467, -0.04980944, -0.01236022],\n",
       "         [-0.07120852,  0.05924639, -0.01806369]],\n",
       " \n",
       "        [[-0.06866188,  0.02937586, -0.02652513],\n",
       "         [ 0.06534389, -0.07022098,  0.04824961],\n",
       "         [ 0.03481637, -0.05273306,  0.02164917]],\n",
       " \n",
       "        [[-0.07828227,  0.07576678,  0.02302073],\n",
       "         [-0.06193601, -0.02614036, -0.06658678],\n",
       "         [-0.03197557, -0.02840262,  0.06671279]],\n",
       " \n",
       "        [[-0.01721314,  0.04294017, -0.05638656],\n",
       "         [ 0.05584848, -0.0558351 , -0.06395224],\n",
       "         [ 0.03444176,  0.00143088,  0.02120905]],\n",
       " \n",
       "        [[-0.01333479, -0.01063546, -0.0358198 ],\n",
       "         [-0.02462506, -0.07541797, -0.02349287],\n",
       "         [ 0.07408005,  0.05087273, -0.02566084]],\n",
       " \n",
       "        [[-0.01987625,  0.07589547,  0.00681432],\n",
       "         [-0.08331601, -0.02037228, -0.03120244],\n",
       "         [ 0.01635945,  0.07983726, -0.0337663 ]],\n",
       " \n",
       "        [[-0.04865871,  0.07863543,  0.02916467],\n",
       "         [-0.07304572, -0.01632507, -0.02377156],\n",
       "         [-0.07028399, -0.0082478 , -0.002943  ]],\n",
       " \n",
       "        [[ 0.03988244, -0.05431832, -0.04417909],\n",
       "         [-0.06396686, -0.03664244, -0.06755328],\n",
       "         [-0.02653078,  0.04459669,  0.07140306]],\n",
       " \n",
       "        [[ 0.05298277,  0.03715552,  0.04097045],\n",
       "         [ 0.05591757, -0.06685877, -0.07453549],\n",
       "         [-0.04910166,  0.06026757,  0.07112204]],\n",
       " \n",
       "        [[ 0.05839808,  0.05553771, -0.05798104],\n",
       "         [ 0.06670991, -0.07554611, -0.0813166 ],\n",
       "         [ 0.0576509 ,  0.0218581 , -0.00584683]],\n",
       " \n",
       "        [[ 0.03386481,  0.07494988,  0.02002457],\n",
       "         [-0.05288181,  0.04929693, -0.05336174],\n",
       "         [ 0.08198367, -0.0807941 ,  0.07099008]],\n",
       " \n",
       "        [[-0.03283875,  0.00770422, -0.04868634],\n",
       "         [-0.04875095, -0.00181767,  0.02345254],\n",
       "         [ 0.01408654,  0.0482869 ,  0.06067706]],\n",
       " \n",
       "        [[ 0.04580803, -0.02898132,  0.05248386],\n",
       "         [-0.04891269,  0.04457327,  0.03649546],\n",
       "         [-0.07804322, -0.00541114,  0.00102269]],\n",
       " \n",
       "        [[ 0.03850947, -0.00558801, -0.05045647],\n",
       "         [ 0.02016853, -0.04702375,  0.02673081],\n",
       "         [ 0.02235579,  0.01182269,  0.01972464]],\n",
       " \n",
       "        [[ 0.04599667, -0.05847809,  0.0149867 ],\n",
       "         [ 0.05238855, -0.08338547, -0.01622796],\n",
       "         [ 0.07740662, -0.00711024, -0.04032902]],\n",
       " \n",
       "        [[ 0.02399167, -0.08308139,  0.05818405],\n",
       "         [-0.06815904,  0.07565946, -0.0664787 ],\n",
       "         [-0.02199672,  0.01896151, -0.06356974]],\n",
       " \n",
       "        [[-0.00203331,  0.06209007, -0.06996339],\n",
       "         [-0.05107676,  0.05944149,  0.0343825 ],\n",
       "         [ 0.0552929 ,  0.00744084, -0.05865683]],\n",
       " \n",
       "        [[-0.05868091, -0.05069363,  0.00071619],\n",
       "         [ 0.0713852 ,  0.05803196,  0.06733323],\n",
       "         [ 0.06662099,  0.00932526,  0.00143863]],\n",
       " \n",
       "        [[ 0.015187  , -0.01006686, -0.05873666],\n",
       "         [ 0.06924009, -0.02318512,  0.06530897],\n",
       "         [-0.00102754,  0.06189469, -0.00751731]],\n",
       " \n",
       "        [[-0.04641448,  0.05091006,  0.07820863],\n",
       "         [-0.08160901,  0.00255645, -0.04307562],\n",
       "         [-0.0826655 , -0.02492783, -0.03125571]]], dtype=float32), bias=-0.0005123369, layer=1, neuron_number=16, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.04967777,  0.08300647, -0.07065197],\n",
       "         [ 0.03303951,  0.01167543, -0.08260387],\n",
       "         [ 0.03381693,  0.05387746,  0.05171454]],\n",
       " \n",
       "        [[-0.01931345, -0.00694972, -0.03055339],\n",
       "         [-0.02679564, -0.00453343, -0.03749016],\n",
       "         [-0.03914693, -0.02920922, -0.05254994]],\n",
       " \n",
       "        [[-0.01065169, -0.05419205, -0.00569667],\n",
       "         [ 0.04952554, -0.00801552,  0.07625832],\n",
       "         [ 0.01554464, -0.06157515,  0.04552794]],\n",
       " \n",
       "        [[ 0.04831181, -0.00166738,  0.00982018],\n",
       "         [ 0.05003895, -0.0443675 ,  0.03638649],\n",
       "         [ 0.05521288,  0.01780183,  0.01389619]],\n",
       " \n",
       "        [[-0.07616638,  0.03414868, -0.01695264],\n",
       "         [ 0.03397631,  0.06721934,  0.05865027],\n",
       "         [-0.04695591,  0.02566815, -0.06127222]],\n",
       " \n",
       "        [[-0.02010085,  0.04091191, -0.04559046],\n",
       "         [ 0.01188042, -0.0652323 ,  0.05324509],\n",
       "         [-0.01229148,  0.07874626, -0.03727397]],\n",
       " \n",
       "        [[-0.03551736,  0.024006  , -0.03326402],\n",
       "         [ 0.03930486, -0.02912238, -0.06705059],\n",
       "         [ 0.06230957, -0.02648782, -0.04460412]],\n",
       " \n",
       "        [[-0.04600916,  0.04952386, -0.06665968],\n",
       "         [ 0.05473188, -0.07192769,  0.07956258],\n",
       "         [-0.0482882 , -0.03457789,  0.04191398]],\n",
       " \n",
       "        [[ 0.04474822, -0.05590504, -0.07596395],\n",
       "         [-0.01859875,  0.03302301, -0.02879833],\n",
       "         [ 0.02899279, -0.02149787, -0.05786932]],\n",
       " \n",
       "        [[ 0.06053055, -0.0429595 , -0.05601514],\n",
       "         [ 0.07353373, -0.04693576, -0.03027417],\n",
       "         [-0.04457758, -0.01125132,  0.05981231]],\n",
       " \n",
       "        [[ 0.0310015 , -0.0691511 ,  0.0285346 ],\n",
       "         [-0.00648265,  0.0179813 ,  0.04844579],\n",
       "         [ 0.03983485,  0.01264942, -0.06506687]],\n",
       " \n",
       "        [[ 0.0778711 , -0.04270774,  0.06885298],\n",
       "         [ 0.05331589, -0.05194734, -0.07687879],\n",
       "         [-0.0026259 , -0.00900168, -0.03375965]],\n",
       " \n",
       "        [[ 0.03465833, -0.00093085, -0.02278056],\n",
       "         [-0.00654955,  0.07161988,  0.05239527],\n",
       "         [ 0.02130156,  0.0339913 ,  0.02919715]],\n",
       " \n",
       "        [[ 0.04095248, -0.06258211, -0.04781465],\n",
       "         [ 0.06452159, -0.05218669, -0.05331588],\n",
       "         [-0.06774954, -0.00625764,  0.0208406 ]],\n",
       " \n",
       "        [[ 0.0128043 ,  0.03867439, -0.07624325],\n",
       "         [ 0.05754806,  0.06324599,  0.0606453 ],\n",
       "         [ 0.04694419, -0.03497389,  0.02036475]],\n",
       " \n",
       "        [[ 0.04664478,  0.00116624,  0.04651572],\n",
       "         [-0.07201795,  0.04349453, -0.07267495],\n",
       "         [ 0.03088897, -0.03876704,  0.07791392]],\n",
       " \n",
       "        [[ 0.04856467,  0.0736732 , -0.02384258],\n",
       "         [-0.00447048,  0.00757479, -0.03040371],\n",
       "         [-0.07427059, -0.02535444, -0.01925534]],\n",
       " \n",
       "        [[-0.01836998,  0.02528229, -0.03089041],\n",
       "         [ 0.04622319,  0.07009816, -0.03316452],\n",
       "         [-0.02656611, -0.01940279, -0.04293493]],\n",
       " \n",
       "        [[ 0.06763155, -0.07765324,  0.02568621],\n",
       "         [ 0.07340027,  0.02192264, -0.03710756],\n",
       "         [ 0.06339414, -0.01437879,  0.06832357]],\n",
       " \n",
       "        [[-0.04508011,  0.0683319 , -0.0706185 ],\n",
       "         [-0.043355  ,  0.06675758,  0.01153166],\n",
       "         [-0.03329258,  0.0490828 ,  0.02861368]],\n",
       " \n",
       "        [[-0.03349069, -0.03753767, -0.07661524],\n",
       "         [ 0.06876954,  0.01713799,  0.02028885],\n",
       "         [ 0.03928351, -0.04810116, -0.07628786]],\n",
       " \n",
       "        [[ 0.07547636, -0.06175254,  0.0834625 ],\n",
       "         [-0.06298614,  0.0333255 , -0.08094691],\n",
       "         [-0.04919127, -0.05063231, -0.04248649]],\n",
       " \n",
       "        [[-0.05038716,  0.05665455, -0.0720978 ],\n",
       "         [ 0.02267602, -0.04579512, -0.00010389],\n",
       "         [ 0.04629981, -0.00927554,  0.00549589]],\n",
       " \n",
       "        [[ 0.06775312, -0.00928516, -0.05017271],\n",
       "         [-0.0534441 ,  0.05754073, -0.03128819],\n",
       "         [-0.01583993, -0.02458625,  0.05336468]],\n",
       " \n",
       "        [[-0.07193518,  0.01575624,  0.04241895],\n",
       "         [-0.04567851, -0.02284371,  0.01685624],\n",
       "         [-0.08192244,  0.01040412, -0.06034288]],\n",
       " \n",
       "        [[ 0.03322891,  0.06042263,  0.06435263],\n",
       "         [-0.06263676,  0.01282418, -0.06446251],\n",
       "         [ 0.04256111, -0.03303614,  0.02294662]],\n",
       " \n",
       "        [[ 0.03901029,  0.05923407, -0.06848692],\n",
       "         [-0.04084317, -0.00359084,  0.06157354],\n",
       "         [-0.04344807, -0.03572129,  0.06516173]],\n",
       " \n",
       "        [[ 0.07544943, -0.08078709, -0.03180415],\n",
       "         [-0.0591372 ,  0.04685738,  0.03721263],\n",
       "         [-0.04739897, -0.03698591,  0.06431655]],\n",
       " \n",
       "        [[-0.00250514,  0.04569726,  0.0230275 ],\n",
       "         [-0.05165239, -0.02299897,  0.00478846],\n",
       "         [ 0.02553841,  0.06948126,  0.07338608]],\n",
       " \n",
       "        [[-0.00166019, -0.00856718, -0.0703304 ],\n",
       "         [-0.02272204,  0.00457049, -0.03727253],\n",
       "         [-0.00432608, -0.07579306, -0.03652696]],\n",
       " \n",
       "        [[-0.04653184, -0.04108306, -0.03690482],\n",
       "         [-0.07095496, -0.00536809,  0.02281114],\n",
       "         [-0.06478301,  0.05526827,  0.04872119]],\n",
       " \n",
       "        [[-0.07519406, -0.05255083,  0.06759915],\n",
       "         [ 0.04234171, -0.06457108, -0.00354967],\n",
       "         [ 0.07978892,  0.01570908, -0.01294595]]], dtype=float32), bias=-0.0010295773, layer=1, neuron_number=17, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  1.59954429e-02,  -4.63615581e-02,  -7.40894973e-02],\n",
       "         [  7.99823254e-02,   2.26783864e-02,   7.27296472e-02],\n",
       "         [  4.79735173e-02,   2.55590435e-02,   4.55155782e-03]],\n",
       " \n",
       "        [[ -2.90689934e-02,   6.32115602e-02,   3.14102918e-02],\n",
       "         [  4.62962277e-02,  -7.54528567e-02,  -1.43015198e-02],\n",
       "         [ -2.77216490e-02,   4.87879105e-02,   6.28896803e-02]],\n",
       " \n",
       "        [[  7.57488310e-02,   7.65223727e-02,  -1.38390260e-02],\n",
       "         [ -3.62774581e-02,   6.32711500e-02,  -3.70004177e-02],\n",
       "         [ -5.00539877e-02,  -1.62412431e-02,  -6.85695410e-02]],\n",
       " \n",
       "        [[ -1.14635173e-02,   1.55285764e-02,  -1.69012565e-02],\n",
       "         [ -5.12870662e-02,   3.42232846e-02,   2.33068112e-02],\n",
       "         [ -3.80400233e-02,  -4.65603881e-02,  -8.08790252e-02]],\n",
       " \n",
       "        [[ -1.77660380e-02,   5.41232117e-02,  -5.73410206e-02],\n",
       "         [ -5.26837222e-02,  -5.66952266e-02,   3.93352024e-02],\n",
       "         [  2.53854133e-02,   7.96054304e-02,  -7.41732270e-02]],\n",
       " \n",
       "        [[ -2.83641554e-03,  -3.84522192e-02,   7.68204182e-02],\n",
       "         [  3.39687280e-02,   6.50430843e-02,   2.95914207e-02],\n",
       "         [  5.09953834e-02,  -6.20454662e-02,   3.14363949e-02]],\n",
       " \n",
       "        [[  4.29514013e-02,   3.80937010e-02,   5.34766540e-02],\n",
       "         [  2.83704922e-02,  -3.56192514e-02,   1.47240032e-02],\n",
       "         [  9.76384268e-04,   8.27524439e-02,   1.08337272e-02]],\n",
       " \n",
       "        [[  7.04085529e-02,  -4.86337394e-02,  -6.15560673e-02],\n",
       "         [ -3.49676423e-02,   6.90878276e-03,   3.15062664e-02],\n",
       "         [ -3.64000835e-02,   7.56774619e-02,  -2.07979325e-02]],\n",
       " \n",
       "        [[ -6.91518337e-02,   6.47491217e-02,   4.03299220e-02],\n",
       "         [  2.20615603e-03,  -8.69287457e-03,  -7.59993121e-02],\n",
       "         [  6.20237403e-02,  -6.58617839e-02,   7.43180811e-02]],\n",
       " \n",
       "        [[ -2.69619487e-02,   8.26145895e-03,  -4.85745305e-03],\n",
       "         [  2.94756796e-02,  -7.31691793e-02,   3.28085832e-02],\n",
       "         [  2.90045589e-02,  -7.20880851e-02,   6.15707710e-02]],\n",
       " \n",
       "        [[ -3.63284089e-02,  -4.63395119e-02,   7.00417608e-02],\n",
       "         [  4.68577109e-02,  -3.28225903e-02,  -6.37198091e-02],\n",
       "         [  5.95961623e-02,  -6.18906319e-02,  -5.49925007e-02]],\n",
       " \n",
       "        [[ -7.58379102e-02,   2.63920031e-03,   3.19850887e-03],\n",
       "         [  2.58734673e-02,   3.24502997e-02,   1.22571569e-02],\n",
       "         [ -1.66873529e-03,   8.18678141e-02,   7.28916898e-02]],\n",
       " \n",
       "        [[  2.08593104e-02,   2.01764852e-02,   1.76476669e-02],\n",
       "         [ -8.27563033e-02,   2.65548322e-02,   1.13542480e-02],\n",
       "         [  3.57654616e-02,   2.72748228e-02,   8.12041759e-02]],\n",
       " \n",
       "        [[  3.97393890e-02,   4.01001871e-02,  -8.95828567e-03],\n",
       "         [ -6.04869649e-02,   6.14051521e-03,  -7.13939313e-03],\n",
       "         [ -5.08803921e-03,   2.13329531e-02,  -8.07236806e-02]],\n",
       " \n",
       "        [[ -6.68869466e-02,  -4.21397686e-02,   2.79854294e-02],\n",
       "         [  1.61321349e-02,   1.88890547e-02,  -4.77009220e-03],\n",
       "         [ -6.97070509e-02,  -3.52180423e-03,  -3.25428471e-02]],\n",
       " \n",
       "        [[ -6.99412003e-02,   6.04322441e-02,   6.97397217e-02],\n",
       "         [ -2.36351800e-04,   4.25181277e-02,   3.46681960e-02],\n",
       "         [ -9.31927701e-04,   2.44631600e-02,  -1.62240565e-02]],\n",
       " \n",
       "        [[ -2.31679063e-02,  -6.68878406e-02,   3.12293861e-02],\n",
       "         [  1.24251992e-02,   6.61146566e-02,  -3.39717790e-02],\n",
       "         [ -3.64216939e-02,   2.58191396e-02,   5.73798195e-02]],\n",
       " \n",
       "        [[ -6.55401126e-02,  -2.69744489e-02,  -1.19851921e-02],\n",
       "         [ -1.50784431e-02,   5.68704195e-02,  -4.26432118e-02],\n",
       "         [  5.12321256e-02,  -4.15705144e-02,  -3.77822258e-02]],\n",
       " \n",
       "        [[  6.26850724e-02,   1.79621037e-02,   2.13281531e-02],\n",
       "         [  4.08581384e-02,  -1.34127541e-02,   3.20928777e-03],\n",
       "         [ -6.39551878e-02,   4.11633216e-02,  -1.70886759e-02]],\n",
       " \n",
       "        [[  8.23921040e-02,  -8.20925608e-02,  -7.49224350e-02],\n",
       "         [  2.47657076e-02,  -4.71828282e-02,   3.84710133e-02],\n",
       "         [ -3.49053517e-02,   9.17365029e-03,  -2.81251390e-02]],\n",
       " \n",
       "        [[ -6.42830878e-02,  -3.61401886e-02,   5.34739271e-02],\n",
       "         [  5.26948310e-02,  -4.45222445e-02,  -4.86950623e-03],\n",
       "         [  3.78839374e-02,   6.85915351e-02,  -1.83734372e-02]],\n",
       " \n",
       "        [[  4.19249618e-03,  -9.92091745e-03,   1.60474032e-02],\n",
       "         [  6.84595034e-02,   1.23912692e-02,   4.14453335e-02],\n",
       "         [ -1.05935363e-02,  -7.51561448e-02,  -6.95675239e-02]],\n",
       " \n",
       "        [[  5.75046390e-02,  -5.02370037e-02,  -6.82675391e-02],\n",
       "         [ -1.64385028e-02,  -8.32955763e-02,  -4.40773666e-02],\n",
       "         [ -5.77023672e-03,  -7.69509971e-02,   7.83708766e-02]],\n",
       " \n",
       "        [[  2.14146972e-02,  -7.01498911e-02,  -7.39408806e-02],\n",
       "         [ -2.58020870e-02,   1.94105338e-02,   8.79551936e-03],\n",
       "         [  2.24360023e-02,   8.20719227e-02,   3.47289667e-02]],\n",
       " \n",
       "        [[ -4.57469225e-02,   7.86742866e-02,  -2.21740417e-02],\n",
       "         [  4.01066504e-02,   6.51190579e-02,   1.11209657e-02],\n",
       "         [  5.34103625e-02,  -3.11763417e-02,  -1.79439131e-02]],\n",
       " \n",
       "        [[ -5.93190528e-02,   1.98851880e-02,   4.56820391e-02],\n",
       "         [  3.31955068e-02,  -4.76095341e-02,  -3.05144116e-04],\n",
       "         [ -1.94493048e-02,  -1.50808925e-02,   8.09408054e-02]],\n",
       " \n",
       "        [[ -3.07362396e-02,  -6.66530728e-02,  -1.00205587e-02],\n",
       "         [ -3.26407999e-02,   1.73663367e-02,  -3.23336311e-02],\n",
       "         [ -7.34012574e-02,   5.36082387e-02,   4.66881618e-02]],\n",
       " \n",
       "        [[  2.42064241e-02,   7.12805763e-02,  -2.24231072e-02],\n",
       "         [ -4.16024961e-02,  -8.92321207e-03,  -4.16823514e-02],\n",
       "         [ -7.95813948e-02,  -3.58529910e-02,  -5.57671376e-02]],\n",
       " \n",
       "        [[  4.19751182e-02,  -1.89456623e-02,   6.76590279e-02],\n",
       "         [  6.18964285e-02,  -5.90465553e-02,   4.26340289e-02],\n",
       "         [  4.85125296e-02,  -6.61338866e-02,   6.56625256e-02]],\n",
       " \n",
       "        [[ -4.64377813e-02,   7.94647858e-02,   2.35652905e-02],\n",
       "         [  3.64722461e-02,  -1.40907746e-02,  -7.33622983e-02],\n",
       "         [ -6.46965131e-02,  -5.75378444e-03,  -7.86924064e-02]],\n",
       " \n",
       "        [[ -4.08110395e-02,  -4.84751537e-02,   3.78381871e-02],\n",
       "         [ -1.66919325e-02,  -3.54668722e-02,   3.05901933e-02],\n",
       "         [ -3.63979116e-02,  -5.92474677e-02,  -6.32958690e-05]],\n",
       " \n",
       "        [[ -2.40760706e-02,  -6.31011054e-02,  -7.12821782e-02],\n",
       "         [  1.90888625e-02,   8.17360580e-02,   2.19064504e-02],\n",
       "         [ -5.52783310e-02,   2.52057463e-02,   3.14569473e-02]]], dtype=float32), bias=0.00012469158, layer=1, neuron_number=18, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.03646953,  0.01859442,  0.06097033],\n",
       "         [ 0.04901921,  0.07353928,  0.02891161],\n",
       "         [-0.0500207 ,  0.06041421,  0.03089265]],\n",
       " \n",
       "        [[ 0.04719203,  0.03478613,  0.08012262],\n",
       "         [ 0.04722846,  0.01068248,  0.05725163],\n",
       "         [ 0.00125106, -0.01677035, -0.07251298]],\n",
       " \n",
       "        [[ 0.0347909 ,  0.01396332,  0.00713348],\n",
       "         [-0.05691659,  0.03725872,  0.06511223],\n",
       "         [ 0.02775053,  0.07808588, -0.0354173 ]],\n",
       " \n",
       "        [[-0.06579502, -0.05884555, -0.00309405],\n",
       "         [ 0.04484231, -0.0772901 , -0.03453579],\n",
       "         [-0.02005168, -0.04884492,  0.07798915]],\n",
       " \n",
       "        [[ 0.05399746, -0.05727902,  0.06995399],\n",
       "         [ 0.03498221,  0.03523628, -0.0721605 ],\n",
       "         [ 0.03721943,  0.05679087, -0.06945235]],\n",
       " \n",
       "        [[ 0.06128752,  0.05722567, -0.05432678],\n",
       "         [ 0.04503256,  0.08295373, -0.0220447 ],\n",
       "         [ 0.05547265, -0.00865465, -0.03898158]],\n",
       " \n",
       "        [[-0.03588189,  0.02662655, -0.05301882],\n",
       "         [-0.04641454,  0.0020566 , -0.0092138 ],\n",
       "         [ 0.02290433, -0.05960413, -0.02615676]],\n",
       " \n",
       "        [[-0.06710603,  0.04831425,  0.05686039],\n",
       "         [-0.06126517,  0.02440007, -0.02012465],\n",
       "         [-0.07612084,  0.05664591,  0.04779767]],\n",
       " \n",
       "        [[-0.02251225,  0.05916032,  0.01489057],\n",
       "         [ 0.02660848,  0.06164661, -0.02964232],\n",
       "         [ 0.06432766, -0.0709975 ,  0.05750056]],\n",
       " \n",
       "        [[-0.00015208, -0.02794825, -0.04173739],\n",
       "         [-0.04592941,  0.04535805,  0.00097196],\n",
       "         [ 0.05627351, -0.06267741, -0.03317443]],\n",
       " \n",
       "        [[ 0.06963664,  0.07327735,  0.07049041],\n",
       "         [-0.02459724,  0.03497002, -0.04989209],\n",
       "         [-0.01975603, -0.00819508, -0.07516598]],\n",
       " \n",
       "        [[-0.03775217, -0.03146494, -0.07841884],\n",
       "         [-0.01630551,  0.02441097, -0.00510261],\n",
       "         [ 0.01020843, -0.0109714 , -0.04787107]],\n",
       " \n",
       "        [[-0.04874893,  0.01476447, -0.01374926],\n",
       "         [ 0.00954708, -0.02591343, -0.0249413 ],\n",
       "         [ 0.07653686,  0.002874  , -0.06153307]],\n",
       " \n",
       "        [[ 0.03760076, -0.06133054, -0.03394964],\n",
       "         [-0.0189716 ,  0.03775148,  0.0170623 ],\n",
       "         [ 0.04057565, -0.03630679,  0.00117352]],\n",
       " \n",
       "        [[ 0.03702629,  0.03489844,  0.04840433],\n",
       "         [-0.00271577,  0.00821414,  0.02244166],\n",
       "         [-0.06157167,  0.03842905,  0.05733553]],\n",
       " \n",
       "        [[-0.06886292,  0.02759617, -0.00095971],\n",
       "         [ 0.06872047, -0.07416829,  0.00655294],\n",
       "         [ 0.00466544,  0.01335936, -0.04058871]],\n",
       " \n",
       "        [[ 0.0188197 , -0.07063349,  0.00294551],\n",
       "         [-0.04120979, -0.03568998,  0.02164195],\n",
       "         [-0.03922442,  0.07894447,  0.00140471]],\n",
       " \n",
       "        [[ 0.04710333, -0.05734472,  0.07337245],\n",
       "         [ 0.05678356,  0.0406845 ,  0.07398282],\n",
       "         [-0.02161607, -0.01905581, -0.01130539]],\n",
       " \n",
       "        [[ 0.04870304,  0.03719025, -0.08188245],\n",
       "         [-0.05514353, -0.02934005,  0.0080039 ],\n",
       "         [ 0.03499241,  0.00503999, -0.06295902]],\n",
       " \n",
       "        [[ 0.07977974, -0.0802426 , -0.04861621],\n",
       "         [ 0.06873921,  0.00220145,  0.05958202],\n",
       "         [ 0.02965886, -0.02435746, -0.0237721 ]],\n",
       " \n",
       "        [[-0.01258501,  0.01288641,  0.01124199],\n",
       "         [-0.02520439, -0.03867089,  0.05179051],\n",
       "         [-0.01076871,  0.02439136,  0.01944058]],\n",
       " \n",
       "        [[ 0.01321043, -0.01958466, -0.08069488],\n",
       "         [ 0.08319253,  0.05701656,  0.05445793],\n",
       "         [ 0.05612484,  0.04163499, -0.04958547]],\n",
       " \n",
       "        [[-0.08120782, -0.00180999,  0.0785241 ],\n",
       "         [-0.00933911,  0.02027197, -0.0532071 ],\n",
       "         [-0.03835685,  0.08070652, -0.00834735]],\n",
       " \n",
       "        [[-0.02222493,  0.0388341 , -0.06601377],\n",
       "         [-0.02215737,  0.00621041,  0.06950208],\n",
       "         [-0.07990375,  0.04497115, -0.02388819]],\n",
       " \n",
       "        [[-0.03147236,  0.03549698,  0.05444548],\n",
       "         [ 0.06827012,  0.0795138 ,  0.02219732],\n",
       "         [-0.07921465,  0.06340907, -0.06027035]],\n",
       " \n",
       "        [[ 0.07552963, -0.08036958,  0.0557338 ],\n",
       "         [ 0.07786026, -0.01437138, -0.02248978],\n",
       "         [-0.04249676,  0.02407434,  0.0103002 ]],\n",
       " \n",
       "        [[-0.03899916, -0.05741692, -0.01405283],\n",
       "         [-0.02248868,  0.03464216, -0.07315837],\n",
       "         [ 0.00670135, -0.0641348 ,  0.06566747]],\n",
       " \n",
       "        [[-0.06269243, -0.00506934, -0.03224262],\n",
       "         [ 0.02844097,  0.05849382, -0.06424169],\n",
       "         [-0.05351656,  0.06295463,  0.00881795]],\n",
       " \n",
       "        [[-0.04050177, -0.00493669, -0.00749718],\n",
       "         [ 0.00735384,  0.08051165, -0.01393861],\n",
       "         [ 0.04485088, -0.00890974,  0.0451876 ]],\n",
       " \n",
       "        [[ 0.00017977, -0.02374614, -0.06232712],\n",
       "         [-0.02135604,  0.02334972, -0.0455061 ],\n",
       "         [ 0.07948195, -0.0116638 , -0.08190162]],\n",
       " \n",
       "        [[-0.02050763, -0.0459639 ,  0.02926349],\n",
       "         [ 0.01391655,  0.0127723 ,  0.01066653],\n",
       "         [-0.02741979,  0.06745311,  0.06429678]],\n",
       " \n",
       "        [[-0.05949276,  0.00726002, -0.01547506],\n",
       "         [ 0.02262348, -0.00476179,  0.08153277],\n",
       "         [ 0.01326221,  0.00796212, -0.04304177]]], dtype=float32), bias=0.00056324265, layer=1, neuron_number=19, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.06754725,  0.04468364, -0.03618871],\n",
       "         [ 0.0289325 , -0.01154928,  0.04177322],\n",
       "         [ 0.05954586,  0.05767085,  0.02608491]],\n",
       " \n",
       "        [[ 0.02335187,  0.07179396, -0.02368169],\n",
       "         [ 0.00858149, -0.03868163, -0.02239402],\n",
       "         [-0.0504604 ,  0.05562842, -0.05842942]],\n",
       " \n",
       "        [[-0.05958375, -0.07533339, -0.0508545 ],\n",
       "         [ 0.02105404,  0.02008789, -0.08216051],\n",
       "         [-0.04546202,  0.08183802,  0.05921186]],\n",
       " \n",
       "        [[-0.06009825,  0.01077958,  0.04507822],\n",
       "         [-0.01634846, -0.01334029, -0.07064205],\n",
       "         [-0.04031035,  0.03976003, -0.07886191]],\n",
       " \n",
       "        [[-0.08153608, -0.00269828,  0.08414644],\n",
       "         [-0.04029614,  0.02983547, -0.03906068],\n",
       "         [-0.01535517, -0.07078343,  0.04139268]],\n",
       " \n",
       "        [[ 0.00994312, -0.07321429, -0.0444954 ],\n",
       "         [-0.07715687,  0.00026621,  0.06434561],\n",
       "         [-0.06007326,  0.03431699,  0.01887122]],\n",
       " \n",
       "        [[ 0.07779141,  0.04837399, -0.03262003],\n",
       "         [ 0.02458219, -0.02213166, -0.04894411],\n",
       "         [-0.06579817, -0.05929556,  0.04201254]],\n",
       " \n",
       "        [[-0.03111563,  0.0285392 , -0.04613328],\n",
       "         [ 0.05866545,  0.05711383, -0.07719099],\n",
       "         [ 0.05270988,  0.03403721, -0.08245035]],\n",
       " \n",
       "        [[-0.00207139,  0.01309472,  0.05781081],\n",
       "         [ 0.06983066,  0.03357974,  0.04708084],\n",
       "         [-0.06245132,  0.00652467,  0.00748498]],\n",
       " \n",
       "        [[-0.00610744,  0.04386166,  0.07762576],\n",
       "         [ 0.00798844,  0.0221127 , -0.052492  ],\n",
       "         [-0.0440446 , -0.06665049, -0.03504197]],\n",
       " \n",
       "        [[-0.06526478,  0.03788767,  0.03377907],\n",
       "         [ 0.03844802, -0.01322587, -0.0298008 ],\n",
       "         [-0.07753816, -0.01380541, -0.04540582]],\n",
       " \n",
       "        [[ 0.08337526,  0.07788876,  0.01443858],\n",
       "         [ 0.04344705,  0.04591655,  0.05743957],\n",
       "         [ 0.03838064, -0.07232717,  0.00916411]],\n",
       " \n",
       "        [[-0.08015725, -0.00168713, -0.02314631],\n",
       "         [-0.01728029, -0.03135178,  0.05243929],\n",
       "         [ 0.05017104, -0.0235159 ,  0.05772169]],\n",
       " \n",
       "        [[ 0.00234795, -0.04993701,  0.04918727],\n",
       "         [ 0.06562196,  0.02867721,  0.01403953],\n",
       "         [-0.05388118, -0.08029864,  0.06084638]],\n",
       " \n",
       "        [[-0.01612826, -0.04997214,  0.04165322],\n",
       "         [-0.0445807 ,  0.07222131, -0.03157863],\n",
       "         [ 0.0590924 ,  0.00453723,  0.05571793]],\n",
       " \n",
       "        [[-0.0341343 , -0.01342162, -0.0778352 ],\n",
       "         [ 0.06095499, -0.01545927, -0.01013668],\n",
       "         [ 0.00713976, -0.00803096,  0.04022592]],\n",
       " \n",
       "        [[ 0.02930934, -0.06869937,  0.01710775],\n",
       "         [ 0.00572367,  0.0273663 ,  0.08102128],\n",
       "         [ 0.05987643,  0.04448505,  0.0352104 ]],\n",
       " \n",
       "        [[-0.06684803,  0.04931146, -0.00843566],\n",
       "         [-0.01053516,  0.06526433, -0.07078449],\n",
       "         [ 0.08284138, -0.02433797,  0.00285461]],\n",
       " \n",
       "        [[ 0.00500047, -0.06501584,  0.07965522],\n",
       "         [ 0.07571509,  0.03825566,  0.01901493],\n",
       "         [ 0.02881081,  0.02559062,  0.02906052]],\n",
       " \n",
       "        [[ 0.04749443,  0.03528402,  0.03361836],\n",
       "         [ 0.00524567, -0.03242416, -0.02950563],\n",
       "         [-0.01748982, -0.00619684,  0.04086728]],\n",
       " \n",
       "        [[ 0.05058205, -0.04871168,  0.06699473],\n",
       "         [ 0.01065179,  0.0397596 , -0.0374575 ],\n",
       "         [-0.03031057, -0.05089771, -0.03737665]],\n",
       " \n",
       "        [[ 0.01177896,  0.05172402, -0.03900142],\n",
       "         [-0.03302596, -0.04142367, -0.05161784],\n",
       "         [-0.01624789,  0.01588424, -0.07687568]],\n",
       " \n",
       "        [[ 0.02096418,  0.04847967, -0.01147742],\n",
       "         [-0.07916202, -0.05413664,  0.05167744],\n",
       "         [ 0.07118163,  0.06130373, -0.00963183]],\n",
       " \n",
       "        [[ 0.08342375,  0.01946053,  0.04081193],\n",
       "         [ 0.05666505, -0.03842654, -0.03881728],\n",
       "         [ 0.01838064, -0.05303791, -0.01559496]],\n",
       " \n",
       "        [[-0.011005  ,  0.02170207, -0.02023903],\n",
       "         [ 0.0205868 , -0.06200569,  0.0647079 ],\n",
       "         [-0.05172364,  0.06384798, -0.06666305]],\n",
       " \n",
       "        [[ 0.0744253 ,  0.0800088 ,  0.02312606],\n",
       "         [-0.06475642, -0.00562781, -0.02705997],\n",
       "         [-0.02685083,  0.00929346, -0.02419817]],\n",
       " \n",
       "        [[ 0.02625359,  0.03810137,  0.00520405],\n",
       "         [-0.00807748, -0.02060599,  0.08280686],\n",
       "         [ 0.02071637,  0.02794235,  0.00610073]],\n",
       " \n",
       "        [[ 0.03025733,  0.00591194, -0.0115126 ],\n",
       "         [-0.05603557,  0.01659177,  0.06774931],\n",
       "         [-0.00251079, -0.07100847,  0.00220364]],\n",
       " \n",
       "        [[ 0.02439053,  0.08027494,  0.02959474],\n",
       "         [ 0.05745013,  0.02145005, -0.07467465],\n",
       "         [-0.05779532,  0.0627676 , -0.01009721]],\n",
       " \n",
       "        [[ 0.05843239, -0.02752105, -0.02904316],\n",
       "         [ 0.01185521,  0.00943852, -0.00899912],\n",
       "         [ 0.01509637,  0.03038771,  0.05526477]],\n",
       " \n",
       "        [[-0.0252672 , -0.00944958,  0.05492594],\n",
       "         [ 0.0240757 ,  0.07177556, -0.06725672],\n",
       "         [-0.01349689,  0.02812021, -0.07445971]],\n",
       " \n",
       "        [[-0.04163254, -0.00772617,  0.08056436],\n",
       "         [-0.07663949, -0.05068332,  0.00594657],\n",
       "         [ 0.0194233 ,  0.00433243,  0.03775034]]], dtype=float32), bias=-0.00054488017, layer=1, neuron_number=20, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  8.02859589e-02,   3.18803377e-02,  -7.13240579e-02],\n",
       "         [  4.06623892e-02,  -8.04293081e-02,  -3.33331563e-02],\n",
       "         [ -4.52566817e-02,  -3.41469422e-02,  -7.60733411e-02]],\n",
       " \n",
       "        [[  3.49622183e-02,   8.20277631e-02,   9.93400067e-03],\n",
       "         [  6.35496601e-02,  -7.49609917e-02,   2.33039130e-02],\n",
       "         [ -2.19581798e-02,   3.97922397e-02,  -7.27120936e-02]],\n",
       " \n",
       "        [[  7.58427009e-02,  -2.20851656e-02,   5.25223464e-02],\n",
       "         [ -2.93857343e-02,  -4.15711403e-02,   5.99312317e-03],\n",
       "         [  7.50990910e-03,  -8.07550251e-02,  -4.91034053e-02]],\n",
       " \n",
       "        [[ -6.76412284e-02,   1.20957419e-02,   2.04175916e-02],\n",
       "         [  5.45859300e-02,  -3.32962833e-02,  -3.69588903e-04],\n",
       "         [  6.27472326e-02,  -8.93846620e-03,   2.55026296e-02]],\n",
       " \n",
       "        [[  1.47279538e-02,   4.92245480e-02,   1.47695269e-03],\n",
       "         [  4.49902192e-02,   9.95534565e-03,   6.50324896e-02],\n",
       "         [  4.05792408e-02,  -3.36147510e-02,  -1.55602107e-02]],\n",
       " \n",
       "        [[  5.90964258e-02,   7.75949135e-02,  -5.24040200e-02],\n",
       "         [  3.02715618e-02,   7.56652802e-02,   3.12842280e-02],\n",
       "         [  5.76155484e-02,  -1.45578887e-02,  -8.18611383e-02]],\n",
       " \n",
       "        [[  4.70873620e-03,  -1.88514162e-02,  -3.62733752e-02],\n",
       "         [ -3.49671803e-02,  -5.27321696e-02,   4.42604423e-02],\n",
       "         [  4.98979688e-02,   7.37840980e-02,   2.14986671e-02]],\n",
       " \n",
       "        [[  5.21940701e-02,  -3.49199250e-02,   1.38169173e-02],\n",
       "         [ -9.28876270e-03,   7.87154138e-02,   3.77627946e-02],\n",
       "         [  8.03628098e-03,  -6.90455958e-02,  -6.67880662e-03]],\n",
       " \n",
       "        [[  7.05638081e-02,  -8.34792480e-02,  -2.48849615e-02],\n",
       "         [ -7.21784234e-02,   7.88993910e-02,  -2.19026934e-02],\n",
       "         [  5.60359247e-02,   1.74683030e-03,  -9.75626148e-03]],\n",
       " \n",
       "        [[ -3.02036535e-02,  -1.10821174e-02,  -8.26378819e-03],\n",
       "         [  1.37417819e-02,   7.49730095e-02,   1.10695250e-02],\n",
       "         [ -3.87185998e-02,  -6.60629719e-02,  -4.41391431e-02]],\n",
       " \n",
       "        [[  5.43178581e-02,  -7.59695470e-02,   2.37150081e-02],\n",
       "         [ -4.53778468e-02,   4.32882980e-02,   5.29437326e-03],\n",
       "         [ -6.05238937e-02,   6.93273842e-02,  -6.85814172e-02]],\n",
       " \n",
       "        [[  4.51248512e-02,   5.24417832e-02,   5.97461388e-02],\n",
       "         [ -3.97415869e-02,  -3.12944837e-02,  -1.77516567e-03],\n",
       "         [ -6.83720410e-02,  -7.57304579e-02,  -3.97223085e-02]],\n",
       " \n",
       "        [[  6.39175847e-02,   3.41929831e-02,   3.51630487e-02],\n",
       "         [  4.58471328e-02,  -8.18317905e-02,   2.15389691e-02],\n",
       "         [  5.17377630e-04,   4.76584844e-02,  -1.29054589e-02]],\n",
       " \n",
       "        [[ -7.99678490e-02,   5.07233255e-02,  -3.61740701e-02],\n",
       "         [  5.57857864e-02,   6.26296848e-02,  -3.72262262e-02],\n",
       "         [  3.42310891e-02,   7.13189542e-02,   1.26648750e-02]],\n",
       " \n",
       "        [[ -7.70975351e-02,   6.73284084e-02,  -4.24086526e-02],\n",
       "         [  3.62037718e-02,   1.50329378e-02,   1.17729111e-02],\n",
       "         [ -2.97753699e-02,   6.34681061e-02,  -4.27350476e-02]],\n",
       " \n",
       "        [[ -8.77918303e-03,   5.90416268e-02,   7.31092542e-02],\n",
       "         [  2.73446040e-03,  -6.56345412e-02,   2.55104564e-02],\n",
       "         [  9.89748165e-03,   1.79525372e-02,  -1.05574084e-02]],\n",
       " \n",
       "        [[ -7.40117580e-02,   3.55822779e-02,  -5.18759973e-02],\n",
       "         [ -5.69651537e-02,   4.35151830e-02,  -7.40891844e-02],\n",
       "         [ -5.86453862e-02,   2.50974912e-02,  -2.05392241e-02]],\n",
       " \n",
       "        [[  5.93669340e-02,  -3.77586414e-03,   5.49332239e-03],\n",
       "         [  3.15016210e-02,   3.14517617e-02,  -1.89295579e-02],\n",
       "         [ -3.36875096e-02,   2.42643617e-02,  -4.60763555e-03]],\n",
       " \n",
       "        [[  4.57167737e-02,  -6.88207075e-02,   5.80355823e-02],\n",
       "         [ -2.76521873e-02,  -4.49648052e-02,  -2.54946090e-02],\n",
       "         [  6.20406121e-02,  -8.34287554e-02,   5.72979786e-02]],\n",
       " \n",
       "        [[ -4.42169830e-02,  -7.73836151e-02,  -2.36030743e-02],\n",
       "         [  6.29923306e-04,  -4.28809747e-02,  -5.50184995e-02],\n",
       "         [ -6.15973026e-02,  -2.52013393e-02,   4.88632098e-02]],\n",
       " \n",
       "        [[ -7.34535754e-02,  -5.59432656e-02,   6.38886392e-02],\n",
       "         [  5.18774055e-02,  -3.45599577e-02,  -5.43986969e-02],\n",
       "         [ -1.87510997e-02,  -6.87829405e-02,   3.09522613e-03]],\n",
       " \n",
       "        [[  1.28585557e-02,  -6.64253533e-02,  -2.10304242e-02],\n",
       "         [ -7.31629133e-02,  -1.09103546e-02,   8.15294534e-02],\n",
       "         [ -5.88956736e-02,  -5.34516796e-02,  -7.48615041e-02]],\n",
       " \n",
       "        [[  5.92458900e-03,  -1.66255832e-02,  -2.73743682e-02],\n",
       "         [ -3.98750640e-02,  -2.32090633e-02,   5.10943048e-02],\n",
       "         [ -4.21529450e-02,   3.15027224e-04,  -1.71509441e-02]],\n",
       " \n",
       "        [[  1.81619748e-02,  -1.16007337e-02,   8.10479075e-02],\n",
       "         [ -7.93067738e-02,  -1.35886408e-02,  -4.68323864e-02],\n",
       "         [ -6.04934879e-02,  -3.86999920e-02,   4.03021276e-02]],\n",
       " \n",
       "        [[  6.85807765e-02,   5.30182198e-02,  -7.52333850e-02],\n",
       "         [  5.65490387e-02,   7.67433643e-02,   5.87821342e-02],\n",
       "         [  5.79213649e-02,   7.37879500e-02,  -1.61265284e-02]],\n",
       " \n",
       "        [[ -6.64715767e-02,  -7.73997530e-02,   6.57320395e-02],\n",
       "         [  8.09407085e-02,  -5.98687620e-06,  -4.83077392e-02],\n",
       "         [ -2.34287139e-02,  -4.39869575e-02,   6.65904805e-02]],\n",
       " \n",
       "        [[  6.75449818e-02,  -2.39787865e-02,  -8.67905375e-03],\n",
       "         [  4.93592024e-02,   6.07421622e-02,  -5.09093292e-02],\n",
       "         [  6.27637580e-02,   4.44246531e-02,   1.34020206e-02]],\n",
       " \n",
       "        [[  2.14164555e-02,   2.24368498e-02,   3.80227342e-02],\n",
       "         [  3.93249169e-02,   7.11216629e-02,   7.61506259e-02],\n",
       "         [ -5.03482744e-02,  -6.74369261e-02,  -5.65831997e-02]],\n",
       " \n",
       "        [[  7.59769008e-02,   2.84133051e-02,  -4.94002663e-02],\n",
       "         [  6.48749918e-02,  -6.19497225e-02,  -7.59688392e-02],\n",
       "         [ -6.72216117e-02,   3.01737431e-02,   7.69554004e-02]],\n",
       " \n",
       "        [[  2.24436726e-02,   2.93740146e-02,   3.54651175e-03],\n",
       "         [ -1.85869876e-02,  -5.09828962e-02,  -2.29917690e-02],\n",
       "         [ -7.60402903e-02,  -4.32180380e-03,   2.28517018e-02]],\n",
       " \n",
       "        [[ -3.65007557e-02,   4.59639356e-02,  -4.58156737e-03],\n",
       "         [  3.19427364e-02,   5.09472676e-02,  -1.54872388e-02],\n",
       "         [  3.49923712e-03,   8.03414732e-02,   6.83533922e-02]],\n",
       " \n",
       "        [[ -6.02706745e-02,   8.35659076e-03,  -7.04913167e-04],\n",
       "         [  2.10988149e-02,   1.02823637e-02,   2.44035609e-02],\n",
       "         [ -7.03341514e-02,  -8.11616611e-03,  -5.05635180e-02]]], dtype=float32), bias=0.0011001856, layer=1, neuron_number=21, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.03965814, -0.07831468,  0.01309698],\n",
       "         [-0.01641905, -0.06505543,  0.07092221],\n",
       "         [-0.07208084, -0.02402942, -0.05293819]],\n",
       " \n",
       "        [[ 0.05441083, -0.0571739 , -0.00746379],\n",
       "         [ 0.01378042, -0.06840947,  0.06931657],\n",
       "         [-0.06991214,  0.04086426, -0.03034326]],\n",
       " \n",
       "        [[-0.06361859,  0.01407223, -0.02869305],\n",
       "         [-0.04030446, -0.05887388,  0.06242929],\n",
       "         [-0.03815441, -0.00877962, -0.0597067 ]],\n",
       " \n",
       "        [[ 0.06386664,  0.0576561 ,  0.03702507],\n",
       "         [-0.00613662,  0.03991795, -0.0169861 ],\n",
       "         [-0.03404942, -0.07377345,  0.0712609 ]],\n",
       " \n",
       "        [[ 0.07152989,  0.04428278,  0.02270282],\n",
       "         [-0.03325438, -0.00079815,  0.03354954],\n",
       "         [ 0.04886612, -0.00081945, -0.07983692]],\n",
       " \n",
       "        [[ 0.0641571 ,  0.07356033, -0.0255605 ],\n",
       "         [ 0.0112427 ,  0.00153965,  0.05895215],\n",
       "         [-0.01256307, -0.07652467,  0.00924419]],\n",
       " \n",
       "        [[ 0.06035464, -0.04266768, -0.02238182],\n",
       "         [-0.04575283,  0.06676968,  0.03510373],\n",
       "         [-0.0103315 ,  0.07340628, -0.00339528]],\n",
       " \n",
       "        [[-0.04529006, -0.02341942,  0.01121263],\n",
       "         [ 0.0420747 ,  0.05083439,  0.0650894 ],\n",
       "         [-0.07616075,  0.05110754,  0.01361022]],\n",
       " \n",
       "        [[ 0.05118038, -0.04730137,  0.01344625],\n",
       "         [-0.01978753, -0.07316449, -0.06850974],\n",
       "         [ 0.04078684, -0.03463247,  0.0653156 ]],\n",
       " \n",
       "        [[-0.08173613, -0.0588068 ,  0.03553476],\n",
       "         [-0.0535796 ,  0.07496873,  0.03034915],\n",
       "         [ 0.00493767, -0.05292558,  0.01241524]],\n",
       " \n",
       "        [[ 0.04496941, -0.08313833, -0.05500488],\n",
       "         [-0.00890284, -0.02509494,  0.0103103 ],\n",
       "         [ 0.01641286,  0.03388509, -0.05244045]],\n",
       " \n",
       "        [[ 0.05152702, -0.04457548,  0.05504338],\n",
       "         [ 0.07802181,  0.02477213,  0.00880997],\n",
       "         [ 0.05997361, -0.02437962, -0.04658545]],\n",
       " \n",
       "        [[ 0.02051597, -0.07548316,  0.01733793],\n",
       "         [-0.05526761,  0.0077062 , -0.0230087 ],\n",
       "         [-0.05098233,  0.03656354,  0.06719165]],\n",
       " \n",
       "        [[-0.07693417, -0.03526803,  0.02837768],\n",
       "         [-0.04558792, -0.07728948, -0.00600879],\n",
       "         [-0.0627292 ,  0.08112226,  0.03100171]],\n",
       " \n",
       "        [[-0.0594648 ,  0.06464072, -0.01672791],\n",
       "         [ 0.01172052,  0.06846773,  0.03822831],\n",
       "         [-0.06011322, -0.06111831,  0.07769538]],\n",
       " \n",
       "        [[-0.08088899, -0.06255087,  0.06333376],\n",
       "         [ 0.0672222 , -0.06215787, -0.07882326],\n",
       "         [ 0.08121248, -0.04119385, -0.05766891]],\n",
       " \n",
       "        [[ 0.03504901, -0.00012969,  0.03554514],\n",
       "         [-0.04788885, -0.00058257, -0.0067401 ],\n",
       "         [ 0.05983484, -0.02645966, -0.0462658 ]],\n",
       " \n",
       "        [[-0.01552432,  0.06399402,  0.0481605 ],\n",
       "         [-0.04771382, -0.00205027, -0.01466799],\n",
       "         [-0.06559734,  0.06129688,  0.05204943]],\n",
       " \n",
       "        [[-0.04490768,  0.0371635 , -0.07098646],\n",
       "         [ 0.04940644,  0.01465771, -0.07048307],\n",
       "         [ 0.01157868, -0.06749982, -0.0127839 ]],\n",
       " \n",
       "        [[ 0.07141806,  0.07648221,  0.05432403],\n",
       "         [ 0.03147538,  0.02053924, -0.01967124],\n",
       "         [-0.02814054, -0.07350834,  0.06933096]],\n",
       " \n",
       "        [[ 0.03833763,  0.02689413, -0.02072323],\n",
       "         [ 0.02896367, -0.02857741, -0.01235969],\n",
       "         [ 0.07557833,  0.014858  , -0.02778321]],\n",
       " \n",
       "        [[ 0.07749302,  0.07813007,  0.02663192],\n",
       "         [ 0.06881536, -0.07765239, -0.05002296],\n",
       "         [-0.0451071 , -0.0287394 , -0.01484295]],\n",
       " \n",
       "        [[-0.06013777,  0.07990991, -0.05625369],\n",
       "         [ 0.0337823 , -0.08062372,  0.0227729 ],\n",
       "         [-0.07173941,  0.06825316,  0.06938314]],\n",
       " \n",
       "        [[ 0.00132572,  0.01955474, -0.05013538],\n",
       "         [-0.00974634, -0.05161794,  0.06287873],\n",
       "         [-0.06352045,  0.00834313, -0.00378039]],\n",
       " \n",
       "        [[-0.00199992, -0.04847541,  0.03416768],\n",
       "         [-0.00377435,  0.05002127, -0.04343599],\n",
       "         [-0.01863003, -0.02832934, -0.02789672]],\n",
       " \n",
       "        [[-0.05524005, -0.03927441,  0.03883813],\n",
       "         [ 0.07427263,  0.00998784, -0.03578607],\n",
       "         [ 0.04955491,  0.06905756, -0.00465488]],\n",
       " \n",
       "        [[-0.03763688,  0.00496716,  0.04733292],\n",
       "         [-0.0257807 , -0.06518887, -0.05865181],\n",
       "         [-0.05745608, -0.00392172,  0.07003305]],\n",
       " \n",
       "        [[ 0.06371829, -0.0720372 , -0.00019123],\n",
       "         [ 0.06802826,  0.0309766 ,  0.06099271],\n",
       "         [-0.03303236, -0.05645978, -0.0053712 ]],\n",
       " \n",
       "        [[ 0.05866579, -0.03513132, -0.00701706],\n",
       "         [ 0.08333568, -0.07348891, -0.05224014],\n",
       "         [ 0.03491196, -0.03780492, -0.06271908]],\n",
       " \n",
       "        [[-0.00660386,  0.01491338, -0.05158247],\n",
       "         [-0.02985653,  0.0811927 ,  0.00477018],\n",
       "         [ 0.03305602, -0.01738431, -0.07620603]],\n",
       " \n",
       "        [[-0.06605242,  0.01168483, -0.01340464],\n",
       "         [ 0.05557738, -0.06462967, -0.01847029],\n",
       "         [-0.00157967,  0.01243166, -0.06991461]],\n",
       " \n",
       "        [[-0.06691241, -0.04966428, -0.07025433],\n",
       "         [ 0.06177755,  0.05352166, -0.00554665],\n",
       "         [ 0.05102872, -0.00524474,  0.00322395]]], dtype=float32), bias=-0.00055185077, layer=1, neuron_number=22, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.0275274 , -0.03725217, -0.04055297],\n",
       "         [-0.07480437, -0.02827458,  0.05129091],\n",
       "         [-0.03105186, -0.04805182,  0.02635835]],\n",
       " \n",
       "        [[-0.06904273, -0.05233041,  0.00064489],\n",
       "         [ 0.05051546,  0.03016189,  0.07566461],\n",
       "         [-0.01261571,  0.05894292, -0.06695639]],\n",
       " \n",
       "        [[ 0.07685809,  0.022186  , -0.05747195],\n",
       "         [ 0.0282256 , -0.04403301,  0.0270437 ],\n",
       "         [ 0.05916303,  0.01070146,  0.05949355]],\n",
       " \n",
       "        [[ 0.05168847, -0.03051734, -0.06380822],\n",
       "         [-0.07144658, -0.02434129,  0.06249762],\n",
       "         [-0.05117781, -0.03190203,  0.025574  ]],\n",
       " \n",
       "        [[ 0.05073242, -0.04746443,  0.0465368 ],\n",
       "         [-0.00737901, -0.07302285, -0.02534176],\n",
       "         [ 0.01902248,  0.02710602, -0.05094759]],\n",
       " \n",
       "        [[ 0.00762245,  0.02115949, -0.075215  ],\n",
       "         [-0.02553918,  0.04593391,  0.02724239],\n",
       "         [-0.01566826,  0.0081814 ,  0.06038176]],\n",
       " \n",
       "        [[-0.01675496,  0.05175043,  0.03936625],\n",
       "         [-0.04350371,  0.07822191, -0.06550217],\n",
       "         [ 0.01706067, -0.02432489, -0.00663519]],\n",
       " \n",
       "        [[ 0.05556514,  0.05664366, -0.0803588 ],\n",
       "         [-0.01717699,  0.03554666,  0.00238935],\n",
       "         [ 0.02679007, -0.03735096,  0.01288163]],\n",
       " \n",
       "        [[-0.02153567, -0.07352657,  0.05528328],\n",
       "         [ 0.01951307, -0.02671672,  0.04783625],\n",
       "         [ 0.04747716,  0.05466306, -0.05169734]],\n",
       " \n",
       "        [[ 0.04752154, -0.0736841 , -0.01251297],\n",
       "         [-0.08177393, -0.06768274, -0.02815708],\n",
       "         [-0.05148982, -0.00963102, -0.05178944]],\n",
       " \n",
       "        [[ 0.07747358,  0.05425332,  0.01306001],\n",
       "         [-0.08230174, -0.03087026, -0.02991433],\n",
       "         [-0.00614213, -0.02053197, -0.02906674]],\n",
       " \n",
       "        [[ 0.04488551, -0.01041843,  0.02646566],\n",
       "         [ 0.00332564, -0.0320609 ,  0.07319634],\n",
       "         [-0.02757327,  0.03901298,  0.06713009]],\n",
       " \n",
       "        [[ 0.08210734,  0.0224194 , -0.05624195],\n",
       "         [ 0.00919105,  0.00854026,  0.03403256],\n",
       "         [-0.06299169,  0.02966009, -0.00635146]],\n",
       " \n",
       "        [[-0.05790963,  0.05011834, -0.08200763],\n",
       "         [ 0.03181574, -0.00920841,  0.0158749 ],\n",
       "         [ 0.05270676, -0.05712476,  0.0748634 ]],\n",
       " \n",
       "        [[ 0.02522498,  0.0622422 ,  0.03057554],\n",
       "         [-0.06225881,  0.02416922, -0.00167823],\n",
       "         [-0.01028992, -0.04592006, -0.03202121]],\n",
       " \n",
       "        [[ 0.01403409,  0.0059841 , -0.07139824],\n",
       "         [-0.019241  , -0.03081442,  0.01128257],\n",
       "         [ 0.00614211,  0.02771036, -0.02452767]],\n",
       " \n",
       "        [[-0.07231028,  0.0616809 , -0.01632251],\n",
       "         [ 0.04280687, -0.03363339,  0.06795499],\n",
       "         [ 0.05734459,  0.0636607 , -0.07747938]],\n",
       " \n",
       "        [[ 0.07507885,  0.02772582,  0.04093682],\n",
       "         [-0.02420791, -0.07766241,  0.02836524],\n",
       "         [-0.03776921,  0.08148965, -0.06830217]],\n",
       " \n",
       "        [[ 0.05493788,  0.06182255, -0.06600378],\n",
       "         [ 0.02396574, -0.0675336 , -0.01420063],\n",
       "         [ 0.01527891, -0.03687145, -0.05154359]],\n",
       " \n",
       "        [[ 0.08214764,  0.03477206, -0.03058275],\n",
       "         [-0.0160147 ,  0.07349803,  0.0749703 ],\n",
       "         [ 0.00909225,  0.08168005,  0.01916559]],\n",
       " \n",
       "        [[ 0.02149674,  0.05329468,  0.06120514],\n",
       "         [-0.02827162, -0.04434797, -0.03849409],\n",
       "         [-0.05947873, -0.03905724,  0.01665505]],\n",
       " \n",
       "        [[-0.06715645, -0.03113861, -0.04807431],\n",
       "         [ 0.01130723,  0.04556983,  0.04241125],\n",
       "         [ 0.06192083, -0.03856879, -0.03343808]],\n",
       " \n",
       "        [[ 0.05954297, -0.02551831,  0.00893451],\n",
       "         [ 0.0043921 ,  0.05838652,  0.04802558],\n",
       "         [-0.05791153,  0.07356217, -0.05710147]],\n",
       " \n",
       "        [[ 0.04005968,  0.05154727,  0.0034783 ],\n",
       "         [ 0.0163572 ,  0.07930095, -0.0177428 ],\n",
       "         [ 0.01648476,  0.04928738,  0.02118459]],\n",
       " \n",
       "        [[-0.01222614,  0.06074033, -0.01578338],\n",
       "         [ 0.0684544 , -0.05270503, -0.05039103],\n",
       "         [ 0.04431204,  0.01383956, -0.00430704]],\n",
       " \n",
       "        [[ 0.01493232, -0.02118642, -0.00749498],\n",
       "         [ 0.00538182, -0.01558372,  0.0736125 ],\n",
       "         [-0.03734902,  0.0825889 ,  0.01802039]],\n",
       " \n",
       "        [[-0.02390738, -0.00010198, -0.01903162],\n",
       "         [-0.01179931,  0.00572175,  0.07650446],\n",
       "         [ 0.0458963 ,  0.03442307, -0.02287992]],\n",
       " \n",
       "        [[ 0.05804873,  0.06308781, -0.04969831],\n",
       "         [ 0.06849976, -0.04357822, -0.04249989],\n",
       "         [ 0.00020667,  0.04740604,  0.06755693]],\n",
       " \n",
       "        [[-0.03222233, -0.02357472,  0.02172297],\n",
       "         [-0.05172544,  0.06706556, -0.03431629],\n",
       "         [-0.03678805, -0.05425816, -0.03923984]],\n",
       " \n",
       "        [[ 0.01099666, -0.03611595, -0.00925181],\n",
       "         [-0.0314952 ,  0.03838973,  0.03706877],\n",
       "         [-0.02719433,  0.07956281, -0.05751596]],\n",
       " \n",
       "        [[ 0.06312341,  0.00209456, -0.01331285],\n",
       "         [ 0.06901991, -0.03008138, -0.05476993],\n",
       "         [-0.03578811,  0.04264798, -0.06938922]],\n",
       " \n",
       "        [[-0.01894968,  0.01240792, -0.03136854],\n",
       "         [-0.01430213,  0.06261884, -0.02505757],\n",
       "         [ 0.01423468,  0.07678604, -0.005326  ]]], dtype=float32), bias=-0.00039880647, layer=1, neuron_number=23, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  7.02523813e-02,   8.14379379e-02,   4.16886359e-02],\n",
       "         [  9.59959719e-03,  -5.35961241e-03,   5.03422953e-02],\n",
       "         [  3.41709331e-02,  -2.37263413e-03,   6.41293675e-02]],\n",
       " \n",
       "        [[ -9.84459277e-03,   3.90621573e-02,   6.80659488e-02],\n",
       "         [ -6.06526285e-02,  -6.34848848e-02,  -3.75268869e-02],\n",
       "         [  6.00441964e-03,  -6.64977208e-02,  -3.64949629e-02]],\n",
       " \n",
       "        [[ -4.36779633e-02,   6.14978522e-02,   4.71632602e-03],\n",
       "         [  7.68425018e-02,  -1.54674361e-02,   6.00471497e-02],\n",
       "         [  7.53143951e-02,  -7.21126199e-02,   6.50866772e-04]],\n",
       " \n",
       "        [[ -6.40327437e-03,  -6.81054816e-02,   2.98911519e-02],\n",
       "         [  8.83136690e-03,   2.57771462e-02,  -4.09517661e-02],\n",
       "         [ -3.79242338e-02,   8.38833079e-02,   5.06837713e-03]],\n",
       " \n",
       "        [[  3.16073336e-02,   3.63105908e-02,  -5.02701588e-02],\n",
       "         [ -3.54541168e-02,  -5.65409325e-02,  -4.56850830e-06],\n",
       "         [ -6.27264520e-03,  -7.96686038e-02,   6.91992864e-02]],\n",
       " \n",
       "        [[ -1.29330643e-02,   4.90301549e-02,   3.30455266e-02],\n",
       "         [  3.35626639e-02,   3.85428779e-02,   7.11059347e-02],\n",
       "         [ -2.78454944e-02,  -1.18827652e-02,  -4.44260314e-02]],\n",
       " \n",
       "        [[ -4.79658954e-02,   4.35594693e-02,   7.06079006e-02],\n",
       "         [  7.79544488e-02,  -7.45190233e-02,   1.85017679e-02],\n",
       "         [ -7.17138425e-02,   1.50563773e-02,  -2.16094777e-03]],\n",
       " \n",
       "        [[  2.47849021e-02,   2.84628989e-03,   1.37855504e-02],\n",
       "         [ -4.73438355e-04,   3.68370749e-02,   3.24399807e-02],\n",
       "         [ -4.54388224e-02,  -1.76872723e-02,   4.09260765e-03]],\n",
       " \n",
       "        [[ -4.07414921e-02,  -5.42023368e-02,  -6.79807663e-02],\n",
       "         [  7.11681768e-02,  -7.85557404e-02,   2.27328464e-02],\n",
       "         [ -8.03766698e-02,  -2.82959044e-02,   8.01562518e-02]],\n",
       " \n",
       "        [[ -3.45646637e-04,   2.63314974e-02,  -2.09162105e-02],\n",
       "         [ -5.58804628e-03,   5.46599515e-02,  -4.54769190e-03],\n",
       "         [ -4.17821594e-02,  -6.70172945e-02,   3.24991569e-02]],\n",
       " \n",
       "        [[ -6.57690018e-02,  -3.29492614e-02,   8.16517994e-02],\n",
       "         [  4.57396880e-02,  -7.61435507e-03,  -4.70534004e-02],\n",
       "         [  8.18008743e-03,   2.26623043e-02,   3.52259651e-02]],\n",
       " \n",
       "        [[ -4.62351628e-02,  -4.63003553e-02,  -2.56272461e-02],\n",
       "         [ -8.18988830e-02,  -5.29130921e-02,  -7.72895291e-02],\n",
       "         [ -2.83130631e-02,   4.61909920e-02,  -5.45060001e-02]],\n",
       " \n",
       "        [[  5.40233850e-02,   3.11579276e-02,   3.07450853e-02],\n",
       "         [ -2.09595240e-03,  -8.12364519e-02,   4.95620668e-02],\n",
       "         [  2.90737860e-02,   4.28377613e-02,   2.89069954e-02]],\n",
       " \n",
       "        [[ -6.63086325e-02,  -4.23900038e-02,  -3.45569141e-02],\n",
       "         [ -3.89798135e-02,  -6.94199875e-02,   4.20751423e-02],\n",
       "         [  8.26004073e-02,  -5.79937845e-02,   7.24355131e-02]],\n",
       " \n",
       "        [[ -5.46471812e-02,  -6.70103356e-02,  -6.32262230e-02],\n",
       "         [  3.47964987e-02,  -4.65027504e-02,  -2.13136654e-02],\n",
       "         [  1.35181844e-02,  -6.77778125e-02,   2.07658275e-03]],\n",
       " \n",
       "        [[  4.95353602e-02,   7.63794705e-02,   4.74976413e-02],\n",
       "         [  6.66845739e-02,  -2.87108459e-02,  -2.63358038e-02],\n",
       "         [  1.50476936e-02,  -5.57698309e-02,  -1.00681931e-02]],\n",
       " \n",
       "        [[ -2.81656943e-02,   5.02567105e-02,  -2.37907246e-02],\n",
       "         [  4.00146805e-02,  -4.69141118e-02,   1.69270206e-02],\n",
       "         [  7.79786110e-02,  -3.62539943e-03,   2.57023498e-02]],\n",
       " \n",
       "        [[ -3.79259959e-02,   5.45282774e-02,   7.49552175e-02],\n",
       "         [  4.46981229e-02,   4.75459211e-02,  -3.03272493e-02],\n",
       "         [ -7.13675395e-02,   7.06448480e-02,   6.76459074e-02]],\n",
       " \n",
       "        [[ -2.66106613e-02,  -3.93110812e-02,   5.86574934e-02],\n",
       "         [ -5.41765541e-02,   5.57850208e-03,   4.69087474e-02],\n",
       "         [ -4.04385943e-03,  -1.20141730e-03,  -6.04279861e-02]],\n",
       " \n",
       "        [[  3.10469624e-02,  -5.24720810e-02,  -5.02180755e-02],\n",
       "         [ -2.82470156e-02,   2.02757660e-02,   9.05367732e-03],\n",
       "         [  4.33752239e-02,  -3.48731019e-02,  -1.54028053e-03]],\n",
       " \n",
       "        [[  1.62848383e-02,   3.83131765e-02,   3.45907398e-02],\n",
       "         [  3.20580378e-02,  -2.37196293e-02,   3.54522578e-02],\n",
       "         [ -6.91385008e-04,  -4.04435247e-02,   6.22105598e-02]],\n",
       " \n",
       "        [[ -1.33990459e-02,   3.30201611e-02,   5.77409305e-02],\n",
       "         [ -7.41448253e-02,   9.77812568e-04,  -4.78255600e-02],\n",
       "         [ -1.90002192e-02,   5.12402393e-02,   2.76547428e-02]],\n",
       " \n",
       "        [[  6.53777122e-02,   5.40243536e-02,  -5.75110205e-02],\n",
       "         [ -3.75532210e-02,   2.35504396e-02,   5.45089208e-02],\n",
       "         [  6.89850077e-02,  -3.78921814e-02,  -1.02491435e-02]],\n",
       " \n",
       "        [[ -9.09458101e-03,  -5.64709418e-02,   2.64799967e-02],\n",
       "         [  8.00675899e-02,  -7.15886950e-02,  -9.81799047e-03],\n",
       "         [ -6.87131435e-02,   1.14702663e-04,  -6.88626245e-02]],\n",
       " \n",
       "        [[ -7.87557140e-02,  -6.67668581e-02,   5.79552837e-02],\n",
       "         [  5.59901120e-03,  -6.29435182e-02,   3.92990969e-02],\n",
       "         [ -7.73565993e-02,   2.63423678e-02,   2.02744436e-02]],\n",
       " \n",
       "        [[ -3.23587880e-02,  -7.95230344e-02,   7.57452697e-02],\n",
       "         [  1.09304553e-02,   4.10178415e-02,   4.73506674e-02],\n",
       "         [ -6.73442483e-02,   5.04072406e-04,  -5.53835891e-02]],\n",
       " \n",
       "        [[ -3.51068005e-02,   5.30682784e-03,   3.77242044e-02],\n",
       "         [ -4.02950570e-02,  -4.93777208e-02,   7.67670106e-04],\n",
       "         [ -3.94051149e-02,   6.25083894e-02,   2.77672987e-02]],\n",
       " \n",
       "        [[  7.20421923e-03,  -3.37710157e-02,   5.75168282e-02],\n",
       "         [  3.64406817e-02,   6.54665008e-02,   7.88316056e-02],\n",
       "         [ -6.51526228e-02,  -3.06541156e-02,   7.79350325e-02]],\n",
       " \n",
       "        [[ -8.24684948e-02,  -1.76316444e-02,  -2.18737330e-02],\n",
       "         [ -6.51450753e-02,   3.37116197e-02,   4.95422333e-02],\n",
       "         [  4.09771688e-02,   8.34360719e-02,   2.74086501e-02]],\n",
       " \n",
       "        [[  1.49564464e-02,   3.97504400e-03,  -6.00723214e-02],\n",
       "         [  1.50978751e-02,   5.19330874e-02,   1.41536146e-02],\n",
       "         [ -7.82717094e-02,  -1.46557922e-02,  -7.07113445e-02]],\n",
       " \n",
       "        [[  4.36674207e-02,  -4.69788574e-02,  -5.27302176e-02],\n",
       "         [ -5.21597788e-02,   1.33492351e-02,  -2.17638426e-02],\n",
       "         [ -1.61277857e-02,   8.31509754e-02,  -1.62982699e-02]],\n",
       " \n",
       "        [[  4.03535068e-02,   7.36137778e-02,   6.84090555e-02],\n",
       "         [  4.28797025e-03,  -9.74280294e-04,  -7.25376457e-02],\n",
       "         [ -8.34207758e-02,   4.75259535e-02,  -1.39679816e-02]]], dtype=float32), bias=0.00099533459, layer=1, neuron_number=24, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.00557703, -0.02078636, -0.06973952],\n",
       "         [ 0.01789305, -0.04733179, -0.07811883],\n",
       "         [ 0.03965376,  0.02705892, -0.02062972]],\n",
       " \n",
       "        [[ 0.04027184,  0.02696276, -0.03478654],\n",
       "         [ 0.01699846, -0.0356561 , -0.06051984],\n",
       "         [ 0.00167699, -0.07468846,  0.02332556]],\n",
       " \n",
       "        [[ 0.00937259, -0.02222659,  0.00294011],\n",
       "         [-0.00453893, -0.02022115,  0.00064542],\n",
       "         [ 0.04134914, -0.0212408 , -0.03578481]],\n",
       " \n",
       "        [[ 0.03350599, -0.01408748, -0.04866175],\n",
       "         [ 0.03157796,  0.00575955, -0.01760453],\n",
       "         [ 0.01945974,  0.07058366,  0.07139949]],\n",
       " \n",
       "        [[ 0.0090562 , -0.05925441, -0.03474   ],\n",
       "         [ 0.02461701,  0.01913464, -0.00364522],\n",
       "         [-0.06241291, -0.00530358,  0.05993728]],\n",
       " \n",
       "        [[-0.00344754,  0.04468941, -0.06493618],\n",
       "         [ 0.07812162, -0.02887907,  0.07704481],\n",
       "         [-0.04664103,  0.05794958,  0.01363096]],\n",
       " \n",
       "        [[-0.07789683,  0.06048554,  0.07498752],\n",
       "         [ 0.0578376 ,  0.07766249, -0.00212652],\n",
       "         [-0.00672533, -0.01531854, -0.03116606]],\n",
       " \n",
       "        [[ 0.01559337,  0.02295598, -0.04138203],\n",
       "         [-0.00083979, -0.01745686, -0.01594988],\n",
       "         [-0.05712173, -0.0429386 , -0.03613848]],\n",
       " \n",
       "        [[ 0.06318442, -0.07966474, -0.03413365],\n",
       "         [-0.07716598, -0.00714352, -0.06761996],\n",
       "         [ 0.03485677, -0.00083253,  0.02408566]],\n",
       " \n",
       "        [[ 0.06248809,  0.00236446,  0.07798347],\n",
       "         [ 0.07014433, -0.03862686,  0.06923039],\n",
       "         [ 0.00197005,  0.0452572 , -0.03437967]],\n",
       " \n",
       "        [[ 0.07512075, -0.02640279, -0.04559949],\n",
       "         [-0.01295788, -0.07262402,  0.07101075],\n",
       "         [-0.03793278,  0.04527963,  0.00715496]],\n",
       " \n",
       "        [[ 0.07913386, -0.05325326,  0.00440085],\n",
       "         [-0.01539252,  0.03396346, -0.07576336],\n",
       "         [ 0.05349663,  0.08076441, -0.04399871]],\n",
       " \n",
       "        [[ 0.00638498,  0.032145  , -0.05908158],\n",
       "         [-0.01791188,  0.00750548,  0.04135259],\n",
       "         [ 0.03023184, -0.00035558, -0.00050247]],\n",
       " \n",
       "        [[ 0.06559899, -0.00257603, -0.04609726],\n",
       "         [ 0.01010065,  0.01415613,  0.05003955],\n",
       "         [ 0.05803361,  0.07743109, -0.04898902]],\n",
       " \n",
       "        [[-0.0470945 , -0.02331729,  0.03947029],\n",
       "         [ 0.04483796,  0.08012351,  0.01518264],\n",
       "         [ 0.04739887, -0.02016873, -0.0501169 ]],\n",
       " \n",
       "        [[ 0.05861279,  0.04574456,  0.06424446],\n",
       "         [-0.02898992,  0.01227377, -0.07005804],\n",
       "         [-0.05033572,  0.08302148,  0.04265022]],\n",
       " \n",
       "        [[-0.05931303, -0.01470257,  0.03496958],\n",
       "         [ 0.03785726,  0.03211269,  0.00986699],\n",
       "         [ 0.02098572,  0.00807763, -0.02748099]],\n",
       " \n",
       "        [[-0.03792346, -0.0368175 ,  0.00664816],\n",
       "         [ 0.06058839, -0.00339388,  0.0478829 ],\n",
       "         [ 0.05204386, -0.00698851,  0.01485206]],\n",
       " \n",
       "        [[-0.04218518,  0.02583669, -0.04126038],\n",
       "         [ 0.03316657, -0.00269161, -0.02921139],\n",
       "         [-0.02050266,  0.03691714,  0.0645415 ]],\n",
       " \n",
       "        [[-0.07778399, -0.01818486, -0.00081741],\n",
       "         [-0.04523399, -0.01793501, -0.03152966],\n",
       "         [ 0.05364239,  0.02662978, -0.06164692]],\n",
       " \n",
       "        [[ 0.05039312, -0.00032002, -0.01618013],\n",
       "         [ 0.04420405, -0.05417594,  0.00349   ],\n",
       "         [ 0.07908449,  0.03072956, -0.01984522]],\n",
       " \n",
       "        [[-0.02716242,  0.07545841,  0.0402212 ],\n",
       "         [-0.04592715, -0.08107109,  0.04583486],\n",
       "         [ 0.00722332,  0.07580162, -0.05138348]],\n",
       " \n",
       "        [[-0.0173174 , -0.06642836, -0.01274783],\n",
       "         [ 0.04387495, -0.07934638, -0.02460725],\n",
       "         [-0.05131903, -0.00043414,  0.0562948 ]],\n",
       " \n",
       "        [[-0.08389887, -0.0613771 , -0.00043947],\n",
       "         [ 0.01029751,  0.03264537,  0.05702966],\n",
       "         [-0.00617032, -0.01487151, -0.05858624]],\n",
       " \n",
       "        [[-0.00820936, -0.01002822, -0.04613715],\n",
       "         [ 0.07425555, -0.02468023,  0.07237402],\n",
       "         [-0.01324206,  0.04161511, -0.0647688 ]],\n",
       " \n",
       "        [[ 0.07877257,  0.05559601, -0.04180017],\n",
       "         [ 0.03841256, -0.02286148,  0.00885351],\n",
       "         [-0.05772046, -0.03281082,  0.05186957]],\n",
       " \n",
       "        [[ 0.01685975, -0.00174445, -0.07819854],\n",
       "         [ 0.07986469,  0.07870494, -0.00800515],\n",
       "         [-0.05033953, -0.00420657,  0.07956921]],\n",
       " \n",
       "        [[-0.02868326, -0.04319962,  0.05229402],\n",
       "         [ 0.04457258, -0.04616517,  0.07691415],\n",
       "         [-0.07593093,  0.03386477,  0.01165166]],\n",
       " \n",
       "        [[ 0.03318991,  0.05594352, -0.06392416],\n",
       "         [ 0.01187314,  0.07501479, -0.07898453],\n",
       "         [ 0.05705049,  0.07210122,  0.0459831 ]],\n",
       " \n",
       "        [[-0.07910678,  0.04052816,  0.05982498],\n",
       "         [ 0.00213553,  0.02099834,  0.02604454],\n",
       "         [ 0.07289563, -0.07472201, -0.02115221]],\n",
       " \n",
       "        [[ 0.01960108,  0.06106883, -0.07490632],\n",
       "         [-0.04842009,  0.08104477,  0.07379447],\n",
       "         [-0.04527414,  0.06460252, -0.05530976]],\n",
       " \n",
       "        [[ 0.05346673, -0.02335242, -0.04049123],\n",
       "         [ 0.06227488, -0.04618554, -0.03537649],\n",
       "         [ 0.03773264,  0.04025238,  0.02192091]]], dtype=float32), bias=-0.00080124172, layer=1, neuron_number=25, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.01618041, -0.03927527, -0.05484176],\n",
       "         [ 0.08017252,  0.05594991, -0.04140204],\n",
       "         [ 0.06840586, -0.04670791,  0.063716  ]],\n",
       " \n",
       "        [[ 0.03160016,  0.07939655,  0.03387303],\n",
       "         [ 0.02834699,  0.04534459, -0.08191822],\n",
       "         [ 0.06304454, -0.018626  , -0.02551736]],\n",
       " \n",
       "        [[ 0.05435057, -0.0336609 ,  0.03652157],\n",
       "         [ 0.07088251, -0.04639693,  0.05648212],\n",
       "         [-0.02885896, -0.05556902,  0.04989658]],\n",
       " \n",
       "        [[ 0.05726847, -0.00955973,  0.07345739],\n",
       "         [-0.04033889,  0.06353053,  0.07063849],\n",
       "         [-0.04699885, -0.07309797,  0.03333117]],\n",
       " \n",
       "        [[ 0.03725642, -0.01542663, -0.05137614],\n",
       "         [ 0.04875112,  0.07525048, -0.04162769],\n",
       "         [-0.0158394 , -0.07432497,  0.05628021]],\n",
       " \n",
       "        [[-0.00585846,  0.06132839, -0.0349552 ],\n",
       "         [ 0.04979542, -0.06859653, -0.02258426],\n",
       "         [-0.01526729, -0.01576369,  0.00813304]],\n",
       " \n",
       "        [[ 0.07149049, -0.05019948,  0.00997694],\n",
       "         [-0.07483825, -0.03821831,  0.04165028],\n",
       "         [ 0.07911815,  0.02455302, -0.03464372]],\n",
       " \n",
       "        [[ 0.00595347, -0.05958797, -0.04214292],\n",
       "         [-0.03927867, -0.06917135,  0.02640091],\n",
       "         [ 0.01747499,  0.06486996, -0.00273266]],\n",
       " \n",
       "        [[ 0.05977595,  0.05685562, -0.04555887],\n",
       "         [ 0.04742549,  0.02203668, -0.05031975],\n",
       "         [-0.07707716, -0.07541544, -0.04048919]],\n",
       " \n",
       "        [[-0.06022128,  0.07085988, -0.05176943],\n",
       "         [-0.02461303, -0.08015531, -0.04529168],\n",
       "         [-0.05640402,  0.0819428 , -0.07610458]],\n",
       " \n",
       "        [[ 0.04185036,  0.00057582,  0.00379836],\n",
       "         [ 0.05337353,  0.05097113,  0.03633162],\n",
       "         [-0.03104332, -0.03700436, -0.05142335]],\n",
       " \n",
       "        [[-0.07626202,  0.05528465, -0.00469609],\n",
       "         [ 0.01260654, -0.03964177, -0.01301352],\n",
       "         [ 0.04699437, -0.00336008,  0.00267317]],\n",
       " \n",
       "        [[-0.01683291,  0.02894861,  0.00201435],\n",
       "         [-0.06020495,  0.00229686,  0.03782898],\n",
       "         [ 0.07001507,  0.0249306 , -0.03420258]],\n",
       " \n",
       "        [[ 0.01112619, -0.0073754 , -0.00454366],\n",
       "         [ 0.03876915, -0.07673393, -0.0388071 ],\n",
       "         [-0.06265629, -0.06954063, -0.075179  ]],\n",
       " \n",
       "        [[-0.0755655 ,  0.07750068,  0.02395595],\n",
       "         [ 0.01218203, -0.01154544, -0.00543215],\n",
       "         [ 0.00345882, -0.00556488, -0.06064808]],\n",
       " \n",
       "        [[-0.01411182,  0.05916464,  0.08038506],\n",
       "         [-0.0080446 , -0.026447  ,  0.07350613],\n",
       "         [ 0.05723808,  0.04810325, -0.05739374]],\n",
       " \n",
       "        [[-0.06581099,  0.03242141,  0.03482816],\n",
       "         [-0.0801963 ,  0.00712622, -0.08100381],\n",
       "         [ 0.04739513,  0.05337121,  0.03191671]],\n",
       " \n",
       "        [[-0.0699551 ,  0.08215311, -0.01993411],\n",
       "         [-0.07327953, -0.0185475 , -0.02684444],\n",
       "         [ 0.01086865,  0.07721789,  0.03325333]],\n",
       " \n",
       "        [[-0.07149965, -0.07916005,  0.0120806 ],\n",
       "         [-0.00949633, -0.01958275, -0.07729461],\n",
       "         [ 0.01342141,  0.01751663,  0.06635444]],\n",
       " \n",
       "        [[-0.07404213, -0.01447511,  0.03940532],\n",
       "         [ 0.01122969,  0.02109384, -0.01120721],\n",
       "         [ 0.01970077,  0.07944089, -0.02711708]],\n",
       " \n",
       "        [[-0.07552887, -0.03448859,  0.0627937 ],\n",
       "         [ 0.0417183 , -0.05012167,  0.02946385],\n",
       "         [-0.00936665, -0.07000724,  0.06251983]],\n",
       " \n",
       "        [[ 0.05114343, -0.0238188 , -0.03438578],\n",
       "         [-0.01445265,  0.06025305, -0.0779927 ],\n",
       "         [-0.06590645,  0.01809734, -0.01540272]],\n",
       " \n",
       "        [[-0.06215191,  0.06503733,  0.07118095],\n",
       "         [-0.03885122,  0.06615954,  0.04264178],\n",
       "         [-0.04710209,  0.07371897,  0.02310124]],\n",
       " \n",
       "        [[ 0.01717652,  0.03050209,  0.07691792],\n",
       "         [-0.03557143,  0.01656714,  0.04660102],\n",
       "         [ 0.01181678,  0.06153755, -0.01058917]],\n",
       " \n",
       "        [[-0.08019271, -0.01655789,  0.00476646],\n",
       "         [ 0.04390863,  0.08111279,  0.01236785],\n",
       "         [-0.06166857,  0.02452065, -0.00861976]],\n",
       " \n",
       "        [[ 0.01434315, -0.06227791,  0.03202913],\n",
       "         [ 0.0123381 ,  0.0828985 , -0.04484707],\n",
       "         [ 0.076004  ,  0.007132  ,  0.02764571]],\n",
       " \n",
       "        [[ 0.014016  , -0.03094533,  0.08248017],\n",
       "         [-0.0268735 , -0.02526129, -0.06215091],\n",
       "         [ 0.00729297, -0.00690853,  0.03990098]],\n",
       " \n",
       "        [[-0.05197344,  0.05948475,  0.02382722],\n",
       "         [-0.05958174, -0.03201976, -0.06651359],\n",
       "         [-0.02333498,  0.01700016,  0.03467419]],\n",
       " \n",
       "        [[-0.07428186, -0.02943331, -0.07779555],\n",
       "         [ 0.00805263,  0.05623279,  0.04675198],\n",
       "         [-0.02753378,  0.05118604, -0.07076503]],\n",
       " \n",
       "        [[-0.04343741, -0.06655147,  0.01212682],\n",
       "         [ 0.01808094, -0.00341346, -0.04935716],\n",
       "         [-0.08360393,  0.05954188, -0.07386082]],\n",
       " \n",
       "        [[-0.06081773, -0.00421151,  0.07266746],\n",
       "         [ 0.07391709,  0.04318587,  0.02749071],\n",
       "         [-0.04322809,  0.02673931,  0.03309424]],\n",
       " \n",
       "        [[-0.04587711,  0.06448971,  0.04482465],\n",
       "         [ 0.04903387, -0.06047604,  0.00392892],\n",
       "         [-0.07472974,  0.01340724, -0.07794278]]], dtype=float32), bias=-0.00087724731, layer=1, neuron_number=26, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.0629634 , -0.04655571, -0.06601918],\n",
       "         [-0.0502752 , -0.03764658,  0.0792862 ],\n",
       "         [ 0.01649042, -0.05241016,  0.02494952]],\n",
       " \n",
       "        [[-0.07658211,  0.06492386,  0.04464868],\n",
       "         [ 0.0049674 ,  0.00840808,  0.07697468],\n",
       "         [ 0.03095407,  0.07030804, -0.02037145]],\n",
       " \n",
       "        [[ 0.03234886,  0.0207263 ,  0.07697316],\n",
       "         [ 0.04895905, -0.02734114, -0.06320836],\n",
       "         [-0.05724387,  0.01020524, -0.06118006]],\n",
       " \n",
       "        [[ 0.03244385, -0.06632006, -0.07733788],\n",
       "         [-0.06451824, -0.00599075,  0.07886743],\n",
       "         [ 0.02059069, -0.06531176, -0.05732468]],\n",
       " \n",
       "        [[-0.04506394,  0.05925669,  0.07053428],\n",
       "         [ 0.05755902, -0.01958854,  0.06676218],\n",
       "         [-0.06681107,  0.0654126 ,  0.02135258]],\n",
       " \n",
       "        [[-0.05902465,  0.04834348, -0.05165535],\n",
       "         [-0.04585087, -0.02464379, -0.03016039],\n",
       "         [-0.01476898,  0.03292927,  0.01126712]],\n",
       " \n",
       "        [[-0.07327807, -0.00781693, -0.01129752],\n",
       "         [-0.03465747, -0.05938919,  0.00418263],\n",
       "         [ 0.01572186, -0.06042886, -0.03837306]],\n",
       " \n",
       "        [[ 0.06477395,  0.06329812,  0.06138834],\n",
       "         [-0.02206857, -0.03533702,  0.04844382],\n",
       "         [ 0.05140229,  0.02144264,  0.07200585]],\n",
       " \n",
       "        [[-0.07962824,  0.01309118,  0.06860705],\n",
       "         [ 0.0615974 ,  0.0653667 ,  0.01065861],\n",
       "         [ 0.00165439, -0.04534441, -0.0716446 ]],\n",
       " \n",
       "        [[ 0.02428612,  0.0788228 ,  0.02461012],\n",
       "         [ 0.04480854,  0.02564735,  0.06455819],\n",
       "         [-0.04450757,  0.03345104, -0.03747677]],\n",
       " \n",
       "        [[-0.03754105,  0.04486798, -0.06895649],\n",
       "         [ 0.03605246, -0.02253746, -0.0695952 ],\n",
       "         [-0.01842006, -0.06988981,  0.06145968]],\n",
       " \n",
       "        [[ 0.01999852,  0.03784651, -0.02520922],\n",
       "         [ 0.08369066, -0.00518512, -0.00659078],\n",
       "         [ 0.05573004, -0.08214981,  0.04944168]],\n",
       " \n",
       "        [[-0.07211749, -0.04795119, -0.00758721],\n",
       "         [-0.04566482,  0.00729431,  0.01072195],\n",
       "         [-0.02804762, -0.05680179, -0.04483857]],\n",
       " \n",
       "        [[-0.07170822, -0.02481896,  0.07370595],\n",
       "         [ 0.00338316, -0.04516104, -0.06891333],\n",
       "         [-0.04299828,  0.02068452,  0.01093194]],\n",
       " \n",
       "        [[-0.00508091, -0.00114251,  0.05115141],\n",
       "         [ 0.00615305, -0.05848209,  0.01339247],\n",
       "         [ 0.00680917,  0.05739971, -0.04734659]],\n",
       " \n",
       "        [[ 0.00902946, -0.05033767,  0.03042491],\n",
       "         [ 0.05225568, -0.04426382,  0.03028078],\n",
       "         [ 0.04666599, -0.0098373 ,  0.03892485]],\n",
       " \n",
       "        [[ 0.08170078,  0.04819366,  0.06781276],\n",
       "         [ 0.02436021,  0.06225654,  0.0655589 ],\n",
       "         [-0.0075116 ,  0.07938796, -0.02463279]],\n",
       " \n",
       "        [[ 0.00070982,  0.08091643, -0.03948696],\n",
       "         [ 0.02382628,  0.04961364,  0.01543003],\n",
       "         [-0.0176063 , -0.04479237,  0.05313865]],\n",
       " \n",
       "        [[ 0.03602849, -0.07772248, -0.00907133],\n",
       "         [ 0.06800658,  0.07789316,  0.0286261 ],\n",
       "         [-0.0168333 , -0.03326736, -0.04850338]],\n",
       " \n",
       "        [[-0.07780266, -0.02267877,  0.07421265],\n",
       "         [-0.00608754, -0.04688877,  0.05621927],\n",
       "         [-0.05846593, -0.0319804 ,  0.05796896]],\n",
       " \n",
       "        [[-0.05486215,  0.07036778, -0.08056029],\n",
       "         [-0.00036319,  0.04784084,  0.05932194],\n",
       "         [ 0.03967572,  0.05090213,  0.01559919]],\n",
       " \n",
       "        [[ 0.05150564,  0.02895131,  0.0334065 ],\n",
       "         [-0.03248823,  0.03416064,  0.07993154],\n",
       "         [-0.0053434 , -0.05930136,  0.02032256]],\n",
       " \n",
       "        [[ 0.01583875, -0.07463463,  0.05691287],\n",
       "         [-0.07730076,  0.03432963,  0.0226124 ],\n",
       "         [ 0.00530368,  0.06156956,  0.06058915]],\n",
       " \n",
       "        [[ 0.01304962, -0.07613375,  0.04823148],\n",
       "         [ 0.03087313, -0.0178219 , -0.00981847],\n",
       "         [-0.08066963,  0.02128425, -0.07215757]],\n",
       " \n",
       "        [[-0.01730078,  0.04490461,  0.01620782],\n",
       "         [ 0.05800519,  0.05372812,  0.02277534],\n",
       "         [ 0.01414042,  0.0376106 , -0.04271121]],\n",
       " \n",
       "        [[-0.07703211,  0.03762095,  0.00829815],\n",
       "         [-0.05257151,  0.03178388,  0.07263171],\n",
       "         [ 0.0068161 ,  0.03643758, -0.0067827 ]],\n",
       " \n",
       "        [[-0.0701955 , -0.04653562, -0.0815603 ],\n",
       "         [ 0.06808379,  0.07113846,  0.05176865],\n",
       "         [-0.03574678,  0.06671207,  0.03537612]],\n",
       " \n",
       "        [[-0.0312868 ,  0.07051899,  0.0532949 ],\n",
       "         [-0.03883601, -0.06900318,  0.00338818],\n",
       "         [ 0.03307486,  0.03066779,  0.0247143 ]],\n",
       " \n",
       "        [[-0.04729488,  0.07297418,  0.01067641],\n",
       "         [-0.04329065, -0.03439736, -0.00386742],\n",
       "         [-0.01102574, -0.0025664 , -0.04994928]],\n",
       " \n",
       "        [[ 0.00623128, -0.03562015,  0.07947665],\n",
       "         [-0.02202983, -0.08166791, -0.02848433],\n",
       "         [ 0.07751023,  0.00584872,  0.01278869]],\n",
       " \n",
       "        [[ 0.0766508 , -0.04632499,  0.05133344],\n",
       "         [ 0.03287754,  0.02514132,  0.0161976 ],\n",
       "         [-0.01398977, -0.07399468, -0.01434327]],\n",
       " \n",
       "        [[ 0.05824724, -0.01706015,  0.05878264],\n",
       "         [ 0.05658169,  0.00423311,  0.04031843],\n",
       "         [-0.06481314, -0.04138655,  0.03718655]]], dtype=float32), bias=-3.9584935e-05, layer=1, neuron_number=27, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ -3.59222777e-02,   6.23584539e-02,  -4.79722209e-03],\n",
       "         [  3.80650237e-02,   4.25308272e-02,  -3.12317982e-02],\n",
       "         [  1.52435079e-02,   8.18049833e-02,  -2.62135845e-02]],\n",
       " \n",
       "        [[ -5.63640036e-02,   5.64614162e-02,   4.05207872e-02],\n",
       "         [  6.93185404e-02,   3.90439592e-02,   4.37966175e-02],\n",
       "         [  2.89706426e-04,  -8.12130347e-02,  -1.54260015e-02]],\n",
       " \n",
       "        [[ -3.17986589e-03,  -2.47807521e-02,   4.51358967e-02],\n",
       "         [ -1.90729387e-02,  -6.99917674e-02,  -7.12873042e-02],\n",
       "         [  6.58295536e-03,  -1.83561835e-02,  -3.58907543e-02]],\n",
       " \n",
       "        [[ -3.55925150e-02,  -6.83552399e-02,  -1.57148372e-02],\n",
       "         [ -5.97476661e-02,  -6.67206794e-02,  -4.95599322e-02],\n",
       "         [  5.22651933e-02,   4.20040777e-03,   4.92454730e-02]],\n",
       " \n",
       "        [[  5.70473028e-03,   7.05542117e-02,  -6.99722096e-02],\n",
       "         [  1.96453296e-02,  -7.88206756e-02,  -6.00478053e-02],\n",
       "         [ -7.80176520e-02,  -5.79957739e-02,  -4.20946702e-02]],\n",
       " \n",
       "        [[  2.00885311e-02,   8.09892565e-02,  -2.76875682e-02],\n",
       "         [  6.69831336e-02,   8.06286857e-02,  -6.04348034e-02],\n",
       "         [ -2.13562325e-02,   1.70865487e-02,   1.63640045e-02]],\n",
       " \n",
       "        [[ -6.00292087e-02,  -6.90605352e-03,  -7.03488812e-02],\n",
       "         [ -4.14387845e-02,  -7.21928552e-02,   1.86292864e-02],\n",
       "         [  3.62762213e-02,   5.10257557e-02,  -3.80288996e-02]],\n",
       " \n",
       "        [[  5.17305583e-02,   1.77216940e-02,  -8.00106302e-03],\n",
       "         [  5.40748425e-02,  -5.39102219e-02,  -1.40373036e-02],\n",
       "         [ -7.81246424e-02,   7.93636888e-02,   5.47362529e-02]],\n",
       " \n",
       "        [[ -3.11346035e-02,   2.81355083e-02,  -7.20220655e-02],\n",
       "         [  4.64998931e-03,   4.90890294e-02,   4.57006879e-02],\n",
       "         [ -2.42981035e-02,  -3.87963057e-02,  -6.69630915e-02]],\n",
       " \n",
       "        [[  5.76085821e-02,  -8.16898867e-02,   5.64357042e-02],\n",
       "         [ -3.73432338e-02,  -2.24757921e-02,  -3.65472189e-03],\n",
       "         [ -4.86780964e-02,  -8.33989605e-02,  -3.91383581e-02]],\n",
       " \n",
       "        [[ -2.33521275e-02,  -7.82589242e-02,   3.26430053e-02],\n",
       "         [  6.85041696e-02,  -5.72785512e-02,   6.45156801e-02],\n",
       "         [ -5.82165793e-02,  -1.65089848e-03,  -2.43363045e-02]],\n",
       " \n",
       "        [[  4.32590302e-03,  -5.56095578e-02,  -4.80048396e-02],\n",
       "         [ -5.37688173e-02,  -4.55152988e-02,  -2.54848152e-02],\n",
       "         [ -7.78518841e-02,   2.03364133e-03,  -1.21325231e-03]],\n",
       " \n",
       "        [[ -1.36426352e-02,  -5.56343310e-02,   8.63836985e-03],\n",
       "         [  7.73314163e-02,  -3.50136422e-02,   7.57859380e-05],\n",
       "         [  3.17639671e-02,  -3.88517044e-02,   2.52458919e-02]],\n",
       " \n",
       "        [[  2.53945589e-03,  -2.94478107e-02,  -3.78935859e-02],\n",
       "         [ -2.67520119e-02,  -2.90265847e-02,   6.45340905e-02],\n",
       "         [ -3.84789445e-02,  -7.06363916e-02,   6.28788918e-02]],\n",
       " \n",
       "        [[  5.96946403e-02,  -1.99194923e-02,   5.15610613e-02],\n",
       "         [  3.08370832e-02,  -6.61173463e-02,  -1.75206773e-02],\n",
       "         [ -4.09012288e-02,   4.52068970e-02,   7.92673305e-02]],\n",
       " \n",
       "        [[ -7.98307136e-02,   3.85508202e-02,   6.83149993e-02],\n",
       "         [  6.43694028e-02,   2.05811975e-03,   1.48344133e-02],\n",
       "         [ -1.42093981e-02,  -5.57765691e-03,   2.47847196e-02]],\n",
       " \n",
       "        [[ -1.51945306e-02,  -6.37651905e-02,   6.68037385e-02],\n",
       "         [  2.65122931e-02,   1.53457755e-02,  -3.47131155e-02],\n",
       "         [ -6.69455305e-02,  -9.23731178e-03,   5.61315641e-02]],\n",
       " \n",
       "        [[ -1.98952481e-02,   8.17968994e-02,  -6.72147125e-02],\n",
       "         [ -1.49650024e-02,   5.32600954e-02,   2.48305257e-02],\n",
       "         [ -4.04121615e-02,  -1.98337324e-02,  -1.79784056e-02]],\n",
       " \n",
       "        [[ -5.34683168e-02,   7.33142272e-02,   4.57321070e-02],\n",
       "         [  5.73084913e-02,  -1.74955698e-03,   6.26096874e-02],\n",
       "         [  3.58470604e-02,  -7.56328553e-02,   6.38872907e-02]],\n",
       " \n",
       "        [[  3.94802056e-02,   3.79417762e-02,   8.25801641e-02],\n",
       "         [ -1.21927084e-02,   5.21017984e-02,  -3.85096520e-02],\n",
       "         [ -2.23052688e-02,   2.05900613e-02,   7.14031681e-02]],\n",
       " \n",
       "        [[ -6.37581795e-02,   6.02023862e-03,   6.37532845e-02],\n",
       "         [ -3.71150859e-02,  -2.79056355e-02,  -2.62162238e-02],\n",
       "         [ -4.18882594e-02,   1.33438101e-02,   4.92372252e-02]],\n",
       " \n",
       "        [[  4.30911630e-02,  -5.55160164e-04,   8.25385526e-02],\n",
       "         [  6.13154732e-02,   5.66263236e-02,   3.40563841e-02],\n",
       "         [ -4.73254323e-02,   4.01295163e-02,  -5.16447471e-03]],\n",
       " \n",
       "        [[  2.17307117e-02,   6.06170483e-02,  -7.28728250e-02],\n",
       "         [ -5.58739081e-02,   6.31384403e-02,   5.07618636e-02],\n",
       "         [ -1.24470815e-02,   8.76488909e-03,   3.30065712e-02]],\n",
       " \n",
       "        [[  2.94020195e-02,  -5.62875867e-02,  -7.51913860e-02],\n",
       "         [ -3.12602520e-02,   1.47807235e-02,   3.05070914e-02],\n",
       "         [ -5.92297390e-02,  -8.25453699e-02,  -1.46161541e-02]],\n",
       " \n",
       "        [[ -5.60172647e-02,   6.74282536e-02,  -3.62053923e-02],\n",
       "         [ -8.15683138e-03,   1.17034512e-02,   5.45725934e-02],\n",
       "         [ -1.42912548e-02,   4.84367907e-02,   6.47806376e-02]],\n",
       " \n",
       "        [[  6.46409988e-02,  -4.18912880e-02,  -8.06887746e-02],\n",
       "         [ -6.10582938e-04,   7.90977329e-02,  -2.17938088e-02],\n",
       "         [  4.05379832e-02,  -6.68355450e-02,  -5.88988923e-02]],\n",
       " \n",
       "        [[ -1.67395547e-02,  -5.81846535e-02,   1.06736748e-02],\n",
       "         [  8.02782774e-02,   2.92885769e-02,   1.88134145e-02],\n",
       "         [  2.23574713e-02,   6.51107654e-02,  -3.93017903e-02]],\n",
       " \n",
       "        [[  2.83798613e-02,   8.18595812e-02,   1.11630035e-03],\n",
       "         [ -2.32414082e-02,  -7.26039335e-02,   5.97985871e-02],\n",
       "         [ -6.23225830e-02,   7.00259581e-02,  -2.22341977e-02]],\n",
       " \n",
       "        [[ -3.03102080e-02,   4.16639872e-04,   8.30974132e-02],\n",
       "         [  6.76286295e-02,   5.12297917e-03,   5.91985546e-02],\n",
       "         [  1.86527297e-02,  -6.20595962e-02,  -4.80234437e-02]],\n",
       " \n",
       "        [[  6.94086403e-02,  -2.52191518e-02,   6.74041286e-02],\n",
       "         [  7.46662691e-02,   6.56381100e-02,   4.49850969e-02],\n",
       "         [  1.27583398e-02,  -1.13409786e-02,  -6.25423491e-02]],\n",
       " \n",
       "        [[ -7.30281696e-02,   2.03546360e-02,   8.17545503e-02],\n",
       "         [  3.54200951e-03,  -1.08422376e-02,  -7.44050890e-02],\n",
       "         [ -3.98693196e-02,   6.17365800e-02,  -5.65784937e-03]],\n",
       " \n",
       "        [[  3.92892472e-02,   4.21129167e-03,  -2.36371625e-02],\n",
       "         [  4.94717620e-03,  -4.27287370e-02,   1.23985754e-02],\n",
       "         [ -6.88462630e-02,  -6.25378117e-02,   7.40134567e-02]]], dtype=float32), bias=0.00078676693, layer=1, neuron_number=28, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.06816307, -0.01184653,  0.00764895],\n",
       "         [-0.07179396, -0.03713436,  0.06678095],\n",
       "         [-0.01495957,  0.00784864, -0.00826953]],\n",
       " \n",
       "        [[ 0.01139002, -0.05304168,  0.07451126],\n",
       "         [-0.04793534, -0.05361909, -0.06995961],\n",
       "         [ 0.0633532 ,  0.04995587,  0.03774171]],\n",
       " \n",
       "        [[-0.07680925, -0.07482   ,  0.06010439],\n",
       "         [ 0.02609388, -0.02983031, -0.07525095],\n",
       "         [-0.08176386,  0.06176942,  0.03166192]],\n",
       " \n",
       "        [[ 0.0773406 ,  0.02302353,  0.05899295],\n",
       "         [-0.07120428, -0.08220311, -0.03579927],\n",
       "         [-0.05193646,  0.02957132,  0.0274939 ]],\n",
       " \n",
       "        [[ 0.03938186, -0.05063187,  0.05352697],\n",
       "         [ 0.02469878, -0.02294541,  0.04794955],\n",
       "         [ 0.04044828,  0.07651115, -0.03816087]],\n",
       " \n",
       "        [[ 0.07326138, -0.06225045, -0.05004352],\n",
       "         [-0.03001793, -0.06935624,  0.08009552],\n",
       "         [-0.04644715, -0.0304463 , -0.05596501]],\n",
       " \n",
       "        [[-0.03531224, -0.05645281, -0.07166452],\n",
       "         [ 0.04913945,  0.07473502,  0.07580886],\n",
       "         [ 0.00623101,  0.01421421, -0.00101869]],\n",
       " \n",
       "        [[ 0.0408315 , -0.02040612, -0.05459569],\n",
       "         [-0.00251811, -0.076456  ,  0.01133358],\n",
       "         [ 0.04724472,  0.06311996,  0.00456444]],\n",
       " \n",
       "        [[ 0.04937689,  0.00068279,  0.07742439],\n",
       "         [ 0.01719804, -0.02065212,  0.01074294],\n",
       "         [ 0.02222372, -0.07193901,  0.08323465]],\n",
       " \n",
       "        [[-0.06814897, -0.00770201, -0.06357756],\n",
       "         [-0.07468393,  0.03459731, -0.01372508],\n",
       "         [ 0.05493439, -0.00821588,  0.06707607]],\n",
       " \n",
       "        [[-0.05990027, -0.03380316,  0.08330178],\n",
       "         [ 0.02677429,  0.06300897,  0.02029086],\n",
       "         [ 0.01251045,  0.0680791 ,  0.07186623]],\n",
       " \n",
       "        [[-0.07144026, -0.03506477, -0.05558202],\n",
       "         [ 0.01024586, -0.04297481, -0.04398432],\n",
       "         [-0.01580785, -0.01007731,  0.02031179]],\n",
       " \n",
       "        [[ 0.01633113,  0.01082878,  0.08417083],\n",
       "         [ 0.03161044, -0.04666848, -0.05740429],\n",
       "         [ 0.05752652, -0.01979979,  0.02707597]],\n",
       " \n",
       "        [[-0.02759968,  0.02526804,  0.02391764],\n",
       "         [-0.02208423, -0.0482137 , -0.05666418],\n",
       "         [ 0.06301764, -0.0185704 , -0.01548492]],\n",
       " \n",
       "        [[ 0.07381942,  0.07734886, -0.05320571],\n",
       "         [ 0.08126985, -0.06699861,  0.07619049],\n",
       "         [-0.0483356 ,  0.02420738, -0.03527401]],\n",
       " \n",
       "        [[-0.06571986, -0.05539494, -0.04892486],\n",
       "         [-0.06725249, -0.03988767, -0.0611809 ],\n",
       "         [-0.08346776,  0.07190239,  0.05083299]],\n",
       " \n",
       "        [[ 0.02035794,  0.00367929, -0.07801999],\n",
       "         [-0.07415784, -0.0386891 ,  0.08247444],\n",
       "         [-0.0776753 ,  0.03839324,  0.01448099]],\n",
       " \n",
       "        [[ 0.00373848,  0.03978803, -0.02577933],\n",
       "         [ 0.05538863, -0.05174467,  0.00231087],\n",
       "         [-0.00857101,  0.06185129, -0.04858402]],\n",
       " \n",
       "        [[-0.0546959 ,  0.00547844, -0.00633229],\n",
       "         [-0.01198252,  0.05126014,  0.01448984],\n",
       "         [-0.00205149, -0.01500716,  0.03543168]],\n",
       " \n",
       "        [[ 0.06097125,  0.03822142,  0.05189864],\n",
       "         [-0.07701806,  0.07511128, -0.04386602],\n",
       "         [-0.08213125, -0.04681933,  0.01094482]],\n",
       " \n",
       "        [[ 0.00556862, -0.00806894, -0.00215609],\n",
       "         [ 0.03661572, -0.03311628,  0.06422065],\n",
       "         [ 0.07618521,  0.06230331,  0.0808517 ]],\n",
       " \n",
       "        [[ 0.05521547, -0.08049755,  0.03073323],\n",
       "         [-0.05437563, -0.03473832,  0.01932717],\n",
       "         [-0.05626998, -0.07800751, -0.0675502 ]],\n",
       " \n",
       "        [[-0.00158041, -0.03374034,  0.01168517],\n",
       "         [-0.04405123,  0.03162034,  0.03416661],\n",
       "         [ 0.01622128,  0.00117502, -0.00573058]],\n",
       " \n",
       "        [[ 0.03260943, -0.05135828,  0.03529251],\n",
       "         [ 0.08382163,  0.06696105, -0.03995303],\n",
       "         [ 0.00272052, -0.01540093,  0.04519149]],\n",
       " \n",
       "        [[ 0.07201607,  0.07264586, -0.03113964],\n",
       "         [-0.03468535, -0.02685958,  0.05403829],\n",
       "         [-0.04853025, -0.00412484,  0.07339152]],\n",
       " \n",
       "        [[ 0.07920467,  0.06639484,  0.03470539],\n",
       "         [ 0.06819701,  0.01066524, -0.00894354],\n",
       "         [-0.05321037, -0.06120212, -0.08134012]],\n",
       " \n",
       "        [[-0.04998884,  0.04463376,  0.05317476],\n",
       "         [ 0.05945379,  0.07325009, -0.01553946],\n",
       "         [-0.02924781,  0.00969956,  0.07591813]],\n",
       " \n",
       "        [[-0.02410554,  0.01975644, -0.06602735],\n",
       "         [-0.01470691, -0.01185629,  0.07922757],\n",
       "         [ 0.06437398,  0.00569036, -0.0495295 ]],\n",
       " \n",
       "        [[-0.04602811,  0.00902958, -0.04381311],\n",
       "         [-0.01269146,  0.03824784,  0.05155025],\n",
       "         [-0.02853893,  0.07672954, -0.0415796 ]],\n",
       " \n",
       "        [[-0.08085057, -0.01771287, -0.04460893],\n",
       "         [ 0.06132965,  0.04829953, -0.00430582],\n",
       "         [-0.00439737,  0.06882319, -0.06659088]],\n",
       " \n",
       "        [[ 0.00178121,  0.04971978,  0.02548978],\n",
       "         [ 0.02733178, -0.01421362,  0.02146486],\n",
       "         [ 0.0733992 , -0.06232751, -0.01775663]],\n",
       " \n",
       "        [[-0.045866  ,  0.08350848, -0.05708642],\n",
       "         [ 0.00529969, -0.01050985, -0.06002374],\n",
       "         [-0.05275461, -0.01486059, -0.06315017]]], dtype=float32), bias=-0.00048018259, layer=1, neuron_number=29, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.08222058,  0.01379034,  0.06193925],\n",
       "         [-0.03191727, -0.03978425,  0.07976682],\n",
       "         [ 0.07733349, -0.08464438, -0.02549525]],\n",
       " \n",
       "        [[ 0.05090218,  0.05427839,  0.07772733],\n",
       "         [ 0.0048132 ,  0.07644667,  0.01360973],\n",
       "         [-0.03653495,  0.05360734, -0.07434622]],\n",
       " \n",
       "        [[-0.03647993,  0.07289678, -0.05480455],\n",
       "         [-0.02587487, -0.0583185 ,  0.03796231],\n",
       "         [ 0.07683373,  0.004309  ,  0.00164203]],\n",
       " \n",
       "        [[ 0.07119005, -0.04193317,  0.02085504],\n",
       "         [-0.01027134,  0.00912971,  0.06562835],\n",
       "         [ 0.0810841 ,  0.06124149, -0.04210809]],\n",
       " \n",
       "        [[ 0.00501233, -0.04913637, -0.05945122],\n",
       "         [-0.04500956, -0.04997279,  0.06896438],\n",
       "         [ 0.04022063, -0.04506569,  0.02831748]],\n",
       " \n",
       "        [[ 0.02331413,  0.08009439,  0.01818521],\n",
       "         [-0.02366374, -0.02126511,  0.03137013],\n",
       "         [-0.02522744,  0.01631017, -0.08274468]],\n",
       " \n",
       "        [[ 0.01294259, -0.05204172,  0.02155668],\n",
       "         [-0.01680382, -0.07736492, -0.04565284],\n",
       "         [ 0.05632732,  0.08205263, -0.04731893]],\n",
       " \n",
       "        [[-0.042908  ,  0.07587972,  0.0005641 ],\n",
       "         [-0.03245022,  0.0306624 , -0.03983934],\n",
       "         [ 0.07205153, -0.02472534,  0.04502268]],\n",
       " \n",
       "        [[-0.00522189, -0.04348881, -0.05694126],\n",
       "         [ 0.03406565,  0.01613907,  0.07757532],\n",
       "         [ 0.03039877, -0.03632617,  0.06507545]],\n",
       " \n",
       "        [[ 0.08025335, -0.07840963,  0.02697963],\n",
       "         [ 0.06382068,  0.05820604,  0.04150006],\n",
       "         [ 0.07627899, -0.0390213 ,  0.02789917]],\n",
       " \n",
       "        [[ 0.03732046,  0.03027949,  0.08198712],\n",
       "         [ 0.04261077, -0.07096259, -0.07721172],\n",
       "         [ 0.0208472 ,  0.0732546 , -0.08270036]],\n",
       " \n",
       "        [[-0.03636059, -0.04463082, -0.02962899],\n",
       "         [-0.07416625, -0.07706112,  0.05769174],\n",
       "         [-0.00437937,  0.02773357,  0.0302727 ]],\n",
       " \n",
       "        [[ 0.008331  ,  0.0288343 ,  0.06540243],\n",
       "         [ 0.02343215,  0.0067611 ,  0.0427966 ],\n",
       "         [ 0.04234354,  0.04353021,  0.02026794]],\n",
       " \n",
       "        [[ 0.02933463, -0.05422735, -0.01211074],\n",
       "         [-0.04496989,  0.07356141, -0.08159266],\n",
       "         [ 0.00591298, -0.06369668,  0.05399815]],\n",
       " \n",
       "        [[ 0.01176586,  0.04031914,  0.00182433],\n",
       "         [ 0.02287494, -0.07224669, -0.0297599 ],\n",
       "         [ 0.02690603, -0.00510873,  0.05536478]],\n",
       " \n",
       "        [[-0.00641124,  0.05915392, -0.05231161],\n",
       "         [-0.02109495, -0.01414679, -0.00594182],\n",
       "         [-0.00310646, -0.03296686,  0.0531676 ]],\n",
       " \n",
       "        [[-0.04549114, -0.04018665,  0.07617576],\n",
       "         [-0.07383605, -0.01736886, -0.04944747],\n",
       "         [-0.08070382, -0.01591374,  0.0715838 ]],\n",
       " \n",
       "        [[ 0.0287272 ,  0.07588535,  0.00762205],\n",
       "         [ 0.06632682, -0.0179162 , -0.01971204],\n",
       "         [ 0.0155274 ,  0.04668438, -0.03311944]],\n",
       " \n",
       "        [[ 0.01634165,  0.02253569, -0.00862191],\n",
       "         [ 0.06806576, -0.05652225,  0.02439267],\n",
       "         [ 0.0680964 ,  0.04550445, -0.00315274]],\n",
       " \n",
       "        [[-0.07730609, -0.00361726, -0.08276495],\n",
       "         [-0.01279945,  0.02438565,  0.0361049 ],\n",
       "         [ 0.03595972,  0.01055326, -0.08144593]],\n",
       " \n",
       "        [[-0.04069035, -0.05946675, -0.07663005],\n",
       "         [-0.07810228, -0.01193228,  0.0047563 ],\n",
       "         [ 0.04398773, -0.07667114,  0.02273372]],\n",
       " \n",
       "        [[ 0.0159686 , -0.06395142,  0.01507646],\n",
       "         [-0.06008461, -0.02509086, -0.08045964],\n",
       "         [ 0.0538734 ,  0.06851155, -0.00136083]],\n",
       " \n",
       "        [[ 0.03052695, -0.02116972,  0.06462723],\n",
       "         [-0.00687437, -0.03048734, -0.03044882],\n",
       "         [ 0.04592777, -0.06351546, -0.03918933]],\n",
       " \n",
       "        [[-0.0152348 ,  0.03260847,  0.04935494],\n",
       "         [-0.03896   , -0.04607279, -0.00916412],\n",
       "         [-0.02039625, -0.00658001, -0.07146513]],\n",
       " \n",
       "        [[-0.0151264 , -0.02299461, -0.03865296],\n",
       "         [-0.02763632,  0.02141622,  0.03442569],\n",
       "         [-0.07540961,  0.07060213, -0.07808338]],\n",
       " \n",
       "        [[ 0.01398807,  0.06782772, -0.0240556 ],\n",
       "         [-0.00798895,  0.07053503,  0.07384893],\n",
       "         [ 0.07430238,  0.02507087,  0.05842688]],\n",
       " \n",
       "        [[-0.03733259, -0.06146992,  0.07170504],\n",
       "         [-0.02225997, -0.0460524 ,  0.00885219],\n",
       "         [ 0.03467456,  0.00725646, -0.04792801]],\n",
       " \n",
       "        [[-0.05832284, -0.05334856, -0.08031054],\n",
       "         [ 0.01983646,  0.03186082,  0.00768161],\n",
       "         [ 0.06737999,  0.03377574,  0.06286911]],\n",
       " \n",
       "        [[ 0.00587876, -0.0103064 ,  0.02965816],\n",
       "         [-0.03268447,  0.0107607 ,  0.00442919],\n",
       "         [-0.02785325,  0.03479845,  0.05790689]],\n",
       " \n",
       "        [[ 0.05956704,  0.08216644,  0.0381769 ],\n",
       "         [ 0.00995541, -0.05879571,  0.02465803],\n",
       "         [ 0.05059232, -0.00079392, -0.00326029]],\n",
       " \n",
       "        [[-0.07130276,  0.00904232,  0.00162138],\n",
       "         [ 0.07443514, -0.0085599 ,  0.03336222],\n",
       "         [ 0.0500945 ,  0.0368841 ,  0.08005407]],\n",
       " \n",
       "        [[-0.03842476, -0.03333732, -0.0672256 ],\n",
       "         [ 0.03904638, -0.0357237 ,  0.06666306],\n",
       "         [ 0.02776148,  0.05547717, -0.00158681]]], dtype=float32), bias=-0.0004146678, layer=1, neuron_number=30, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.06870846, -0.02335355,  0.07688865],\n",
       "         [-0.05405191, -0.05298047, -0.0584386 ],\n",
       "         [ 0.07675221, -0.06430099, -0.08184589]],\n",
       " \n",
       "        [[-0.01191967,  0.02218289,  0.07899187],\n",
       "         [-0.06080629, -0.00445542,  0.04662922],\n",
       "         [ 0.03391901,  0.04518653,  0.07896731]],\n",
       " \n",
       "        [[-0.02634742,  0.01629908, -0.05548754],\n",
       "         [-0.02865413, -0.07155716, -0.03603948],\n",
       "         [ 0.03394781,  0.03459138,  0.01960785]],\n",
       " \n",
       "        [[ 0.00047257, -0.03264087,  0.04387368],\n",
       "         [-0.02973473,  0.02821674, -0.04231917],\n",
       "         [-0.05937709, -0.01763506, -0.07018507]],\n",
       " \n",
       "        [[-0.07042235,  0.01867176,  0.00504658],\n",
       "         [-0.06450525,  0.05977832, -0.06901421],\n",
       "         [ 0.01956213,  0.01683987, -0.05035908]],\n",
       " \n",
       "        [[-0.02198285,  0.07789432, -0.0070722 ],\n",
       "         [-0.08268081,  0.01834935, -0.04250123],\n",
       "         [-0.01331492,  0.0192396 , -0.02932009]],\n",
       " \n",
       "        [[-0.07392114,  0.02028798, -0.03255079],\n",
       "         [ 0.00291658,  0.01771633, -0.07150095],\n",
       "         [ 0.06256126, -0.06414977, -0.04804072]],\n",
       " \n",
       "        [[ 0.01967793,  0.02490888, -0.02962582],\n",
       "         [-0.08029442, -0.0641368 , -0.07753682],\n",
       "         [ 0.00443913,  0.04240036, -0.08047895]],\n",
       " \n",
       "        [[ 0.02072131, -0.04212305,  0.08010794],\n",
       "         [ 0.00711053,  0.0492124 , -0.02202312],\n",
       "         [-0.0526694 , -0.01068825, -0.07954359]],\n",
       " \n",
       "        [[-0.03076631,  0.00922854,  0.06035564],\n",
       "         [ 0.05903749,  0.00779778, -0.0449335 ],\n",
       "         [ 0.00337146,  0.00356349, -0.06436158]],\n",
       " \n",
       "        [[ 0.00099973, -0.01018468,  0.04039574],\n",
       "         [-0.06064665,  0.00913749, -0.03227665],\n",
       "         [ 0.01013569,  0.06174633,  0.06033316]],\n",
       " \n",
       "        [[-0.00310643, -0.0208484 , -0.05646446],\n",
       "         [-0.05172971,  0.07814907, -0.03031024],\n",
       "         [ 0.03515691,  0.05497422, -0.06768031]],\n",
       " \n",
       "        [[ 0.07660677,  0.00572789, -0.07325541],\n",
       "         [ 0.07124905,  0.00266702,  0.05375328],\n",
       "         [ 0.03279572,  0.04318782, -0.08078299]],\n",
       " \n",
       "        [[-0.05971427, -0.04703244, -0.03801291],\n",
       "         [ 0.04538921, -0.05622911,  0.07811537],\n",
       "         [-0.05926901, -0.01931055,  0.0328686 ]],\n",
       " \n",
       "        [[ 0.01799337, -0.02039936, -0.04680739],\n",
       "         [ 0.06074728, -0.03429932, -0.0744845 ],\n",
       "         [ 0.00429185, -0.06584843, -0.02957712]],\n",
       " \n",
       "        [[-0.00885191, -0.07749514,  0.02150706],\n",
       "         [ 0.04411202, -0.04149766,  0.06956836],\n",
       "         [-0.03387013,  0.07541787, -0.05471571]],\n",
       " \n",
       "        [[-0.05921019, -0.08261567,  0.00443543],\n",
       "         [-0.04127524, -0.02846226, -0.0787711 ],\n",
       "         [ 0.03436121,  0.05487991,  0.01397193]],\n",
       " \n",
       "        [[ 0.01642449, -0.0474891 ,  0.03659574],\n",
       "         [-0.03505606,  0.02632262,  0.02199095],\n",
       "         [ 0.07138038,  0.05297764, -0.01340876]],\n",
       " \n",
       "        [[ 0.06732383,  0.05781958, -0.05722917],\n",
       "         [-0.00337602, -0.0783825 ,  0.0368673 ],\n",
       "         [-0.00342358, -0.05315859, -0.07218835]],\n",
       " \n",
       "        [[ 0.0336198 ,  0.01069277,  0.06636091],\n",
       "         [ 0.02929362, -0.06027859, -0.02017135],\n",
       "         [-0.04279672, -0.01973473,  0.0749866 ]],\n",
       " \n",
       "        [[-0.02875822, -0.07198773,  0.00486969],\n",
       "         [ 0.04194034, -0.00561646,  0.02896643],\n",
       "         [-0.03247033, -0.06173775,  0.02283768]],\n",
       " \n",
       "        [[ 0.04116143, -0.06322643, -0.04538667],\n",
       "         [ 0.01427164, -0.01822705, -0.00174226],\n",
       "         [ 0.00602533,  0.07417089, -0.0011749 ]],\n",
       " \n",
       "        [[ 0.07085433, -0.07455028, -0.01995909],\n",
       "         [ 0.06719372,  0.01979793,  0.06868316],\n",
       "         [ 0.02144177,  0.01107087,  0.08102501]],\n",
       " \n",
       "        [[-0.03405306,  0.00952512,  0.07168272],\n",
       "         [-0.05251882, -0.06977829, -0.00718358],\n",
       "         [ 0.05714121, -0.05237648, -0.06316821]],\n",
       " \n",
       "        [[-0.00725467,  0.08253004, -0.02057743],\n",
       "         [-0.02821411,  0.01376294,  0.06891661],\n",
       "         [-0.02426843, -0.01980303, -0.0826524 ]],\n",
       " \n",
       "        [[-0.08321373,  0.0365781 ,  0.03648099],\n",
       "         [ 0.06548128, -0.00980583, -0.06642483],\n",
       "         [ 0.06148843, -0.04700047, -0.03379495]],\n",
       " \n",
       "        [[ 0.06915065, -0.01840732,  0.03384512],\n",
       "         [-0.06737804,  0.01321985, -0.07024984],\n",
       "         [ 0.07285179, -0.06136423, -0.04753057]],\n",
       " \n",
       "        [[ 0.06266444,  0.04945837,  0.06829257],\n",
       "         [-0.08181796,  0.00930655,  0.07590719],\n",
       "         [-0.02580609,  0.0383094 ,  0.02455601]],\n",
       " \n",
       "        [[ 0.00063152, -0.04761236, -0.05498077],\n",
       "         [ 0.02073917, -0.00405201,  0.08104613],\n",
       "         [ 0.02961993, -0.05424609,  0.05327587]],\n",
       " \n",
       "        [[-0.02433565,  0.00952869, -0.05427925],\n",
       "         [-0.0588544 ,  0.07420354,  0.03490898],\n",
       "         [-0.00759126,  0.03423823,  0.03626033]],\n",
       " \n",
       "        [[ 0.02155949, -0.05111113, -0.0729162 ],\n",
       "         [-0.07104329,  0.01226775, -0.00219208],\n",
       "         [ 0.06417329, -0.04855886,  0.033173  ]],\n",
       " \n",
       "        [[-0.0650147 ,  0.00944543, -0.08275007],\n",
       "         [ 0.03474089,  0.0654698 , -0.02454241],\n",
       "         [ 0.00354195,  0.05771976,  0.04244642]]], dtype=float32), bias=0.0012207183, layer=1, neuron_number=31, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.07528048, -0.01488071,  0.01385984],\n",
       "         [-0.06238994, -0.02676319,  0.013274  ],\n",
       "         [-0.06691976,  0.0063978 ,  0.08288047]],\n",
       " \n",
       "        [[-0.01775878,  0.03880732,  0.06652396],\n",
       "         [ 0.07876866,  0.0194721 ,  0.07094097],\n",
       "         [-0.02923325,  0.00495772, -0.04663987]],\n",
       " \n",
       "        [[-0.07490796,  0.00824849, -0.06446735],\n",
       "         [ 0.04935068, -0.06417863, -0.06697495],\n",
       "         [ 0.02118616, -0.00195324,  0.01140238]],\n",
       " \n",
       "        [[ 0.04010214,  0.04522024, -0.04866806],\n",
       "         [-0.06394318,  0.08233868,  0.03741432],\n",
       "         [ 0.02385528, -0.08199759,  0.07945955]],\n",
       " \n",
       "        [[-0.05074199, -0.07376913,  0.07089414],\n",
       "         [-0.00173655, -0.04291655, -0.03731407],\n",
       "         [-0.05655882,  0.03781696,  0.04057042]],\n",
       " \n",
       "        [[-0.03070392,  0.07307258, -0.00975586],\n",
       "         [ 0.06832385, -0.07283551, -0.01418137],\n",
       "         [-0.01008463, -0.02353896, -0.03440002]],\n",
       " \n",
       "        [[ 0.00297515, -0.00589438,  0.00333678],\n",
       "         [-0.02606872,  0.04220346, -0.03677752],\n",
       "         [ 0.01051822,  0.04441613,  0.05321442]],\n",
       " \n",
       "        [[ 0.07086676,  0.07300439,  0.06982294],\n",
       "         [ 0.03261102,  0.0123281 ,  0.01519347],\n",
       "         [ 0.01218524,  0.03249321,  0.05956393]],\n",
       " \n",
       "        [[ 0.06721613, -0.03537961, -0.04745333],\n",
       "         [-0.0654864 ,  0.00783295, -0.05100614],\n",
       "         [ 0.0218391 , -0.08235443,  0.06947736]],\n",
       " \n",
       "        [[-0.06802101,  0.07650851,  0.05265319],\n",
       "         [ 0.03700507, -0.01691268, -0.07437334],\n",
       "         [ 0.08350056, -0.03624912,  0.0557466 ]],\n",
       " \n",
       "        [[-0.03274021, -0.06410584, -0.04610164],\n",
       "         [ 0.04055999, -0.02973514,  0.01127454],\n",
       "         [-0.00276619,  0.02144181, -0.03832173]],\n",
       " \n",
       "        [[ 0.03887414, -0.06685339, -0.02592946],\n",
       "         [ 0.08114698, -0.060508  ,  0.0154218 ],\n",
       "         [ 0.05039098,  0.03781946, -0.06792186]],\n",
       " \n",
       "        [[ 0.07586773, -0.01342718,  0.06571645],\n",
       "         [ 0.03389   ,  0.00304955, -0.0589514 ],\n",
       "         [-0.08076608,  0.05115169, -0.02360814]],\n",
       " \n",
       "        [[ 0.00814722,  0.06521886, -0.00263927],\n",
       "         [-0.07738776,  0.04777363,  0.07728411],\n",
       "         [-0.05832421,  0.05620782, -0.07882675]],\n",
       " \n",
       "        [[-0.04515604,  0.00694104, -0.0152473 ],\n",
       "         [ 0.0056392 ,  0.05854253,  0.02517087],\n",
       "         [-0.01106712,  0.03680765, -0.07951789]],\n",
       " \n",
       "        [[ 0.02001085,  0.07123922,  0.0092148 ],\n",
       "         [ 0.04340403,  0.05732974, -0.01942517],\n",
       "         [ 0.00632846, -0.04149335,  0.01201407]],\n",
       " \n",
       "        [[-0.02315   ,  0.05076457,  0.03053868],\n",
       "         [ 0.01621417, -0.0520882 , -0.07813377],\n",
       "         [ 0.00372429,  0.06982357,  0.06974586]],\n",
       " \n",
       "        [[ 0.05054678,  0.00265021,  0.03831719],\n",
       "         [-0.06231058,  0.07936148, -0.00664192],\n",
       "         [ 0.02107637, -0.03985604, -0.06224413]],\n",
       " \n",
       "        [[ 0.01678855,  0.00893989,  0.0011507 ],\n",
       "         [ 0.04540844, -0.05273519, -0.00322723],\n",
       "         [-0.00027191, -0.01843986, -0.05203784]],\n",
       " \n",
       "        [[ 0.01580697,  0.02069238, -0.06564311],\n",
       "         [-0.05011139, -0.03031204, -0.02803083],\n",
       "         [-0.06440999, -0.07654668,  0.04817796]],\n",
       " \n",
       "        [[-0.03101833,  0.04026189, -0.01656275],\n",
       "         [ 0.00125559, -0.07319096, -0.0287739 ],\n",
       "         [-0.06469299,  0.00180841,  0.00188836]],\n",
       " \n",
       "        [[-0.01798143, -0.01839399,  0.06011363],\n",
       "         [-0.07261803,  0.01103292,  0.07101735],\n",
       "         [ 0.06201826, -0.07109465, -0.04058511]],\n",
       " \n",
       "        [[-0.08060086,  0.05322052,  0.05592639],\n",
       "         [-0.04588145, -0.03208746, -0.08045004],\n",
       "         [ 0.0148099 ,  0.00577619, -0.0586041 ]],\n",
       " \n",
       "        [[ 0.00192146,  0.01877469,  0.01551294],\n",
       "         [-0.04027456, -0.01103215, -0.02563958],\n",
       "         [ 0.05719887, -0.00773015,  0.05439501]],\n",
       " \n",
       "        [[ 0.0773363 , -0.01565376, -0.03954677],\n",
       "         [ 0.00536064,  0.07090828, -0.02267986],\n",
       "         [-0.0058181 , -0.02034159, -0.04561892]],\n",
       " \n",
       "        [[-0.06549083, -0.040142  ,  0.07384545],\n",
       "         [-0.05620654, -0.02424188, -0.06772085],\n",
       "         [-0.06112143, -0.07110611,  0.00910325]],\n",
       " \n",
       "        [[-0.04873501,  0.06164898,  0.04602426],\n",
       "         [-0.05516263,  0.0669848 ,  0.02308617],\n",
       "         [-0.04234871,  0.08167587, -0.005227  ]],\n",
       " \n",
       "        [[-0.07395732, -0.03620759, -0.05284945],\n",
       "         [-0.0376339 ,  0.04178899,  0.00523691],\n",
       "         [-0.00457214, -0.07869107,  0.01475834]],\n",
       " \n",
       "        [[-0.01437569,  0.01960366,  0.05409748],\n",
       "         [-0.08004526, -0.05980498,  0.06264389],\n",
       "         [ 0.0536492 , -0.00875537,  0.01618363]],\n",
       " \n",
       "        [[-0.03675587,  0.05235945,  0.03147958],\n",
       "         [ 0.04192227,  0.01890402,  0.04946884],\n",
       "         [ 0.00930675,  0.0300796 , -0.06141523]],\n",
       " \n",
       "        [[-0.05365854, -0.05378671,  0.03801853],\n",
       "         [-0.06702127,  0.03141584, -0.07414071],\n",
       "         [ 0.0418406 , -0.07025213,  0.07207644]],\n",
       " \n",
       "        [[-0.04343094,  0.0645259 , -0.04432259],\n",
       "         [-0.05368514,  0.03036781, -0.02276159],\n",
       "         [ 0.05091556,  0.05677184, -0.05726559]]], dtype=float32), bias=6.9990812e-05, layer=1, neuron_number=32, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.05728843, -0.0419036 ,  0.05015749],\n",
       "         [ 0.04744085,  0.03632535,  0.06026909],\n",
       "         [ 0.06236322, -0.0062454 , -0.00676155]],\n",
       " \n",
       "        [[ 0.06258573, -0.05499881,  0.07007234],\n",
       "         [ 0.02973888, -0.07688326,  0.004867  ],\n",
       "         [ 0.07843429,  0.05962529, -0.00201265]],\n",
       " \n",
       "        [[-0.04100533,  0.0428195 , -0.03759983],\n",
       "         [-0.03126005,  0.01833983, -0.05362825],\n",
       "         [ 0.06892065, -0.04168822,  0.00792647]],\n",
       " \n",
       "        [[ 0.07184505,  0.01068175,  0.06138707],\n",
       "         [ 0.00727979, -0.01638524,  0.06966522],\n",
       "         [ 0.06221389, -0.05148939,  0.07828267]],\n",
       " \n",
       "        [[ 0.07437411, -0.0807046 , -0.06802484],\n",
       "         [ 0.01041621,  0.04903428,  0.05555505],\n",
       "         [-0.02356906, -0.06163984, -0.04343388]],\n",
       " \n",
       "        [[-0.02652808,  0.03626033,  0.01865181],\n",
       "         [ 0.04664832, -0.06465354,  0.0802982 ],\n",
       "         [-0.0361245 , -0.06166242,  0.04433118]],\n",
       " \n",
       "        [[ 0.05560019, -0.0770247 , -0.01578477],\n",
       "         [ 0.07591848, -0.01057241,  0.04340041],\n",
       "         [-0.0275304 , -0.02733665,  0.03607265]],\n",
       " \n",
       "        [[ 0.0807545 , -0.0252367 ,  0.07382768],\n",
       "         [-0.0263275 , -0.0782084 , -0.06136166],\n",
       "         [ 0.02351998,  0.02522612,  0.07325367]],\n",
       " \n",
       "        [[ 0.04051919,  0.05888483, -0.03707739],\n",
       "         [ 0.07440765,  0.07412854, -0.02837452],\n",
       "         [ 0.04879985, -0.03557859, -0.03597179]],\n",
       " \n",
       "        [[-0.0090693 , -0.0065754 ,  0.07702009],\n",
       "         [ 0.07293507, -0.01728652,  0.00579832],\n",
       "         [-0.04136883, -0.05141374, -0.06480636]],\n",
       " \n",
       "        [[ 0.04128351, -0.05354465,  0.05704694],\n",
       "         [ 0.05075399,  0.07989936,  0.02439273],\n",
       "         [-0.03290014,  0.06966031, -0.0751014 ]],\n",
       " \n",
       "        [[ 0.0293825 ,  0.00477128,  0.0187121 ],\n",
       "         [ 0.05125785, -0.04111913,  0.02219556],\n",
       "         [ 0.00019249,  0.00844928, -0.02955606]],\n",
       " \n",
       "        [[ 0.05833873,  0.04227914,  0.06164902],\n",
       "         [ 0.04989739,  0.0521849 , -0.01714369],\n",
       "         [ 0.03520364,  0.04219671,  0.02734639]],\n",
       " \n",
       "        [[-0.06449691,  0.08065092,  0.05050182],\n",
       "         [-0.07237599,  0.02542103, -0.00732444],\n",
       "         [ 0.0693581 ,  0.00986112, -0.03279953]],\n",
       " \n",
       "        [[ 0.06249087,  0.02344067,  0.06336315],\n",
       "         [ 0.0069664 , -0.07060993, -0.02187908],\n",
       "         [-0.00219875, -0.02061016,  0.07618185]],\n",
       " \n",
       "        [[ 0.07790721,  0.01367411, -0.00626071],\n",
       "         [-0.07288247, -0.02061677, -0.00655872],\n",
       "         [-0.04033255,  0.05221189, -0.05620312]],\n",
       " \n",
       "        [[-0.07764937, -0.00877594, -0.06656885],\n",
       "         [-0.03310322, -0.05743108, -0.04340595],\n",
       "         [-0.04320588,  0.01953393,  0.05400106]],\n",
       " \n",
       "        [[-0.03554751,  0.03865235,  0.07659858],\n",
       "         [-0.02146988,  0.05570707,  0.05666224],\n",
       "         [ 0.06214117,  0.06565814, -0.00099709]],\n",
       " \n",
       "        [[ 0.03520373,  0.01909915,  0.04503728],\n",
       "         [ 0.04813332,  0.00786221,  0.02070941],\n",
       "         [-0.0799403 , -0.00476774, -0.00570431]],\n",
       " \n",
       "        [[ 0.03812049, -0.05847449,  0.04117319],\n",
       "         [ 0.00066201, -0.05241049, -0.01297252],\n",
       "         [-0.01028617, -0.0223671 , -0.07321953]],\n",
       " \n",
       "        [[ 0.04222127,  0.03911803,  0.06688641],\n",
       "         [ 0.00466   ,  0.06895967, -0.03064184],\n",
       "         [-0.05623517,  0.08261817,  0.05729868]],\n",
       " \n",
       "        [[-0.04413745,  0.08283801,  0.05822266],\n",
       "         [ 0.01383302,  0.00639361, -0.01296837],\n",
       "         [-0.00938022,  0.04104956, -0.06927578]],\n",
       " \n",
       "        [[-0.03833566, -0.05654991,  0.06649213],\n",
       "         [ 0.00615391, -0.05275617, -0.03085458],\n",
       "         [ 0.08098407, -0.0187144 ,  0.0497799 ]],\n",
       " \n",
       "        [[-0.06473821, -0.02121163,  0.00903926],\n",
       "         [ 0.05047278,  0.00610348, -0.03491051],\n",
       "         [ 0.06510156,  0.05333932,  0.01752029]],\n",
       " \n",
       "        [[-0.01030574, -0.05238754, -0.04054627],\n",
       "         [-0.01663327,  0.05890597, -0.01041216],\n",
       "         [-0.02821773, -0.08171915,  0.06076635]],\n",
       " \n",
       "        [[-0.01858071,  0.01853797, -0.06088483],\n",
       "         [-0.03141855,  0.04120971, -0.0219065 ],\n",
       "         [-0.00226803,  0.06331244,  0.05938984]],\n",
       " \n",
       "        [[-0.06498966, -0.05399088, -0.01535003],\n",
       "         [ 0.02220664, -0.05372186,  0.03913182],\n",
       "         [-0.07278711,  0.08241576,  0.02600686]],\n",
       " \n",
       "        [[-0.0506888 ,  0.03279027,  0.03267713],\n",
       "         [-0.07360122, -0.04849297,  0.00141924],\n",
       "         [ 0.06400609,  0.0082646 ,  0.07786559]],\n",
       " \n",
       "        [[-0.05418977, -0.00037594,  0.08026835],\n",
       "         [ 0.04056604, -0.0588835 , -0.03854186],\n",
       "         [ 0.03560918,  0.07666627, -0.07466657]],\n",
       " \n",
       "        [[ 0.05153728,  0.04321165, -0.05112464],\n",
       "         [ 0.00892907,  0.07256202, -0.02497096],\n",
       "         [ 0.0242679 , -0.06226076, -0.04550626]],\n",
       " \n",
       "        [[ 0.00791587,  0.00587117, -0.05338211],\n",
       "         [ 0.02509844,  0.04319535,  0.00588577],\n",
       "         [-0.0099956 ,  0.04902269, -0.04402902]],\n",
       " \n",
       "        [[ 0.06331426, -0.0362809 , -0.05858947],\n",
       "         [-0.02984927, -0.06001987, -0.0170475 ],\n",
       "         [-0.01943861, -0.05864443,  0.05532852]]], dtype=float32), bias=0.00021619449, layer=1, neuron_number=33, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.00527052, -0.04528871, -0.07503862],\n",
       "         [-0.04164863, -0.02697214,  0.04951569],\n",
       "         [ 0.07802258,  0.02760246,  0.04961289]],\n",
       " \n",
       "        [[ 0.05424698,  0.00518063, -0.04860521],\n",
       "         [-0.01138702,  0.07986408, -0.04082471],\n",
       "         [ 0.03637853, -0.00929678, -0.04668715]],\n",
       " \n",
       "        [[-0.00719366, -0.04432372,  0.00362373],\n",
       "         [-0.07602882, -0.01103237,  0.02281762],\n",
       "         [ 0.01099273, -0.07806294,  0.01866488]],\n",
       " \n",
       "        [[-0.07790551,  0.00444023, -0.04280784],\n",
       "         [ 0.03171379,  0.02042539,  0.04336099],\n",
       "         [ 0.03821933,  0.07848408,  0.07185237]],\n",
       " \n",
       "        [[ 0.05078572, -0.07497865, -0.07928307],\n",
       "         [-0.02793216, -0.07150552,  0.07781284],\n",
       "         [ 0.06543447, -0.01950552, -0.07136791]],\n",
       " \n",
       "        [[ 0.01790384, -0.01334599, -0.04712093],\n",
       "         [-0.038887  , -0.04803605, -0.01793606],\n",
       "         [-0.04347328,  0.04937165,  0.04153899]],\n",
       " \n",
       "        [[ 0.0600574 ,  0.06348603,  0.00705036],\n",
       "         [ 0.00520767, -0.03727265,  0.07810168],\n",
       "         [-0.02792514,  0.00528824, -0.02234343]],\n",
       " \n",
       "        [[-0.04219421,  0.06146547, -0.04690529],\n",
       "         [-0.04718444, -0.00297765, -0.05146113],\n",
       "         [ 0.03371258, -0.074493  , -0.02843857]],\n",
       " \n",
       "        [[-0.04374281,  0.04435568, -0.03352241],\n",
       "         [-0.0021896 ,  0.03970047, -0.06442662],\n",
       "         [ 0.07770996,  0.06593996, -0.04858854]],\n",
       " \n",
       "        [[ 0.07317834,  0.06355637,  0.03755416],\n",
       "         [ 0.06436951,  0.07356633,  0.03764694],\n",
       "         [ 0.07966271,  0.0601377 ,  0.03333156]],\n",
       " \n",
       "        [[-0.01300573, -0.07324645,  0.02307803],\n",
       "         [-0.01838878, -0.06028365,  0.06332128],\n",
       "         [-0.08017063, -0.04752484,  0.07685659]],\n",
       " \n",
       "        [[ 0.08119994,  0.05780881, -0.0070522 ],\n",
       "         [ 0.06039181,  0.00627017, -0.02673829],\n",
       "         [ 0.07791412, -0.01700219, -0.06216044]],\n",
       " \n",
       "        [[-0.03350624,  0.04070489,  0.00293145],\n",
       "         [-0.08312404, -0.0367624 , -0.00323836],\n",
       "         [-0.04921668,  0.00898963, -0.00527407]],\n",
       " \n",
       "        [[ 0.04024324,  0.02283109,  0.01107453],\n",
       "         [ 0.02866852,  0.03651422, -0.01500222],\n",
       "         [ 0.07489656, -0.08194213, -0.07886826]],\n",
       " \n",
       "        [[-0.0041507 , -0.03896837, -0.02675956],\n",
       "         [-0.0356557 ,  0.0558719 ,  0.02569469],\n",
       "         [ 0.02082742, -0.02954504,  0.01235403]],\n",
       " \n",
       "        [[-0.0062463 , -0.03253717,  0.06498117],\n",
       "         [-0.00871077,  0.07794762, -0.07900742],\n",
       "         [-0.07929744,  0.02771194,  0.07021707]],\n",
       " \n",
       "        [[-0.05999482,  0.03943204, -0.00516171],\n",
       "         [ 0.01568232,  0.05248054,  0.06069916],\n",
       "         [-0.06815686,  0.00953821,  0.03188048]],\n",
       " \n",
       "        [[ 0.02205195, -0.08091965,  0.02362112],\n",
       "         [-0.00860901, -0.05813846, -0.04377778],\n",
       "         [ 0.05279637, -0.06765758, -0.02203635]],\n",
       " \n",
       "        [[ 0.02534129,  0.05943406,  0.00360785],\n",
       "         [ 0.02631313, -0.02698661, -0.08117574],\n",
       "         [ 0.07640456, -0.0765954 , -0.04864737]],\n",
       " \n",
       "        [[-0.03927983, -0.05584474,  0.04205624],\n",
       "         [ 0.01204999, -0.07312879,  0.02581004],\n",
       "         [-0.0121557 , -0.03125421,  0.07220914]],\n",
       " \n",
       "        [[-0.03247517, -0.02784412, -0.07257093],\n",
       "         [ 0.02573253, -0.06050593, -0.07373881],\n",
       "         [-0.07507466,  0.02519398, -0.03475553]],\n",
       " \n",
       "        [[-0.05894475,  0.08191225,  0.06360167],\n",
       "         [ 0.06949239,  0.07133915, -0.02571693],\n",
       "         [ 0.02580595, -0.05939534, -0.06543616]],\n",
       " \n",
       "        [[-0.06493123, -0.03257169, -0.07597595],\n",
       "         [-0.01571689,  0.01544641,  0.03633922],\n",
       "         [ 0.05183753,  0.06585903, -0.00880185]],\n",
       " \n",
       "        [[-0.00721523, -0.07601578,  0.05593976],\n",
       "         [-0.03645543, -0.02849159, -0.03964943],\n",
       "         [ 0.04952072,  0.04564429,  0.0022395 ]],\n",
       " \n",
       "        [[ 0.00378356, -0.03530287, -0.0139881 ],\n",
       "         [-0.07796656,  0.06403219,  0.01579866],\n",
       "         [ 0.01909916, -0.04103083, -0.05430953]],\n",
       " \n",
       "        [[-0.04671394,  0.04511317,  0.03565103],\n",
       "         [ 0.08310311, -0.06917822, -0.01181541],\n",
       "         [ 0.05765013, -0.04111354,  0.08154191]],\n",
       " \n",
       "        [[-0.05020846,  0.00223566, -0.0640383 ],\n",
       "         [ 0.07804585, -0.03439021, -0.05091222],\n",
       "         [ 0.04209317,  0.04055424, -0.07374068]],\n",
       " \n",
       "        [[-0.08079593, -0.05617512,  0.06697193],\n",
       "         [-0.08083426,  0.0078723 , -0.00390491],\n",
       "         [ 0.00373148, -0.05264607, -0.03929581]],\n",
       " \n",
       "        [[-0.00081884,  0.03503717,  0.06530816],\n",
       "         [-0.05804399, -0.02135431,  0.05157129],\n",
       "         [ 0.07094939, -0.04256282,  0.03734046]],\n",
       " \n",
       "        [[ 0.05536635, -0.02994295,  0.01096076],\n",
       "         [ 0.06826697,  0.05316323, -0.0681235 ],\n",
       "         [ 0.05257868, -0.01627295,  0.04127555]],\n",
       " \n",
       "        [[ 0.04988341,  0.00315556, -0.01169969],\n",
       "         [ 0.0427913 ,  0.04905493,  0.08418012],\n",
       "         [ 0.05380641, -0.02045123, -0.07545912]],\n",
       " \n",
       "        [[-0.03763857,  0.0621247 ,  0.08343223],\n",
       "         [-0.04918345, -0.07858475,  0.06314745],\n",
       "         [ 0.03771079, -0.05376519, -0.03697064]]], dtype=float32), bias=0.0011840316, layer=1, neuron_number=34, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  7.51600042e-02,   5.82082160e-02,   1.77457128e-02],\n",
       "         [  8.04878250e-02,  -3.80291380e-02,  -2.22421996e-02],\n",
       "         [  5.87136261e-02,   1.10680042e-02,  -6.85651228e-02]],\n",
       " \n",
       "        [[ -4.72692214e-03,   6.49138838e-02,  -3.74357328e-02],\n",
       "         [  4.13990989e-02,  -2.21744850e-02,   4.64675985e-02],\n",
       "         [  4.30092588e-02,   1.28800916e-02,  -2.36754175e-02]],\n",
       " \n",
       "        [[  3.20241824e-02,  -2.61108670e-02,  -7.96800330e-02],\n",
       "         [  1.38478465e-02,   5.83153404e-02,  -8.95243138e-03],\n",
       "         [  3.28336060e-02,   7.14885071e-02,  -2.19297241e-02]],\n",
       " \n",
       "        [[ -5.82319163e-02,  -1.16235847e-02,  -2.64827758e-02],\n",
       "         [ -5.05638635e-03,   2.30032187e-02,  -5.99915013e-02],\n",
       "         [ -3.96230333e-02,   7.53226206e-02,  -5.29005863e-02]],\n",
       " \n",
       "        [[  7.08227530e-02,  -3.64447869e-02,  -7.26530477e-02],\n",
       "         [  6.91831410e-02,   4.14923206e-02,  -8.17811936e-02],\n",
       "         [  4.87488620e-02,  -5.32489717e-02,  -4.15980481e-02]],\n",
       " \n",
       "        [[ -1.62987076e-02,   4.25261259e-03,  -4.31302339e-02],\n",
       "         [  2.55404580e-02,  -3.75686996e-02,   4.89512756e-02],\n",
       "         [  3.00575234e-02,   3.08760330e-02,   2.84023024e-02]],\n",
       " \n",
       "        [[  4.45387475e-02,  -6.44755810e-02,  -2.32614409e-02],\n",
       "         [ -5.76674044e-02,   5.27989417e-02,   4.64566983e-02],\n",
       "         [  3.55233476e-02,  -2.10998859e-02,  -3.34570333e-02]],\n",
       " \n",
       "        [[  5.51816262e-02,   1.97847150e-02,   5.46067506e-02],\n",
       "         [  6.32121414e-02,  -5.91963530e-02,   6.48321211e-02],\n",
       "         [  7.60046989e-02,  -3.54792327e-02,   3.37890163e-02]],\n",
       " \n",
       "        [[ -1.75189227e-02,  -3.88149694e-02,  -5.11340275e-02],\n",
       "         [  3.35024223e-02,  -1.64535884e-02,   7.62813538e-02],\n",
       "         [  3.11399028e-02,  -8.29044729e-02,  -6.22631386e-02]],\n",
       " \n",
       "        [[  6.77238107e-02,  -5.89223541e-02,  -7.42767975e-02],\n",
       "         [ -4.55003157e-02,   5.45333028e-02,  -5.33876531e-02],\n",
       "         [ -6.21729344e-02,   4.65097651e-02,   7.35436678e-02]],\n",
       " \n",
       "        [[  7.34324008e-02,   6.94960877e-02,  -6.82040304e-02],\n",
       "         [ -3.23095322e-02,  -1.36725102e-02,  -4.33375649e-02],\n",
       "         [  7.49541223e-02,  -3.63704525e-02,  -8.27149078e-02]],\n",
       " \n",
       "        [[  5.79592548e-02,   8.45987070e-03,   6.83269054e-02],\n",
       "         [ -5.88753037e-02,   7.29706045e-03,   3.18209901e-02],\n",
       "         [  5.22132814e-02,  -7.63791874e-02,   1.22432960e-02]],\n",
       " \n",
       "        [[ -2.05943864e-02,  -5.96684106e-02,  -2.62406860e-02],\n",
       "         [ -7.14232028e-03,   3.07817701e-02,   1.51755083e-02],\n",
       "         [  1.33533413e-02,  -2.29649860e-02,   3.58737679e-03]],\n",
       " \n",
       "        [[  5.39854392e-02,  -3.01894620e-02,  -2.82258857e-02],\n",
       "         [ -6.04139976e-02,   8.12204704e-02,  -2.75866725e-02],\n",
       "         [  4.68589999e-02,   4.53064498e-03,  -6.57543447e-03]],\n",
       " \n",
       "        [[  7.04266652e-02,  -4.26871097e-03,   1.23273153e-02],\n",
       "         [ -8.21122229e-02,  -4.41944152e-02,  -3.96133773e-02],\n",
       "         [ -3.17923613e-02,  -7.65244737e-02,  -7.91674554e-02]],\n",
       " \n",
       "        [[ -2.70988792e-02,  -3.95606905e-02,   3.91857065e-02],\n",
       "         [  3.53366919e-02,   9.43044852e-03,   5.47575252e-03],\n",
       "         [ -2.31474359e-02,  -6.26159757e-02,   7.00155497e-02]],\n",
       " \n",
       "        [[ -3.32006961e-02,   6.29893551e-03,  -5.19954860e-02],\n",
       "         [ -2.45742518e-02,   1.71061829e-02,   2.68806238e-02],\n",
       "         [ -6.25366047e-02,   1.17791966e-02,  -6.88605085e-02]],\n",
       " \n",
       "        [[  5.25971018e-02,   1.49337659e-02,  -8.45746696e-02],\n",
       "         [  3.76841575e-02,   2.34344751e-02,  -1.97549793e-03],\n",
       "         [  6.37273490e-02,   3.70718241e-02,  -4.94922400e-02]],\n",
       " \n",
       "        [[ -1.93399936e-02,  -1.37913190e-02,   4.86454777e-02],\n",
       "         [  1.03508933e-02,  -2.73455400e-02,  -5.64138032e-02],\n",
       "         [  5.55253923e-02,   2.71057300e-02,   5.53524829e-02]],\n",
       " \n",
       "        [[ -3.85846943e-02,  -3.19739990e-02,  -9.75072104e-03],\n",
       "         [  1.33475126e-03,  -2.56195664e-02,   7.58387148e-02],\n",
       "         [  4.26028781e-02,  -8.04603249e-02,   2.83037163e-02]],\n",
       " \n",
       "        [[ -1.56655069e-02,   6.09691665e-02,   6.53845444e-02],\n",
       "         [ -1.53915184e-02,  -7.65003338e-02,   3.97808440e-02],\n",
       "         [ -2.24103294e-02,   5.75339086e-02,   3.00275572e-02]],\n",
       " \n",
       "        [[ -1.03364075e-02,   4.32062186e-02,  -5.06864116e-02],\n",
       "         [  2.68008653e-02,   8.08617324e-02,   3.32874767e-02],\n",
       "         [  6.41086325e-02,  -5.02006225e-02,   3.47859003e-02]],\n",
       " \n",
       "        [[ -1.61310274e-03,  -7.65125230e-02,   8.94463435e-03],\n",
       "         [ -8.32810774e-02,  -2.44844286e-03,   1.27134117e-05],\n",
       "         [  4.23405729e-02,   5.13528567e-03,   5.48750721e-02]],\n",
       " \n",
       "        [[  7.36320913e-02,   4.47017923e-02,  -5.11940010e-02],\n",
       "         [ -6.12049699e-02,   7.51376152e-03,   4.46542725e-02],\n",
       "         [ -4.82427105e-02,   3.61844674e-02,   7.42525235e-03]],\n",
       " \n",
       "        [[ -3.19424532e-02,   6.98969290e-02,   2.88557317e-02],\n",
       "         [  4.43661213e-02,  -4.03745957e-02,  -6.11314252e-02],\n",
       "         [  5.25091179e-02,   8.14094841e-02,  -3.31221409e-02]],\n",
       " \n",
       "        [[ -3.16932052e-02,  -5.27696218e-03,  -5.68334647e-02],\n",
       "         [  5.12891635e-02,   3.77888151e-04,   2.15366576e-03],\n",
       "         [  6.58290088e-02,   7.53128678e-02,   6.01412058e-02]],\n",
       " \n",
       "        [[ -5.26370443e-02,  -6.90078512e-02,  -4.80221137e-02],\n",
       "         [ -3.61370668e-02,   1.01838261e-02,  -1.20114749e-02],\n",
       "         [  3.85379083e-02,   5.91641366e-02,   5.29032294e-03]],\n",
       " \n",
       "        [[  4.21958975e-02,  -7.43900165e-02,  -3.03473193e-02],\n",
       "         [ -5.08784205e-02,   7.66521841e-02,   3.00205313e-02],\n",
       "         [  7.13836169e-03,   8.05562958e-02,   2.46525686e-02]],\n",
       " \n",
       "        [[  6.24633767e-02,   6.79336116e-02,   3.95893566e-02],\n",
       "         [ -6.18735142e-02,   2.26345100e-02,   5.33986278e-02],\n",
       "         [  5.54627515e-02,   5.47226742e-02,  -2.45554186e-02]],\n",
       " \n",
       "        [[ -2.72719897e-02,  -5.54252975e-02,  -5.07863536e-02],\n",
       "         [  2.26023737e-02,   3.47219557e-02,   5.34001999e-02],\n",
       "         [  5.53767011e-02,   6.37403652e-02,   2.32039597e-02]],\n",
       " \n",
       "        [[  3.18019208e-03,  -3.52851823e-02,   8.19479749e-02],\n",
       "         [  2.89563108e-02,  -3.14670987e-02,  -8.41890797e-02],\n",
       "         [  4.22599316e-02,   1.36998529e-02,  -6.73675619e-04]],\n",
       " \n",
       "        [[  7.04640746e-02,   2.47885846e-02,  -5.29161356e-02],\n",
       "         [  7.21547976e-02,  -4.02962603e-02,   3.44652124e-02],\n",
       "         [  5.67992255e-02,  -5.03805503e-02,  -6.94364496e-03]]], dtype=float32), bias=-0.00096921821, layer=1, neuron_number=35, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  2.16287915e-02,   8.02297294e-02,   7.79542467e-03],\n",
       "         [  3.45489942e-02,   8.32382403e-03,   3.51770706e-02],\n",
       "         [ -2.89972182e-02,   2.40281224e-02,   3.46901007e-02]],\n",
       " \n",
       "        [[ -4.03432101e-02,   6.34975657e-02,  -7.41707906e-02],\n",
       "         [ -9.85461939e-03,   6.58667907e-02,   2.58291997e-02],\n",
       "         [ -6.03972785e-02,   2.13504825e-02,  -2.49416996e-02]],\n",
       " \n",
       "        [[ -5.69171049e-02,   1.57389324e-02,   8.06175023e-02],\n",
       "         [  4.74551469e-02,  -4.93434109e-02,   4.11218666e-02],\n",
       "         [  5.00824004e-02,  -6.17760383e-02,  -5.61999530e-02]],\n",
       " \n",
       "        [[  7.92133585e-02,  -3.90557870e-02,   7.53198117e-02],\n",
       "         [  7.09359944e-02,  -7.59345815e-02,   4.47213985e-02],\n",
       "         [ -6.16327487e-02,  -2.48285905e-02,  -3.94285172e-02]],\n",
       " \n",
       "        [[  7.97398761e-02,   8.36817697e-02,   2.76273135e-02],\n",
       "         [  7.32382014e-02,  -1.65371317e-02,   1.67521108e-02],\n",
       "         [ -7.47692063e-02,  -6.75325990e-02,   5.22581153e-02]],\n",
       " \n",
       "        [[ -7.03329816e-02,  -6.08969145e-02,   7.53455535e-02],\n",
       "         [ -7.33048655e-03,  -3.83434258e-02,  -4.92999852e-02],\n",
       "         [  1.31067364e-02,  -2.31087692e-02,  -1.78378206e-02]],\n",
       " \n",
       "        [[ -2.24577803e-02,  -4.25668806e-02,   4.97166812e-02],\n",
       "         [  2.80904956e-02,   3.06643862e-02,  -4.69641620e-03],\n",
       "         [  5.35161234e-02,  -4.58975732e-02,  -5.03978655e-02]],\n",
       " \n",
       "        [[ -2.91152317e-02,   5.93671538e-02,  -7.77761918e-03],\n",
       "         [ -3.56213041e-02,   4.58048433e-02,  -3.13155726e-02],\n",
       "         [ -4.54211235e-02,   4.25275005e-02,   2.92568803e-02]],\n",
       " \n",
       "        [[ -5.82067259e-02,   7.62604475e-02,  -7.22815841e-02],\n",
       "         [ -3.90279554e-02,   2.86150202e-02,  -2.11156979e-02],\n",
       "         [ -6.15214705e-02,  -5.17526865e-02,   6.49103224e-02]],\n",
       " \n",
       "        [[ -7.29935318e-02,   1.58851296e-02,   4.98798452e-02],\n",
       "         [  7.29152039e-02,   3.05956341e-02,   5.45652956e-02],\n",
       "         [ -8.88737850e-03,  -6.76264688e-02,   3.30724195e-02]],\n",
       " \n",
       "        [[  3.17980498e-02,   1.74584631e-02,   5.89739457e-02],\n",
       "         [  1.85281280e-02,   1.21414755e-02,   4.49340418e-02],\n",
       "         [ -7.58311152e-02,   4.28454690e-02,  -7.11232796e-02]],\n",
       " \n",
       "        [[ -1.90574471e-02,  -3.65334153e-02,  -5.86729869e-02],\n",
       "         [ -3.08819041e-02,  -6.47968845e-04,  -2.59424075e-02],\n",
       "         [  7.68658072e-02,  -1.47395511e-03,   9.89865791e-03]],\n",
       " \n",
       "        [[ -2.37295916e-03,  -3.50413360e-02,  -4.54691797e-02],\n",
       "         [ -4.43847291e-02,   3.64149138e-02,   6.12438098e-02],\n",
       "         [  6.76673353e-02,   5.21100983e-02,   1.76614374e-02]],\n",
       " \n",
       "        [[  1.21999783e-02,  -3.45966183e-02,  -1.17641920e-02],\n",
       "         [ -7.20719919e-02,  -6.17615916e-02,   1.56287402e-02],\n",
       "         [  2.30241697e-02,   1.91611219e-02,  -3.36377956e-02]],\n",
       " \n",
       "        [[  1.26068834e-02,  -5.93123063e-02,   4.43037227e-02],\n",
       "         [ -4.97163124e-02,   5.83206676e-02,  -7.64381588e-02],\n",
       "         [  5.44832908e-02,  -4.04986665e-02,   4.95864488e-02]],\n",
       " \n",
       "        [[  2.19019093e-02,   7.75355175e-02,   8.15004557e-02],\n",
       "         [  4.38359492e-02,   5.56783825e-02,  -6.19719736e-02],\n",
       "         [ -6.32467940e-02,  -3.92592419e-03,   6.15509506e-03]],\n",
       " \n",
       "        [[  3.59821208e-02,   8.21272954e-02,  -7.47269839e-02],\n",
       "         [ -7.63980001e-02,  -4.56615649e-02,   1.02554783e-02],\n",
       "         [ -6.02369346e-02,   2.01400854e-02,  -5.82271405e-02]],\n",
       " \n",
       "        [[  2.18126122e-02,  -3.97675410e-02,   4.27909046e-02],\n",
       "         [  4.50868718e-02,   2.93752067e-02,  -6.71730330e-03],\n",
       "         [ -8.17447901e-02,   1.16184698e-02,  -3.91953567e-04]],\n",
       " \n",
       "        [[  2.08275323e-03,  -5.41048422e-02,  -8.05022493e-02],\n",
       "         [  3.54934037e-02,   8.40684623e-02,   2.84729190e-02],\n",
       "         [  6.09630123e-02,  -3.79752666e-02,   3.68246017e-03]],\n",
       " \n",
       "        [[ -2.57296562e-02,  -4.98874001e-02,  -1.22445635e-02],\n",
       "         [  2.41220789e-03,  -3.15375850e-02,  -3.22287600e-03],\n",
       "         [ -4.53349315e-02,   2.26318464e-02,  -5.82070015e-02]],\n",
       " \n",
       "        [[ -1.85623970e-02,  -6.76004961e-02,   1.95393879e-02],\n",
       "         [ -4.37583663e-02,  -8.33196682e-05,  -3.14268097e-02],\n",
       "         [ -3.85332783e-03,  -4.22089137e-02,  -4.73388918e-02]],\n",
       " \n",
       "        [[  4.41965461e-02,   4.87734042e-02,   8.24503303e-02],\n",
       "         [ -7.47405663e-02,  -2.92424560e-02,   1.91203021e-02],\n",
       "         [  3.35038640e-02,  -4.39973921e-02,   2.42715161e-02]],\n",
       " \n",
       "        [[ -7.22621195e-03,   2.93972753e-02,   7.62813818e-03],\n",
       "         [ -2.13518683e-02,  -3.26230861e-02,   2.66416948e-02],\n",
       "         [  5.30886278e-02,   2.99508087e-02,   2.77215056e-02]],\n",
       " \n",
       "        [[  3.74033935e-02,  -3.16167772e-02,   8.18013772e-02],\n",
       "         [ -6.06976338e-02,   8.19363743e-02,  -1.48596633e-02],\n",
       "         [  4.65917177e-02,  -5.70524037e-02,   1.08980536e-02]],\n",
       " \n",
       "        [[  7.49634951e-02,  -4.58242036e-02,  -6.87193424e-02],\n",
       "         [  6.09015040e-02,   6.95757791e-02,   7.17150196e-02],\n",
       "         [ -3.73654738e-02,  -7.26326555e-02,  -1.91055797e-02]],\n",
       " \n",
       "        [[  5.15158474e-02,  -1.86128020e-02,   3.48469429e-02],\n",
       "         [  4.01195511e-02,  -4.21293750e-02,   4.43142615e-02],\n",
       "         [  8.52193236e-02,   7.20269084e-02,   1.32039702e-02]],\n",
       " \n",
       "        [[  2.10445374e-02,   5.49364910e-02,   2.88568325e-02],\n",
       "         [  7.42744878e-02,  -3.09118181e-02,  -6.15481175e-02],\n",
       "         [  6.50408957e-03,   2.94423867e-02,   8.31886679e-02]],\n",
       " \n",
       "        [[  5.86873665e-02,  -3.37262563e-02,  -9.79615375e-03],\n",
       "         [ -1.43702691e-02,  -2.57465374e-02,   3.27598415e-02],\n",
       "         [  5.53811118e-02,  -2.40485556e-02,   7.68723488e-02]],\n",
       " \n",
       "        [[  1.43130561e-02,  -1.52780619e-02,  -2.34124828e-02],\n",
       "         [ -1.17427669e-02,   5.82983308e-02,   7.01073036e-02],\n",
       "         [  4.32191789e-02,   3.30942571e-02,  -1.48722474e-02]],\n",
       " \n",
       "        [[  5.40605374e-02,   7.07192570e-02,  -2.12413948e-02],\n",
       "         [ -7.75594562e-02,  -9.92212724e-03,  -6.10035583e-02],\n",
       "         [  6.56950772e-02,  -4.22086827e-02,  -1.74041968e-02]],\n",
       " \n",
       "        [[ -3.57614234e-02,   8.22076723e-02,   2.80944351e-02],\n",
       "         [  5.55564510e-03,  -4.71564243e-03,   2.90410649e-02],\n",
       "         [ -7.96164349e-02,  -6.30016178e-02,  -7.37502649e-02]],\n",
       " \n",
       "        [[  1.35429623e-02,  -7.20041022e-02,   3.62357721e-02],\n",
       "         [ -7.11868852e-02,  -6.81645945e-02,   4.99655865e-02],\n",
       "         [ -6.93269968e-02,   3.44583280e-02,  -5.20486012e-02]]], dtype=float32), bias=0.00069244258, layer=1, neuron_number=36, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.06163583, -0.05937907,  0.01695073],\n",
       "         [ 0.07693966, -0.08201753, -0.05276876],\n",
       "         [-0.0751588 ,  0.03962576, -0.00588046]],\n",
       " \n",
       "        [[ 0.04910225,  0.03572748,  0.07606248],\n",
       "         [-0.05114017, -0.06151215,  0.01079799],\n",
       "         [-0.06833193, -0.04816343,  0.0200777 ]],\n",
       " \n",
       "        [[-0.0351896 , -0.04340456, -0.02163806],\n",
       "         [ 0.01734586,  0.08439326,  0.05726884],\n",
       "         [ 0.00905525,  0.03683359, -0.04673901]],\n",
       " \n",
       "        [[ 0.00843894,  0.0493653 ,  0.03931712],\n",
       "         [-0.02040808, -0.04339182, -0.00362904],\n",
       "         [-0.06761575, -0.05205349,  0.05055619]],\n",
       " \n",
       "        [[ 0.03689725,  0.00453989, -0.01259088],\n",
       "         [-0.07657574, -0.04020903, -0.05282564],\n",
       "         [ 0.00235036, -0.04821879,  0.07122386]],\n",
       " \n",
       "        [[-0.06945283,  0.0197795 , -0.07471015],\n",
       "         [-0.00256295,  0.0299406 , -0.04065254],\n",
       "         [-0.05286712,  0.03334517, -0.05326824]],\n",
       " \n",
       "        [[-0.01265245,  0.01114101,  0.06625787],\n",
       "         [ 0.01542473,  0.02109018, -0.05676989],\n",
       "         [ 0.03536048, -0.05167682, -0.01608438]],\n",
       " \n",
       "        [[ 0.02447021, -0.07649271,  0.04360936],\n",
       "         [-0.08195849,  0.00787244,  0.0766251 ],\n",
       "         [ 0.06264072, -0.05210962,  0.07575165]],\n",
       " \n",
       "        [[ 0.02355173,  0.02006942, -0.05446364],\n",
       "         [-0.00416642,  0.00214572,  0.01435599],\n",
       "         [ 0.0298967 , -0.00019118,  0.0405631 ]],\n",
       " \n",
       "        [[ 0.08310908, -0.02711668, -0.00589303],\n",
       "         [ 0.02510648, -0.01352192,  0.03254797],\n",
       "         [ 0.0355124 , -0.02386246,  0.04466611]],\n",
       " \n",
       "        [[-0.05095106,  0.04740453,  0.00065182],\n",
       "         [ 0.0593727 ,  0.07749893,  0.00568968],\n",
       "         [-0.02136849,  0.05811892, -0.03503267]],\n",
       " \n",
       "        [[ 0.0479054 ,  0.0316439 , -0.03308278],\n",
       "         [-0.06531861, -0.02478729,  0.01304302],\n",
       "         [ 0.05411303,  0.02362151,  0.04319213]],\n",
       " \n",
       "        [[-0.02762671,  0.06739317,  0.04433639],\n",
       "         [ 0.04804688, -0.03848508,  0.0152523 ],\n",
       "         [-0.02100584, -0.06282479, -0.07325295]],\n",
       " \n",
       "        [[-0.036183  ,  0.00252716, -0.03208644],\n",
       "         [-0.01221489,  0.04603327, -0.05376153],\n",
       "         [ 0.03400302,  0.002589  ,  0.03512528]],\n",
       " \n",
       "        [[-0.05044075,  0.01416833, -0.06278837],\n",
       "         [ 0.07083788,  0.03964738, -0.05990727],\n",
       "         [ 0.07827376, -0.05933277,  0.00086551]],\n",
       " \n",
       "        [[ 0.01015544, -0.06912246,  0.07526945],\n",
       "         [ 0.00524553, -0.05324699,  0.02105583],\n",
       "         [ 0.04222532, -0.02079371,  0.01746394]],\n",
       " \n",
       "        [[ 0.07393807, -0.06746722, -0.00740235],\n",
       "         [-0.02879966,  0.04048333, -0.00522467],\n",
       "         [-0.00811234,  0.03438747, -0.05777966]],\n",
       " \n",
       "        [[ 0.03175038,  0.07283878, -0.00849577],\n",
       "         [ 0.04431393,  0.04749776,  0.01780023],\n",
       "         [ 0.05470016,  0.04965551,  0.06275134]],\n",
       " \n",
       "        [[ 0.0642904 , -0.07990152, -0.07064116],\n",
       "         [-0.07593407,  0.06295893, -0.01088291],\n",
       "         [-0.01507591, -0.03820322, -0.01497387]],\n",
       " \n",
       "        [[ 0.05318694,  0.02983896, -0.05039297],\n",
       "         [-0.01270694, -0.05376877, -0.04988204],\n",
       "         [ 0.0318421 , -0.06939942, -0.02893951]],\n",
       " \n",
       "        [[ 0.0447635 , -0.03767736,  0.0843651 ],\n",
       "         [ 0.04910063,  0.02996826, -0.07723677],\n",
       "         [-0.04201872, -0.00665323, -0.06938138]],\n",
       " \n",
       "        [[-0.00963653, -0.0302285 ,  0.05659336],\n",
       "         [ 0.01602609, -0.05755042,  0.07593946],\n",
       "         [ 0.05668045,  0.01560233,  0.06350211]],\n",
       " \n",
       "        [[ 0.0058869 ,  0.0524418 , -0.06258956],\n",
       "         [ 0.08234537, -0.01140011, -0.04192311],\n",
       "         [ 0.045883  ,  0.06394273,  0.0434602 ]],\n",
       " \n",
       "        [[ 0.06595581, -0.03326642,  0.05404514],\n",
       "         [ 0.06712685, -0.0745398 , -0.03837254],\n",
       "         [ 0.07270855,  0.05236575,  0.02868697]],\n",
       " \n",
       "        [[-0.01674509,  0.01933245, -0.05005456],\n",
       "         [-0.07113416, -0.06956632, -0.07175516],\n",
       "         [ 0.06180974, -0.06275453,  0.07776309]],\n",
       " \n",
       "        [[-0.06121857,  0.02861772,  0.04387902],\n",
       "         [ 0.04056407, -0.03093006,  0.02281454],\n",
       "         [ 0.0597536 ,  0.0345152 ,  0.00182144]],\n",
       " \n",
       "        [[ 0.06602519, -0.06345019, -0.06049552],\n",
       "         [-0.05457392, -0.0346006 , -0.00548969],\n",
       "         [-0.05316975,  0.02173228, -0.07330316]],\n",
       " \n",
       "        [[-0.02441773,  0.08281698, -0.04305347],\n",
       "         [-0.01749609, -0.03345171,  0.0278191 ],\n",
       "         [-0.01268412, -0.04346801,  0.07674713]],\n",
       " \n",
       "        [[ 0.08177656, -0.02714453,  0.05631542],\n",
       "         [ 0.03341309, -0.06967454, -0.06455868],\n",
       "         [-0.06979766,  0.0807318 , -0.02667958]],\n",
       " \n",
       "        [[ 0.02329565, -0.04225626,  0.00800473],\n",
       "         [-0.07238738,  0.02365379, -0.02756561],\n",
       "         [ 0.05036112, -0.02611014,  0.06150918]],\n",
       " \n",
       "        [[ 0.01376726, -0.02727092,  0.04390435],\n",
       "         [-0.01986882,  0.06908755, -0.04158273],\n",
       "         [-0.03762572, -0.05775241,  0.07246517]],\n",
       " \n",
       "        [[-0.08065863, -0.02550811, -0.06433637],\n",
       "         [ 0.05140073, -0.07677726, -0.06704933],\n",
       "         [-0.04843815, -0.01819307, -0.01456655]]], dtype=float32), bias=0.00010656845, layer=1, neuron_number=37, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.05753729, -0.01516693, -0.08433949],\n",
       "         [ 0.01441017,  0.03925661, -0.04211525],\n",
       "         [ 0.07953178, -0.04768141,  0.01619342]],\n",
       " \n",
       "        [[ 0.00607461, -0.00605793,  0.04131722],\n",
       "         [-0.02204602,  0.04791579,  0.01289921],\n",
       "         [-0.05110604, -0.01960774,  0.05859843]],\n",
       " \n",
       "        [[ 0.04445175, -0.00397301, -0.03586439],\n",
       "         [-0.07039127, -0.02146527, -0.0044674 ],\n",
       "         [-0.06296032, -0.02181035,  0.06354627]],\n",
       " \n",
       "        [[-0.06104423, -0.06309176,  0.00306588],\n",
       "         [-0.07329404,  0.07475399,  0.00662121],\n",
       "         [-0.00470781,  0.02218336, -0.02278109]],\n",
       " \n",
       "        [[ 0.02802573, -0.05098836, -0.02238865],\n",
       "         [-0.04616623,  0.06109019,  0.06653517],\n",
       "         [-0.07411803, -0.02200307,  0.02505169]],\n",
       " \n",
       "        [[ 0.05019777,  0.08253129, -0.00607755],\n",
       "         [-0.05404378, -0.04272202,  0.00443456],\n",
       "         [ 0.02359661, -0.02824743, -0.02189775]],\n",
       " \n",
       "        [[ 0.03625096, -0.0698104 , -0.0333852 ],\n",
       "         [ 0.02537009,  0.04269418, -0.07212225],\n",
       "         [ 0.02062318,  0.00980065, -0.02544894]],\n",
       " \n",
       "        [[ 0.07959174,  0.05990437, -0.05528958],\n",
       "         [ 0.02301019, -0.05042844,  0.02197138],\n",
       "         [-0.03699828, -0.01111931,  0.06206376]],\n",
       " \n",
       "        [[-0.05753271, -0.07804559,  0.02012107],\n",
       "         [-0.06684542, -0.06229239,  0.00341408],\n",
       "         [-0.07110593,  0.07923645,  0.02119579]],\n",
       " \n",
       "        [[ 0.0396447 ,  0.00012635,  0.00735563],\n",
       "         [ 0.02530729, -0.07592054,  0.07451482],\n",
       "         [-0.03385715,  0.07838506, -0.0405673 ]],\n",
       " \n",
       "        [[-0.04616143, -0.03400264,  0.06470425],\n",
       "         [-0.04320023,  0.0128149 ,  0.01931831],\n",
       "         [ 0.03997783, -0.02927389,  0.06897309]],\n",
       " \n",
       "        [[ 0.05108072, -0.07472254,  0.04424161],\n",
       "         [ 0.03405446,  0.01557995,  0.00680644],\n",
       "         [ 0.03030492,  0.01875069,  0.00604815]],\n",
       " \n",
       "        [[-0.0620889 ,  0.06669132, -0.04253628],\n",
       "         [-0.01678683,  0.07261311, -0.07358493],\n",
       "         [ 0.04491663, -0.00498461,  0.02742613]],\n",
       " \n",
       "        [[ 0.00279738, -0.06970328, -0.00949693],\n",
       "         [ 0.00103783, -0.08360505, -0.06641504],\n",
       "         [-0.03727593,  0.06556047, -0.05665965]],\n",
       " \n",
       "        [[-0.0815234 ,  0.08212572, -0.018992  ],\n",
       "         [ 0.04692646,  0.07314875, -0.03798681],\n",
       "         [ 0.03776602,  0.04886862,  0.02709772]],\n",
       " \n",
       "        [[ 0.06898955,  0.02920517,  0.05821469],\n",
       "         [ 0.02983333, -0.02031395, -0.05979346],\n",
       "         [-0.04350472,  0.05708791, -0.00556831]],\n",
       " \n",
       "        [[-0.02399283, -0.062051  , -0.00521679],\n",
       "         [-0.05847505,  0.08160559,  0.04659781],\n",
       "         [ 0.0330235 ,  0.06830301, -0.01138045]],\n",
       " \n",
       "        [[-0.02737181,  0.01651319, -0.03842673],\n",
       "         [ 0.0275327 ,  0.02890093, -0.05721301],\n",
       "         [ 0.05679473, -0.05444346, -0.07032073]],\n",
       " \n",
       "        [[-0.04900831, -0.04595298,  0.0426874 ],\n",
       "         [ 0.02362795,  0.01321889, -0.02538354],\n",
       "         [-0.02357741,  0.02350969,  0.0237383 ]],\n",
       " \n",
       "        [[ 0.07941762, -0.058259  ,  0.06880412],\n",
       "         [ 0.02212577, -0.01467258,  0.06489809],\n",
       "         [-0.0552883 ,  0.02490453,  0.03632329]],\n",
       " \n",
       "        [[ 0.00295564,  0.08279616,  0.01545638],\n",
       "         [ 0.05286134, -0.01621457, -0.06454863],\n",
       "         [ 0.06420311, -0.04190513,  0.05080023]],\n",
       " \n",
       "        [[-0.01066948,  0.04867167, -0.03415013],\n",
       "         [-0.04823883, -0.07274061, -0.00069319],\n",
       "         [-0.05603997, -0.06330094,  0.0379683 ]],\n",
       " \n",
       "        [[ 0.07005692, -0.0002319 , -0.01826869],\n",
       "         [-0.05291915,  0.00532596,  0.03897915],\n",
       "         [ 0.03534234, -0.06198348,  0.06904607]],\n",
       " \n",
       "        [[-0.02259206, -0.05528283, -0.04072109],\n",
       "         [ 0.07023432,  0.07960368, -0.08176136],\n",
       "         [-0.0291547 ,  0.0002204 , -0.00749955]],\n",
       " \n",
       "        [[-0.05971013, -0.07346296,  0.08163778],\n",
       "         [-0.04441268, -0.08328395,  0.01279129],\n",
       "         [-0.0278578 , -0.0575243 ,  0.05422123]],\n",
       " \n",
       "        [[ 0.06272501,  0.05308194, -0.05518172],\n",
       "         [ 0.03081017,  0.00223155,  0.0820614 ],\n",
       "         [ 0.06751914,  0.02159196,  0.05016807]],\n",
       " \n",
       "        [[-0.02070609, -0.05807827, -0.01344579],\n",
       "         [-0.00259183, -0.04410989, -0.02623387],\n",
       "         [-0.00200584, -0.00984357,  0.07437737]],\n",
       " \n",
       "        [[-0.05487245,  0.03743957,  0.01388261],\n",
       "         [ 0.06232401,  0.05955658, -0.059477  ],\n",
       "         [ 0.0514775 , -0.02969157,  0.00955818]],\n",
       " \n",
       "        [[-0.02949841,  0.01726177, -0.0314539 ],\n",
       "         [ 0.0411786 , -0.02906289,  0.0044304 ],\n",
       "         [ 0.06825618,  0.07467936, -0.06416469]],\n",
       " \n",
       "        [[-0.06411672, -0.04960158,  0.02088881],\n",
       "         [ 0.00102777, -0.00782002, -0.03589041],\n",
       "         [ 0.00281514, -0.07420629,  0.04218384]],\n",
       " \n",
       "        [[-0.07957119,  0.05775425,  0.01823657],\n",
       "         [ 0.07159586,  0.04082957,  0.05916324],\n",
       "         [ 0.00014738, -0.02154108,  0.06063006]],\n",
       " \n",
       "        [[ 0.05859683,  0.03097717,  0.00971423],\n",
       "         [-0.04567548, -0.02869971,  0.0414763 ],\n",
       "         [ 0.021754  , -0.00753495, -0.03521853]]], dtype=float32), bias=-0.0012538338, layer=1, neuron_number=38, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.03871338,  0.05076273,  0.00431518],\n",
       "         [-0.00968215,  0.06586257, -0.05118171],\n",
       "         [-0.00447845,  0.08159836,  0.06228194]],\n",
       " \n",
       "        [[-0.05685404,  0.00455828,  0.017068  ],\n",
       "         [-0.01012742,  0.04083417, -0.01179693],\n",
       "         [ 0.04834355, -0.08244585,  0.02047854]],\n",
       " \n",
       "        [[-0.0373035 , -0.07659952,  0.08167968],\n",
       "         [ 0.01599023, -0.08262474,  0.00867534],\n",
       "         [-0.05024669,  0.05019736, -0.07545239]],\n",
       " \n",
       "        [[ 0.05488097,  0.06637473,  0.02635802],\n",
       "         [-0.04588887, -0.01335787,  0.04511552],\n",
       "         [ 0.02553233,  0.04558315, -0.00876737]],\n",
       " \n",
       "        [[-0.07081224,  0.07041904, -0.07611033],\n",
       "         [-0.00945742, -0.02656043,  0.01953127],\n",
       "         [-0.07726589,  0.06801079, -0.00754731]],\n",
       " \n",
       "        [[ 0.03869224,  0.05649365, -0.03563678],\n",
       "         [ 0.04059535, -0.06187578, -0.0176451 ],\n",
       "         [-0.07447763, -0.00432053, -0.01804064]],\n",
       " \n",
       "        [[ 0.00279426, -0.05396614,  0.0156803 ],\n",
       "         [ 0.04693779,  0.05370243, -0.03630387],\n",
       "         [ 0.03952958,  0.04886207, -0.0345704 ]],\n",
       " \n",
       "        [[ 0.03710695, -0.05451594,  0.04728986],\n",
       "         [-0.03377106,  0.00925164, -0.00870021],\n",
       "         [-0.08103209, -0.05451715, -0.04953498]],\n",
       " \n",
       "        [[-0.04782818, -0.02398653, -0.01135419],\n",
       "         [-0.05123867,  0.06313027,  0.02549912],\n",
       "         [ 0.0592396 , -0.05276036,  0.02691268]],\n",
       " \n",
       "        [[ 0.00558877,  0.05451435,  0.07407329],\n",
       "         [-0.02668357,  0.07880751, -0.05824887],\n",
       "         [-0.04299466,  0.02022846,  0.03067087]],\n",
       " \n",
       "        [[-0.02699613,  0.06238244,  0.03811096],\n",
       "         [-0.00433366,  0.07172734, -0.02422787],\n",
       "         [-0.05363056,  0.02459772, -0.06736428]],\n",
       " \n",
       "        [[-0.07836238, -0.01496505, -0.05161956],\n",
       "         [ 0.07820997, -0.07589054,  0.05532596],\n",
       "         [-0.03429233, -0.0743586 ,  0.00014344]],\n",
       " \n",
       "        [[ 0.00320012,  0.03606502,  0.00281043],\n",
       "         [-0.00486458, -0.00389891,  0.02772652],\n",
       "         [-0.01718228,  0.04604045, -0.03689953]],\n",
       " \n",
       "        [[-0.01985324, -0.00892949, -0.02083158],\n",
       "         [-0.05467458,  0.08326026, -0.06389809],\n",
       "         [ 0.05285948, -0.06116873,  0.08043711]],\n",
       " \n",
       "        [[ 0.05497167,  0.00634529,  0.06969108],\n",
       "         [ 0.02453225, -0.06448963, -0.04139932],\n",
       "         [ 0.0131674 , -0.04831739,  0.01825127]],\n",
       " \n",
       "        [[-0.03953406, -0.07794562,  0.01048233],\n",
       "         [-0.07591752,  0.01003235, -0.03223081],\n",
       "         [ 0.01480495,  0.02595183, -0.02276847]],\n",
       " \n",
       "        [[-0.04191002, -0.03243022,  0.00263537],\n",
       "         [-0.01638616,  0.03411936,  0.03504679],\n",
       "         [ 0.00086875, -0.03333633,  0.02828931]],\n",
       " \n",
       "        [[ 0.05476024,  0.04559147,  0.02573638],\n",
       "         [ 0.00138307, -0.07829797,  0.02077643],\n",
       "         [ 0.00738583,  0.06524444,  0.06685332]],\n",
       " \n",
       "        [[-0.07345317, -0.0510545 ,  0.00786219],\n",
       "         [-0.01678748, -0.0331445 ,  0.05760871],\n",
       "         [ 0.01656814,  0.04941001, -0.01053241]],\n",
       " \n",
       "        [[-0.03529537, -0.02717134,  0.02641401],\n",
       "         [-0.02359889,  0.0734019 , -0.01672493],\n",
       "         [ 0.0352275 , -0.03148971, -0.05794143]],\n",
       " \n",
       "        [[ 0.004939  ,  0.00859204, -0.05644365],\n",
       "         [-0.0510035 , -0.01913293,  0.0282608 ],\n",
       "         [ 0.01851581,  0.03916765,  0.02522955]],\n",
       " \n",
       "        [[ 0.0180085 , -0.05211958,  0.00631093],\n",
       "         [ 0.01721196,  0.07570866,  0.00616971],\n",
       "         [ 0.07547382,  0.04627416, -0.05854125]],\n",
       " \n",
       "        [[-0.05761763, -0.01589804, -0.04404547],\n",
       "         [-0.00790619, -0.05036576,  0.01765245],\n",
       "         [ 0.05945234, -0.03376102, -0.05923895]],\n",
       " \n",
       "        [[-0.00942099,  0.03330654, -0.05160382],\n",
       "         [ 0.01894065,  0.02806666,  0.06171696],\n",
       "         [-0.07458775, -0.01330522, -0.06201229]],\n",
       " \n",
       "        [[-0.04113862, -0.07554051, -0.04761514],\n",
       "         [ 0.03217893, -0.02219205, -0.05043112],\n",
       "         [-0.01759563,  0.05824096, -0.03916118]],\n",
       " \n",
       "        [[ 0.04482746, -0.01530331, -0.03569539],\n",
       "         [-0.0315089 ,  0.05367349,  0.06814325],\n",
       "         [-0.00134323, -0.00417454, -0.01054652]],\n",
       " \n",
       "        [[-0.07789394,  0.06947329,  0.07815667],\n",
       "         [-0.08201471, -0.02731946,  0.07570448],\n",
       "         [-0.0485608 , -0.00077452,  0.04821655]],\n",
       " \n",
       "        [[-0.05001434,  0.04225372, -0.04667953],\n",
       "         [ 0.07383577, -0.0203196 ,  0.03895555],\n",
       "         [ 0.05389494,  0.01059329, -0.05128388]],\n",
       " \n",
       "        [[ 0.02815015, -0.0564354 ,  0.06045894],\n",
       "         [-0.04000404, -0.01769475, -0.03594027],\n",
       "         [-0.06912668,  0.00042687, -0.03319381]],\n",
       " \n",
       "        [[-0.04671154, -0.01519546, -0.08225341],\n",
       "         [-0.02239802,  0.05849114,  0.00334483],\n",
       "         [ 0.02814416, -0.00282649, -0.02256855]],\n",
       " \n",
       "        [[ 0.04678961, -0.06101412, -0.06261732],\n",
       "         [-0.07848914,  0.04097528, -0.07826804],\n",
       "         [ 0.01901979,  0.01009422,  0.05319752]],\n",
       " \n",
       "        [[ 0.06100301, -0.03464731,  0.00698823],\n",
       "         [-0.06143841, -0.03066417, -0.06585405],\n",
       "         [ 0.04837576,  0.00260933,  0.06255127]]], dtype=float32), bias=0.0001592146, layer=1, neuron_number=39, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.04193111,  0.01181357, -0.05654999],\n",
       "         [-0.04358024,  0.0547655 ,  0.07886466],\n",
       "         [-0.03315164, -0.08162332, -0.06305145]],\n",
       " \n",
       "        [[ 0.068013  ,  0.07694862, -0.06312309],\n",
       "         [-0.00534416,  0.05732495,  0.04050333],\n",
       "         [-0.03201421, -0.05869268, -0.06863876]],\n",
       " \n",
       "        [[ 0.07588671,  0.01159893,  0.02695304],\n",
       "         [ 0.06578105,  0.03403544, -0.07729508],\n",
       "         [ 0.061134  , -0.07939045,  0.05003236]],\n",
       " \n",
       "        [[-0.05087289, -0.05934241,  0.02334321],\n",
       "         [ 0.0822269 ,  0.01145503, -0.06548444],\n",
       "         [ 0.04119797, -0.08267242, -0.02806276]],\n",
       " \n",
       "        [[-0.06811856,  0.01748456, -0.05899301],\n",
       "         [-0.07900346,  0.04778672,  0.00199766],\n",
       "         [ 0.02753809, -0.04522991,  0.03380936]],\n",
       " \n",
       "        [[-0.07811311,  0.00385676, -0.02127291],\n",
       "         [ 0.05413334,  0.01536622,  0.05060167],\n",
       "         [ 0.04914884, -0.00807371,  0.02611255]],\n",
       " \n",
       "        [[ 0.04624646, -0.02068295,  0.03658447],\n",
       "         [-0.05754165,  0.06232462, -0.05362656],\n",
       "         [-0.03711357,  0.06375591, -0.01837644]],\n",
       " \n",
       "        [[ 0.05286366,  0.03330751, -0.03412766],\n",
       "         [ 0.02324197,  0.07241032,  0.05139697],\n",
       "         [ 0.0394717 ,  0.06242377, -0.0062042 ]],\n",
       " \n",
       "        [[ 0.0534059 , -0.02145031,  0.05288236],\n",
       "         [-0.01843999, -0.01645252, -0.02904752],\n",
       "         [ 0.02994617,  0.03237449,  0.01326035]],\n",
       " \n",
       "        [[ 0.01555595,  0.06716666,  0.03455051],\n",
       "         [ 0.06986942,  0.00968797,  0.0614748 ],\n",
       "         [-0.06561915,  0.00806594,  0.06709788]],\n",
       " \n",
       "        [[-0.05169583,  0.05609759, -0.04977154],\n",
       "         [-0.01792864,  0.012107  ,  0.0296232 ],\n",
       "         [-0.04249382, -0.05570722, -0.01594485]],\n",
       " \n",
       "        [[ 0.0273643 , -0.03370503, -0.06729299],\n",
       "         [-0.01442563, -0.06167443,  0.06924633],\n",
       "         [-0.04608068, -0.04807806,  0.00269883]],\n",
       " \n",
       "        [[ 0.0250948 , -0.05107137, -0.01323181],\n",
       "         [ 0.05248492, -0.00996669, -0.04465811],\n",
       "         [ 0.02652263,  0.04459656,  0.00495707]],\n",
       " \n",
       "        [[ 0.05786132, -0.01966497, -0.07278096],\n",
       "         [ 0.07200055, -0.06045676, -0.06436813],\n",
       "         [-0.03773759,  0.04995693,  0.04709211]],\n",
       " \n",
       "        [[ 0.03100727,  0.06833673,  0.07959022],\n",
       "         [ 0.0752364 , -0.06705533, -0.01808441],\n",
       "         [-0.00695634,  0.03313388,  0.03534523]],\n",
       " \n",
       "        [[ 0.06771018,  0.07009488,  0.05989518],\n",
       "         [-0.05018947,  0.03476119,  0.00820369],\n",
       "         [ 0.00320708,  0.07045883,  0.07034156]],\n",
       " \n",
       "        [[-0.0611799 ,  0.00298918,  0.02367746],\n",
       "         [ 0.02829374,  0.01065992,  0.04282997],\n",
       "         [-0.04879583, -0.07112506,  0.06982442]],\n",
       " \n",
       "        [[ 0.05502455,  0.01973863,  0.08117445],\n",
       "         [-0.04899527, -0.01715979, -0.05247838],\n",
       "         [-0.06144401, -0.04514864,  0.03228986]],\n",
       " \n",
       "        [[ 0.04177333,  0.06864993,  0.02547387],\n",
       "         [ 0.01978001, -0.0442138 ,  0.02408384],\n",
       "         [-0.05425512,  0.04753859,  0.05538101]],\n",
       " \n",
       "        [[ 0.03108314, -0.03863479, -0.01033862],\n",
       "         [-0.02313571, -0.00830005, -0.06712879],\n",
       "         [-0.06216299, -0.00709647, -0.0012558 ]],\n",
       " \n",
       "        [[ 0.04568598,  0.08033522,  0.07490393],\n",
       "         [ 0.07591991, -0.08116116, -0.05083543],\n",
       "         [ 0.07208614, -0.02193035, -0.00723554]],\n",
       " \n",
       "        [[-0.01932181, -0.03774454, -0.07217877],\n",
       "         [ 0.01097988, -0.0716747 ,  0.01240385],\n",
       "         [-0.05354786,  0.07765207, -0.08185721]],\n",
       " \n",
       "        [[ 0.01739564, -0.06031825, -0.06834476],\n",
       "         [-0.01761972, -0.04126278,  0.05759174],\n",
       "         [-0.06988028, -0.02146165,  0.04845798]],\n",
       " \n",
       "        [[ 0.06719428, -0.04932233,  0.00012679],\n",
       "         [-0.01715141, -0.08180172, -0.01714612],\n",
       "         [ 0.01370317, -0.06250866,  0.08222945]],\n",
       " \n",
       "        [[ 0.06912977, -0.00850029,  0.01723314],\n",
       "         [-0.01466013,  0.05320062,  0.08123773],\n",
       "         [ 0.02416934,  0.03789936, -0.05498407]],\n",
       " \n",
       "        [[ 0.0262759 , -0.0728263 ,  0.06311423],\n",
       "         [ 0.03129148,  0.04613523,  0.05045044],\n",
       "         [-0.07791073,  0.03060029,  0.04632307]],\n",
       " \n",
       "        [[ 0.00472715,  0.03017722,  0.07595368],\n",
       "         [-0.04202182,  0.05914969,  0.01690207],\n",
       "         [-0.07372304, -0.0103727 ,  0.02199908]],\n",
       " \n",
       "        [[-0.05283607,  0.06075499,  0.00993607],\n",
       "         [ 0.0354918 ,  0.06625369,  0.05039363],\n",
       "         [-0.01088782,  0.00278068, -0.07934888]],\n",
       " \n",
       "        [[-0.034314  , -0.06223927,  0.05423542],\n",
       "         [ 0.03221203,  0.04515941, -0.00884823],\n",
       "         [-0.0802104 , -0.03975266, -0.00227418]],\n",
       " \n",
       "        [[-0.00137136, -0.02375299,  0.04623018],\n",
       "         [ 0.05978416, -0.01538646,  0.02586714],\n",
       "         [ 0.00864676, -0.03604757,  0.0062154 ]],\n",
       " \n",
       "        [[ 0.02320038, -0.01964692, -0.02249499],\n",
       "         [ 0.00524125,  0.05810277, -0.0044775 ],\n",
       "         [-0.05182807,  0.03848467, -0.05995988]],\n",
       " \n",
       "        [[-0.03089445,  0.04932493, -0.03009649],\n",
       "         [-0.06238675, -0.01205317,  0.06688341],\n",
       "         [ 0.02807187,  0.03009717, -0.06452126]]], dtype=float32), bias=0.0011117831, layer=1, neuron_number=40, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ -3.00669577e-02,   1.73878539e-02,  -8.11085328e-02],\n",
       "         [  3.25142033e-02,  -2.52015181e-02,  -7.50301555e-02],\n",
       "         [ -8.29083100e-03,  -3.15643325e-02,   2.93100514e-02]],\n",
       " \n",
       "        [[  2.35282946e-02,  -3.13163176e-02,  -5.54793999e-02],\n",
       "         [  5.21716699e-02,   4.48244158e-03,   5.72564602e-02],\n",
       "         [ -1.80244260e-03,  -6.40195012e-02,   1.98237915e-02]],\n",
       " \n",
       "        [[  2.81963423e-02,  -3.40631902e-02,  -6.99196607e-02],\n",
       "         [  1.07489189e-03,  -7.15545863e-02,   3.64276324e-03],\n",
       "         [ -1.44912582e-02,  -7.13647204e-03,  -7.60838762e-02]],\n",
       " \n",
       "        [[ -1.55443782e-02,   5.09471111e-02,   4.04834487e-02],\n",
       "         [ -2.76363958e-02,   7.46503770e-02,  -8.07558075e-02],\n",
       "         [  1.92987584e-02,   5.15107997e-03,   7.53488168e-02]],\n",
       " \n",
       "        [[  6.04010709e-02,   1.60628073e-02,  -5.47338054e-02],\n",
       "         [  1.35458456e-02,   6.29854649e-02,  -8.11279565e-02],\n",
       "         [  4.20557372e-02,   3.71268615e-02,   2.90241279e-02]],\n",
       " \n",
       "        [[ -7.95405507e-02,   5.14703430e-02,   5.11929877e-02],\n",
       "         [ -3.22945192e-02,  -3.98231894e-02,   5.66603430e-02],\n",
       "         [ -6.32474050e-02,   3.06843985e-02,  -1.59935057e-02]],\n",
       " \n",
       "        [[  1.33618629e-02,   6.47990853e-02,  -3.91196981e-02],\n",
       "         [  2.59445570e-02,   2.63941474e-02,  -5.83626181e-02],\n",
       "         [ -2.16477253e-02,  -5.55451028e-02,  -2.97145508e-02]],\n",
       " \n",
       "        [[ -8.28416571e-02,  -1.75939742e-02,  -5.11728674e-02],\n",
       "         [  6.30645975e-02,   5.13213165e-02,  -1.72115937e-02],\n",
       "         [ -5.89268617e-02,  -6.25486821e-02,  -8.18909481e-02]],\n",
       " \n",
       "        [[  5.17552719e-02,   4.21139859e-02,   5.53515330e-02],\n",
       "         [ -7.69609213e-02,  -7.47759268e-02,  -3.15344632e-02],\n",
       "         [  5.55312866e-03,  -2.79756207e-02,   6.94811791e-02]],\n",
       " \n",
       "        [[  7.78302252e-02,  -1.24336053e-02,  -4.96450253e-03],\n",
       "         [  2.41145808e-02,  -2.11908910e-02,  -2.94914935e-02],\n",
       "         [ -7.22767692e-03,  -5.58333062e-02,   4.96638659e-03]],\n",
       " \n",
       "        [[ -6.66189194e-02,  -7.91613534e-02,   2.11831145e-02],\n",
       "         [ -6.64233118e-02,  -6.65453598e-02,   7.34565705e-02],\n",
       "         [ -7.08287880e-02,  -1.99473519e-02,  -5.07455878e-02]],\n",
       " \n",
       "        [[ -2.43368912e-02,  -7.52015486e-02,  -5.55383675e-02],\n",
       "         [ -4.69078422e-02,  -1.05890110e-02,  -2.32803784e-02],\n",
       "         [ -3.18177454e-02,   1.89421084e-02,  -4.40821201e-02]],\n",
       " \n",
       "        [[  1.46577265e-02,   4.75689434e-02,   7.27417320e-02],\n",
       "         [  2.17323676e-02,  -7.55892098e-02,  -1.26168141e-02],\n",
       "         [ -3.09452303e-02,  -1.48749864e-02,   6.05149120e-02]],\n",
       " \n",
       "        [[ -4.52339612e-02,   6.10694587e-02,   7.54982755e-02],\n",
       "         [  2.23671440e-02,   4.69876826e-02,  -4.86086607e-02],\n",
       "         [  3.47790564e-03,   5.99376634e-02,  -7.88859129e-02]],\n",
       " \n",
       "        [[ -7.58266225e-02,  -4.90928590e-02,   2.78732143e-02],\n",
       "         [ -3.95717323e-02,   3.48808728e-02,  -7.20866993e-02],\n",
       "         [ -4.37565148e-02,  -6.26460239e-02,  -6.06459975e-02]],\n",
       " \n",
       "        [[  1.56518165e-02,  -3.53382118e-02,  -2.46776957e-02],\n",
       "         [ -6.24236763e-02,  -2.08578650e-02,  -5.79549633e-02],\n",
       "         [ -3.65119055e-02,  -5.32838218e-02,   5.08411452e-02]],\n",
       " \n",
       "        [[ -4.53602672e-02,  -2.71890704e-02,  -1.43456906e-02],\n",
       "         [  4.74548601e-02,  -5.66453636e-02,   6.54407367e-02],\n",
       "         [ -1.36404103e-02,   3.18335705e-02,  -6.03568032e-02]],\n",
       " \n",
       "        [[  5.53119043e-03,  -3.67455594e-02,  -7.86429569e-02],\n",
       "         [ -7.96767697e-02,  -2.77948286e-02,  -6.38058186e-02],\n",
       "         [ -5.88618815e-02,   1.66901946e-02,  -9.83403879e-04]],\n",
       " \n",
       "        [[  5.94069809e-02,  -3.94538939e-02,  -3.52278501e-02],\n",
       "         [ -6.91568032e-02,  -4.42936681e-02,   1.53829502e-02],\n",
       "         [ -4.95403297e-02,  -8.72459542e-03,   5.67546114e-02]],\n",
       " \n",
       "        [[ -3.07626277e-02,   4.99709137e-02,  -4.59130201e-03],\n",
       "         [ -3.95324081e-03,  -3.93208256e-03,  -2.11573076e-02],\n",
       "         [  2.85102688e-02,  -3.49820368e-02,   2.51969229e-02]],\n",
       " \n",
       "        [[ -4.52556200e-02,  -4.40950580e-02,  -6.17001913e-02],\n",
       "         [  4.21313383e-03,  -4.25435929e-03,   7.40074143e-02],\n",
       "         [ -4.89156321e-02,  -1.86622646e-02,  -1.92297325e-02]],\n",
       " \n",
       "        [[ -5.10451086e-02,  -3.65299210e-02,  -4.97270823e-02],\n",
       "         [  2.01082565e-02,  -2.70992126e-02,  -7.23446533e-02],\n",
       "         [  8.14304873e-02,   2.77182590e-02,   6.10089973e-02]],\n",
       " \n",
       "        [[  7.30297863e-02,   6.52232543e-02,  -3.52969393e-02],\n",
       "         [ -5.79497814e-02,  -2.90056597e-03,   6.20249733e-02],\n",
       "         [ -4.83688749e-02,  -5.93880899e-02,  -5.03715128e-03]],\n",
       " \n",
       "        [[ -5.62516674e-02,   1.23049738e-02,  -1.05905877e-02],\n",
       "         [ -5.15369736e-02,  -3.59082781e-02,  -4.02038172e-02],\n",
       "         [  1.18763251e-02,  -3.56203616e-02,  -4.34865691e-02]],\n",
       " \n",
       "        [[  8.10140744e-02,   8.09769705e-02,   6.84046224e-02],\n",
       "         [ -4.39966768e-02,  -5.62603325e-02,  -5.95520809e-02],\n",
       "         [ -1.13371387e-02,  -5.27983904e-03,   5.83806224e-02]],\n",
       " \n",
       "        [[  2.07964461e-02,  -3.41799632e-02,   5.26200719e-02],\n",
       "         [  3.01928329e-03,   5.10938801e-02,   3.25560868e-02],\n",
       "         [  7.35419393e-02,  -6.22154213e-02,  -5.19528762e-02]],\n",
       " \n",
       "        [[  7.98454657e-02,  -1.09477900e-02,  -1.79390348e-02],\n",
       "         [ -7.11994246e-02,  -1.46520901e-02,   2.73745712e-02],\n",
       "         [ -2.47050673e-02,   1.58952866e-02,  -4.01147716e-02]],\n",
       " \n",
       "        [[ -4.53536138e-02,   4.71594110e-02,   1.12300860e-02],\n",
       "         [ -6.55151084e-02,  -8.08668286e-02,   2.22247727e-02],\n",
       "         [ -2.55212542e-02,   4.92961891e-02,  -9.01831407e-03]],\n",
       " \n",
       "        [[ -8.20643455e-03,  -1.11399451e-02,  -2.62194984e-02],\n",
       "         [ -3.60927247e-02,  -5.61452992e-02,   7.68506061e-03],\n",
       "         [  2.42351145e-02,   5.98015301e-02,   2.00456362e-02]],\n",
       " \n",
       "        [[ -2.69697644e-02,  -1.87994260e-02,   6.09997287e-02],\n",
       "         [  1.22259473e-02,   6.84823617e-02,  -4.78696860e-02],\n",
       "         [  4.00502421e-02,  -7.01887161e-02,   4.84745353e-02]],\n",
       " \n",
       "        [[  2.06428561e-02,  -5.19996102e-05,   2.86522452e-02],\n",
       "         [ -3.71772386e-02,  -4.02692035e-02,   8.09713528e-02],\n",
       "         [  7.38730580e-02,   5.72110154e-02,  -1.02355182e-02]],\n",
       " \n",
       "        [[ -2.63847187e-02,  -2.25655027e-02,   8.50612205e-03],\n",
       "         [  6.83509037e-02,  -2.26731822e-02,  -4.83005606e-02],\n",
       "         [  6.69814274e-02,  -1.36232348e-02,   6.96176216e-02]]], dtype=float32), bias=-0.0011692954, layer=1, neuron_number=41, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ -5.85446097e-02,  -2.65658125e-02,  -1.45145860e-02],\n",
       "         [ -1.60504058e-02,  -9.68848728e-03,   7.15847835e-02],\n",
       "         [  3.22487876e-02,  -1.23052951e-02,   5.44829257e-02]],\n",
       " \n",
       "        [[  4.21538018e-02,  -1.95483733e-02,  -2.90201064e-02],\n",
       "         [  5.46892211e-02,   1.71469878e-02,  -3.40583324e-02],\n",
       "         [ -4.16385382e-03,   3.70069258e-02,  -5.68202287e-02]],\n",
       " \n",
       "        [[ -3.96446139e-02,  -1.29968850e-02,   2.87284516e-02],\n",
       "         [  7.89704397e-02,   2.07408685e-02,  -5.84347546e-02],\n",
       "         [ -2.63054688e-02,  -1.68922786e-02,  -1.20756729e-02]],\n",
       " \n",
       "        [[ -1.11284973e-02,  -6.39229342e-02,  -3.36939469e-02],\n",
       "         [  2.42292676e-02,   5.57049848e-02,   7.43687227e-02],\n",
       "         [ -1.62219554e-02,   3.08444481e-02,   7.24065006e-02]],\n",
       " \n",
       "        [[  4.24916744e-02,   7.31222406e-02,  -7.86967427e-02],\n",
       "         [ -9.03115049e-03,   1.11777885e-02,   2.76178494e-02],\n",
       "         [  2.31872015e-02,   2.86143236e-02,   3.76999862e-02]],\n",
       " \n",
       "        [[  6.97180778e-02,   1.41535113e-02,  -1.56413317e-02],\n",
       "         [  4.99138460e-02,   3.09041934e-04,  -8.22955519e-02],\n",
       "         [ -2.63288543e-02,  -4.02993485e-02,   5.50343394e-02]],\n",
       " \n",
       "        [[  8.08506384e-02,   7.00865090e-02,  -5.60571440e-02],\n",
       "         [ -4.82278280e-02,  -6.69984147e-02,  -7.98747092e-02],\n",
       "         [  5.48141114e-02,  -4.25740294e-02,  -2.72325296e-02]],\n",
       " \n",
       "        [[  1.12406937e-02,  -1.32579040e-02,   2.68101040e-02],\n",
       "         [  7.49166310e-02,  -3.70541587e-02,   2.16932874e-02],\n",
       "         [ -5.50788753e-02,  -6.94321096e-02,  -3.40825953e-02]],\n",
       " \n",
       "        [[ -2.73496453e-02,  -7.32931076e-03,   2.76806597e-02],\n",
       "         [ -2.05353983e-02,   2.17777584e-02,   5.10183759e-02],\n",
       "         [  3.73428352e-02,   6.69673234e-02,   7.32654706e-02]],\n",
       " \n",
       "        [[  8.10609683e-02,  -5.13576064e-03,  -4.46229875e-02],\n",
       "         [ -4.63459687e-03,  -2.52376404e-02,   7.54607841e-02],\n",
       "         [ -3.83116268e-02,   1.47416601e-02,  -7.52140954e-02]],\n",
       " \n",
       "        [[ -6.64186701e-02,   2.77575254e-02,   7.76032209e-02],\n",
       "         [ -6.96376190e-02,   7.68103600e-02,   6.95136338e-02],\n",
       "         [  3.19245420e-02,  -3.52060160e-05,   1.23814857e-02]],\n",
       " \n",
       "        [[ -6.41960874e-02,   3.81366909e-02,   4.53529023e-02],\n",
       "         [ -6.93275556e-02,   8.17432627e-02,   1.35859707e-02],\n",
       "         [  1.77855475e-03,   6.00591674e-03,  -5.27130254e-03]],\n",
       " \n",
       "        [[  1.94111988e-02,   6.90673385e-03,   2.44397428e-02],\n",
       "         [  2.77013965e-02,  -1.74041316e-02,  -3.68904881e-02],\n",
       "         [ -7.48597160e-02,  -6.10445626e-02,   6.82340041e-02]],\n",
       " \n",
       "        [[ -7.79323280e-02,   2.13838108e-02,  -4.13328633e-02],\n",
       "         [  5.08348867e-02,  -4.27093133e-02,  -5.89345694e-02],\n",
       "         [ -6.55289739e-02,  -4.35871296e-02,  -1.86164007e-02]],\n",
       " \n",
       "        [[ -5.03665432e-02,  -7.88638070e-02,  -4.62722071e-02],\n",
       "         [  1.13968560e-02,  -4.43312638e-02,   4.11638282e-02],\n",
       "         [ -6.90801218e-02,   7.40336180e-02,   5.79890348e-02]],\n",
       " \n",
       "        [[ -3.86845581e-02,   4.97320034e-02,  -5.04794344e-02],\n",
       "         [ -7.21099004e-02,  -6.71979189e-02,  -7.54112601e-02],\n",
       "         [  2.03923099e-02,   1.93005260e-02,  -5.46048321e-02]],\n",
       " \n",
       "        [[  6.79593831e-02,  -7.02174753e-02,  -5.69225438e-02],\n",
       "         [  3.89293879e-02,  -8.03032964e-02,  -5.71207963e-02],\n",
       "         [ -2.07298081e-02,  -7.47160614e-02,  -1.40941329e-02]],\n",
       " \n",
       "        [[  5.00305556e-02,   7.67456219e-02,  -5.27108088e-02],\n",
       "         [ -7.34655783e-02,  -1.86105240e-02,   5.92978895e-02],\n",
       "         [ -3.98690328e-02,  -3.44343446e-02,   5.28279785e-03]],\n",
       " \n",
       "        [[ -2.62426045e-02,   3.27588581e-02,   1.88587455e-03],\n",
       "         [  1.39015028e-02,   3.47918458e-02,   7.96267390e-02],\n",
       "         [  6.84115961e-02,  -2.72464547e-02,  -2.43960265e-02]],\n",
       " \n",
       "        [[ -3.18410881e-02,   3.50219272e-02,  -2.97698751e-02],\n",
       "         [  3.38749215e-02,  -1.63925011e-02,  -2.85620280e-02],\n",
       "         [  7.51959160e-02,   6.81846440e-02,  -3.16385031e-02]],\n",
       " \n",
       "        [[  6.42723739e-02,  -5.94214238e-02,   6.27449304e-02],\n",
       "         [  3.61173600e-02,   8.00301880e-02,   6.17891438e-02],\n",
       "         [ -1.72761977e-02,  -7.08340406e-02,   6.16334006e-02]],\n",
       " \n",
       "        [[  3.19791734e-02,   1.95941259e-03,   6.67543188e-02],\n",
       "         [  1.85424667e-02,   1.17562721e-02,   6.93427771e-02],\n",
       "         [ -3.12494859e-02,  -8.33099335e-02,  -7.98428357e-02]],\n",
       " \n",
       "        [[ -3.85759994e-02,  -5.47819026e-02,  -8.88507720e-03],\n",
       "         [ -3.76123860e-02,  -6.78244382e-02,   7.63140246e-02],\n",
       "         [ -8.66071228e-03,   1.67066958e-02,   2.52910256e-02]],\n",
       " \n",
       "        [[ -2.56040823e-02,  -9.59385186e-03,  -3.31623591e-02],\n",
       "         [  3.46145704e-02,   7.38877356e-02,  -7.93158934e-02],\n",
       "         [ -4.02371921e-02,  -2.60519590e-02,   2.99630295e-02]],\n",
       " \n",
       "        [[ -4.29246165e-02,   6.93662539e-02,  -5.70139922e-02],\n",
       "         [ -6.72887713e-02,  -4.94545959e-02,   8.15900788e-02],\n",
       "         [  3.37308720e-02,   5.37818782e-02,   4.10272144e-02]],\n",
       " \n",
       "        [[ -3.16081159e-02,   3.05351131e-02,  -6.26336336e-02],\n",
       "         [ -1.63330548e-02,  -2.80148853e-02,   7.88687840e-02],\n",
       "         [  7.21127391e-02,   2.90664230e-02,  -5.60655482e-02]],\n",
       " \n",
       "        [[ -3.42331454e-02,   6.69552460e-02,   2.15178337e-02],\n",
       "         [ -3.59294601e-02,   4.23438512e-02,   5.01890294e-02],\n",
       "         [  3.16249579e-02,   2.00868417e-02,  -1.35213975e-03]],\n",
       " \n",
       "        [[  1.90830417e-02,  -8.14310014e-02,   7.12008551e-02],\n",
       "         [  1.37853436e-03,  -5.83831146e-02,   7.53998309e-02],\n",
       "         [  7.00650513e-02,  -3.43397912e-03,  -6.00445177e-03]],\n",
       " \n",
       "        [[  7.21886754e-02,   4.57068533e-02,   5.63748442e-02],\n",
       "         [ -4.30544242e-02,   3.46030854e-02,  -4.53566872e-02],\n",
       "         [ -1.41682485e-02,  -5.62961074e-03,  -6.83981255e-02]],\n",
       " \n",
       "        [[  3.47613730e-02,  -6.82837442e-02,  -2.20778808e-02],\n",
       "         [ -6.04500659e-02,  -7.35822544e-02,   5.03779873e-02],\n",
       "         [  9.41665098e-03,  -5.00771254e-02,  -7.48273581e-02]],\n",
       " \n",
       "        [[ -3.30277830e-02,  -3.27374600e-03,  -1.13905752e-02],\n",
       "         [ -1.60019118e-02,  -6.59737140e-02,  -3.36950906e-02],\n",
       "         [ -2.84815952e-02,   2.25312915e-02,   3.19107547e-02]],\n",
       " \n",
       "        [[ -1.02632691e-03,  -1.74791478e-02,   8.33400246e-03],\n",
       "         [  8.09401348e-02,   1.56683568e-02,  -5.47507107e-02],\n",
       "         [  9.78599116e-03,  -4.66620773e-02,   1.60599221e-02]]], dtype=float32), bias=-0.00032146939, layer=1, neuron_number=42, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.07904352, -0.01231853, -0.07486563],\n",
       "         [-0.00227296, -0.00492153,  0.06026952],\n",
       "         [-0.05240919, -0.06534844,  0.05115613]],\n",
       " \n",
       "        [[ 0.02008331, -0.00920289, -0.01178747],\n",
       "         [-0.01916838,  0.08315093,  0.03219428],\n",
       "         [-0.07420342, -0.07047538,  0.03942123]],\n",
       " \n",
       "        [[ 0.07454085,  0.06329644,  0.07437845],\n",
       "         [ 0.03124048,  0.03027562,  0.0312417 ],\n",
       "         [-0.05704993,  0.01719657, -0.0294572 ]],\n",
       " \n",
       "        [[-0.01431281,  0.01300762,  0.03762481],\n",
       "         [-0.07658792, -0.03606928, -0.03383692],\n",
       "         [ 0.0084877 , -0.02027543,  0.05369204]],\n",
       " \n",
       "        [[-0.07231033, -0.01184101, -0.07926008],\n",
       "         [-0.02279818,  0.04957881,  0.01272304],\n",
       "         [-0.02542332, -0.05506333,  0.06772804]],\n",
       " \n",
       "        [[-0.00954325, -0.07778686, -0.05681762],\n",
       "         [ 0.06873643,  0.05452667,  0.02049784],\n",
       "         [-0.00166645, -0.04418029, -0.07084841]],\n",
       " \n",
       "        [[-0.06496599, -0.02387809, -0.07483018],\n",
       "         [-0.04975197,  0.078887  , -0.08285809],\n",
       "         [-0.01232452, -0.04920304, -0.02339781]],\n",
       " \n",
       "        [[ 0.01353479, -0.02335424, -0.00728274],\n",
       "         [-0.06572131,  0.04395771,  0.02509984],\n",
       "         [ 0.04850094,  0.07874972, -0.05205737]],\n",
       " \n",
       "        [[-0.05066286, -0.02038353,  0.07366772],\n",
       "         [ 0.06646271, -0.0123763 ,  0.01021069],\n",
       "         [ 0.02218504, -0.0680961 , -0.05871763]],\n",
       " \n",
       "        [[-0.05507259, -0.06732596,  0.07231447],\n",
       "         [-0.02008444,  0.01646856, -0.08014495],\n",
       "         [-0.07710674,  0.0539766 , -0.07585029]],\n",
       " \n",
       "        [[-0.06719137,  0.05144553,  0.01608082],\n",
       "         [ 0.01709595, -0.07977685,  0.05246213],\n",
       "         [-0.03593836,  0.01331871, -0.07410157]],\n",
       " \n",
       "        [[ 0.06965237, -0.07823981, -0.07555532],\n",
       "         [ 0.06220456, -0.05820936, -0.06279113],\n",
       "         [-0.00481781,  0.0780896 ,  0.05434641]],\n",
       " \n",
       "        [[-0.04925677,  0.06825617, -0.0421441 ],\n",
       "         [ 0.01795421, -0.03020782, -0.06037711],\n",
       "         [-0.01111331, -0.03069048, -0.03963936]],\n",
       " \n",
       "        [[-0.0263968 ,  0.03553122, -0.04137937],\n",
       "         [ 0.04341161,  0.02951789,  0.06336553],\n",
       "         [-0.01280814, -0.05496358,  0.01045965]],\n",
       " \n",
       "        [[ 0.03413334, -0.02471375, -0.022943  ],\n",
       "         [ 0.01545836, -0.03805685,  0.0199195 ],\n",
       "         [ 0.07385228,  0.03383971,  0.04901273]],\n",
       " \n",
       "        [[ 0.04331626,  0.00807872, -0.0281515 ],\n",
       "         [ 0.00082692,  0.07846235, -0.07138383],\n",
       "         [ 0.01339622, -0.0241648 ,  0.06909282]],\n",
       " \n",
       "        [[ 0.0726717 ,  0.06187342,  0.01557321],\n",
       "         [-0.02346194, -0.0786429 ,  0.02976499],\n",
       "         [ 0.03612049, -0.06378445,  0.00732513]],\n",
       " \n",
       "        [[-0.06219206, -0.04555157, -0.07561487],\n",
       "         [ 0.05977366,  0.00062539,  0.0612773 ],\n",
       "         [-0.07672752,  0.06626414,  0.07552299]],\n",
       " \n",
       "        [[ 0.01357461,  0.01446087, -0.00952169],\n",
       "         [-0.06608821, -0.01531073, -0.00557049],\n",
       "         [ 0.05774293,  0.02676386,  0.06283138]],\n",
       " \n",
       "        [[ 0.04455962, -0.00911436, -0.01090434],\n",
       "         [ 0.01574561, -0.01401553, -0.01939793],\n",
       "         [ 0.01740666, -0.06480622,  0.07621227]],\n",
       " \n",
       "        [[ 0.06973784,  0.04482944, -0.0330469 ],\n",
       "         [ 0.02394407,  0.03155475, -0.04566055],\n",
       "         [-0.08148017,  0.02081747, -0.06911372]],\n",
       " \n",
       "        [[ 0.00961137,  0.00070097, -0.00346403],\n",
       "         [ 0.07003102, -0.03487777,  0.06888568],\n",
       "         [-0.02761106, -0.05826641,  0.07990909]],\n",
       " \n",
       "        [[ 0.02458659,  0.03598219,  0.01678771],\n",
       "         [ 0.04308388,  0.02813665, -0.06188755],\n",
       "         [-0.01728881,  0.04548322,  0.07446349]],\n",
       " \n",
       "        [[ 0.08238143,  0.03473332, -0.07203448],\n",
       "         [-0.08011916,  0.01940464,  0.03053575],\n",
       "         [-0.01843766,  0.07072673,  0.04251777]],\n",
       " \n",
       "        [[ 0.01789161,  0.01216264,  0.07370456],\n",
       "         [ 0.00556942, -0.00750068, -0.01544153],\n",
       "         [ 0.03029344, -0.06795979, -0.083051  ]],\n",
       " \n",
       "        [[-0.00290649, -0.00115729,  0.04706281],\n",
       "         [-0.07609654,  0.04119446,  0.07906221],\n",
       "         [-0.01934507,  0.00714656, -0.06828374]],\n",
       " \n",
       "        [[ 0.03786801,  0.06155352, -0.03471059],\n",
       "         [ 0.0199621 ,  0.05732745, -0.04125367],\n",
       "         [ 0.01931869, -0.07653483, -0.0811711 ]],\n",
       " \n",
       "        [[ 0.06598441, -0.04039875, -0.02471146],\n",
       "         [-0.03101805, -0.03230513,  0.05262654],\n",
       "         [-0.05929603, -0.04982657, -0.06707427]],\n",
       " \n",
       "        [[-0.05512926,  0.00815285, -0.07910102],\n",
       "         [ 0.00701216, -0.06373567, -0.07903667],\n",
       "         [ 0.06426011,  0.04772271,  0.03382453]],\n",
       " \n",
       "        [[ 0.06029141, -0.01603857,  0.05238643],\n",
       "         [ 0.00952147, -0.06152138, -0.02758878],\n",
       "         [-0.00336712, -0.07713894, -0.07635036]],\n",
       " \n",
       "        [[ 0.03973683,  0.01545771, -0.08237214],\n",
       "         [-0.01818502, -0.08217421, -0.0064513 ],\n",
       "         [-0.07776083, -0.02795908,  0.03362472]],\n",
       " \n",
       "        [[ 0.04678186,  0.04153603,  0.04148706],\n",
       "         [-0.06957024,  0.026908  , -0.06874014],\n",
       "         [-0.0597164 ,  0.0455731 ,  0.06235667]]], dtype=float32), bias=0.0005008406, layer=1, neuron_number=43, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.01963707,  0.0616352 , -0.03626093],\n",
       "         [ 0.07864653, -0.03636817, -0.04889459],\n",
       "         [ 0.02748179,  0.01152167, -0.05671737]],\n",
       " \n",
       "        [[ 0.03455075, -0.0294306 , -0.01959376],\n",
       "         [ 0.01227774, -0.02472023, -0.0578989 ],\n",
       "         [ 0.06867701, -0.05541862,  0.05106781]],\n",
       " \n",
       "        [[ 0.05306212, -0.00760215, -0.03916038],\n",
       "         [ 0.03838603,  0.07477297,  0.00798605],\n",
       "         [-0.034179  , -0.01955149,  0.02188495]],\n",
       " \n",
       "        [[ 0.07894976,  0.05369622,  0.040235  ],\n",
       "         [-0.05522152, -0.04790686, -0.01897566],\n",
       "         [ 0.05313133,  0.06795438, -0.04965628]],\n",
       " \n",
       "        [[-0.03831395,  0.07103717, -0.03814455],\n",
       "         [-0.07512106, -0.06391391,  0.0117127 ],\n",
       "         [-0.05710528,  0.01175924, -0.03409057]],\n",
       " \n",
       "        [[-0.06411913,  0.05984727,  0.07415951],\n",
       "         [ 0.049985  , -0.02568659, -0.04678686],\n",
       "         [ 0.08186495, -0.02985095,  0.03688948]],\n",
       " \n",
       "        [[ 0.02799872,  0.00433157,  0.00159906],\n",
       "         [ 0.01133985, -0.00201137, -0.00274532],\n",
       "         [-0.06061336, -0.03821911, -0.03802086]],\n",
       " \n",
       "        [[-0.05941745,  0.03334209, -0.0739494 ],\n",
       "         [ 0.0791653 , -0.02702282, -0.02843783],\n",
       "         [-0.07003266,  0.04927997,  0.06039173]],\n",
       " \n",
       "        [[ 0.01779896, -0.06795865,  0.07650343],\n",
       "         [-0.06927629,  0.00118773,  0.00078705],\n",
       "         [-0.0633916 , -0.02075147,  0.08209597]],\n",
       " \n",
       "        [[ 0.06445736,  0.05992272, -0.01182086],\n",
       "         [ 0.08246505,  0.07280488,  0.04378459],\n",
       "         [ 0.04659974,  0.02737635, -0.07359029]],\n",
       " \n",
       "        [[ 0.01017045, -0.08125634,  0.078664  ],\n",
       "         [-0.00037792, -0.03644658,  0.0802592 ],\n",
       "         [-0.05581047, -0.04972992, -0.00740397]],\n",
       " \n",
       "        [[ 0.04794871, -0.06625239, -0.06131627],\n",
       "         [ 0.05038922,  0.08002038, -0.03405368],\n",
       "         [-0.02204169,  0.00647777,  0.04523392]],\n",
       " \n",
       "        [[-0.0726153 ,  0.03760798, -0.02636736],\n",
       "         [ 0.07431678,  0.05710429,  0.01059349],\n",
       "         [-0.06783134, -0.03036535,  0.01939958]],\n",
       " \n",
       "        [[-0.0666243 , -0.03146023,  0.00932707],\n",
       "         [ 0.06998824,  0.07292508, -0.00777089],\n",
       "         [ 0.03386825,  0.07576936,  0.00975347]],\n",
       " \n",
       "        [[ 0.05152125,  0.02133656,  0.07011867],\n",
       "         [-0.04325166,  0.07042648,  0.04791193],\n",
       "         [-0.02087972, -0.03227334, -0.01704826]],\n",
       " \n",
       "        [[ 0.03919864, -0.04856344,  0.04059264],\n",
       "         [ 0.00966324, -0.01017948,  0.01373351],\n",
       "         [ 0.04773908,  0.05350245,  0.05076286]],\n",
       " \n",
       "        [[-0.00392971, -0.06028283,  0.08155264],\n",
       "         [-0.03631301, -0.0509093 ,  0.02926374],\n",
       "         [ 0.05501889,  0.00649363,  0.04950883]],\n",
       " \n",
       "        [[ 0.06028026,  0.01696139, -0.05430032],\n",
       "         [-0.05604727, -0.00380254, -0.0046769 ],\n",
       "         [ 0.00125241, -0.02180546, -0.02044095]],\n",
       " \n",
       "        [[-0.05904518, -0.04587621, -0.06537232],\n",
       "         [ 0.04356999,  0.0177008 ,  0.05429792],\n",
       "         [-0.04445187,  0.01931992, -0.0642314 ]],\n",
       " \n",
       "        [[-0.01996542,  0.00915757,  0.03869886],\n",
       "         [ 0.06604532, -0.07543087, -0.07191876],\n",
       "         [-0.04197856, -0.04132207, -0.04069244]],\n",
       " \n",
       "        [[ 0.0592385 , -0.04251679, -0.05917618],\n",
       "         [ 0.03955534,  0.04376284,  0.05011318],\n",
       "         [ 0.00056877,  0.02618863,  0.00423507]],\n",
       " \n",
       "        [[-0.05881691, -0.00989911,  0.06784989],\n",
       "         [ 0.08022913, -0.06122968,  0.06558571],\n",
       "         [ 0.02394742, -0.01772938,  0.00919744]],\n",
       " \n",
       "        [[-0.04311017, -0.05606233,  0.06236789],\n",
       "         [ 0.0410652 ,  0.04992596,  0.0736873 ],\n",
       "         [ 0.00177243,  0.0745092 ,  0.0095392 ]],\n",
       " \n",
       "        [[ 0.06268676, -0.02161758, -0.01398774],\n",
       "         [ 0.04944635, -0.0188677 ,  0.0217075 ],\n",
       "         [-0.04794481, -0.07739106,  0.00338341]],\n",
       " \n",
       "        [[-0.04647095, -0.01713281, -0.00921564],\n",
       "         [ 0.03872033, -0.07708082,  0.07104517],\n",
       "         [ 0.00935647, -0.05881865,  0.07797775]],\n",
       " \n",
       "        [[-0.06910834,  0.00805168,  0.06291118],\n",
       "         [-0.00793771, -0.00939272,  0.08271356],\n",
       "         [ 0.04061438, -0.05351174, -0.01107187]],\n",
       " \n",
       "        [[-0.06927816,  0.00333934,  0.01389737],\n",
       "         [ 0.0295042 , -0.00541695,  0.01220421],\n",
       "         [-0.04237549,  0.04833415,  0.02972678]],\n",
       " \n",
       "        [[ 0.02067692,  0.03428459,  0.01120657],\n",
       "         [ 0.04492542, -0.0198425 ,  0.02439662],\n",
       "         [ 0.08030743, -0.04417688,  0.00029219]],\n",
       " \n",
       "        [[ 0.01355359,  0.07579315,  0.05529561],\n",
       "         [-0.05729377, -0.06773235, -0.08154852],\n",
       "         [ 0.04438122, -0.03479061, -0.01235227]],\n",
       " \n",
       "        [[-0.04784312, -0.03798054, -0.01913409],\n",
       "         [ 0.02745963, -0.01592602, -0.06406123],\n",
       "         [-0.00684103, -0.07661748,  0.06118367]],\n",
       " \n",
       "        [[-0.01881612,  0.00434571,  0.04190767],\n",
       "         [-0.00972014, -0.07100512,  0.07517055],\n",
       "         [ 0.05653076, -0.06122005, -0.05015066]],\n",
       " \n",
       "        [[ 0.01972144,  0.07138082, -0.04051914],\n",
       "         [ 0.04519947,  0.0541953 , -0.05656109],\n",
       "         [ 0.07466775, -0.02530784, -0.02510701]]], dtype=float32), bias=0.0013302437, layer=1, neuron_number=44, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.06454464,  0.00284999, -0.04842773],\n",
       "         [-0.07664821, -0.03301697, -0.01551386],\n",
       "         [ 0.01790439, -0.00795935, -0.04329797]],\n",
       " \n",
       "        [[ 0.05749065, -0.06674677,  0.02080936],\n",
       "         [-0.07629206, -0.07711455, -0.02146739],\n",
       "         [-0.04670469,  0.02154761,  0.0193891 ]],\n",
       " \n",
       "        [[ 0.03565615,  0.00581555, -0.05226278],\n",
       "         [ 0.0288519 , -0.02888368,  0.00312106],\n",
       "         [ 0.07292003,  0.07653835, -0.02568916]],\n",
       " \n",
       "        [[ 0.02028986,  0.02728497, -0.04476526],\n",
       "         [-0.02125124, -0.02081634, -0.0187752 ],\n",
       "         [ 0.06993402,  0.04904404,  0.05631001]],\n",
       " \n",
       "        [[ 0.05000831, -0.05455612, -0.02387378],\n",
       "         [ 0.06142965, -0.03439605,  0.06102857],\n",
       "         [ 0.08076557,  0.04195698, -0.0198424 ]],\n",
       " \n",
       "        [[ 0.07952806, -0.02653782, -0.01493498],\n",
       "         [-0.06716637,  0.05746622, -0.0690382 ],\n",
       "         [ 0.05450581,  0.07190052,  0.05482538]],\n",
       " \n",
       "        [[ 0.0422372 ,  0.05369619, -0.03622574],\n",
       "         [-0.05758072,  0.03088165,  0.02091391],\n",
       "         [ 0.05414046,  0.01301414, -0.03866196]],\n",
       " \n",
       "        [[-0.04520787, -0.03003269, -0.01975892],\n",
       "         [ 0.04171665, -0.0124129 ,  0.07624393],\n",
       "         [-0.03115341,  0.01202934,  0.07076643]],\n",
       " \n",
       "        [[ 0.0428775 , -0.03748862, -0.04396063],\n",
       "         [-0.01474696,  0.00233159,  0.04075868],\n",
       "         [ 0.01875757,  0.03945633,  0.00732323]],\n",
       " \n",
       "        [[ 0.01951073, -0.03226211, -0.04573465],\n",
       "         [ 0.05073101, -0.01021968, -0.05281891],\n",
       "         [ 0.02338868,  0.06063673,  0.02889646]],\n",
       " \n",
       "        [[ 0.03972927, -0.07003665, -0.06704435],\n",
       "         [-0.02346517, -0.06251854,  0.0464413 ],\n",
       "         [-0.06686809, -0.05096262,  0.052189  ]],\n",
       " \n",
       "        [[ 0.02307453,  0.0272346 ,  0.06999757],\n",
       "         [-0.01433338, -0.01298123, -0.06227903],\n",
       "         [ 0.06747622, -0.02858475, -0.03848859]],\n",
       " \n",
       "        [[ 0.06747124, -0.04227024, -0.0335298 ],\n",
       "         [ 0.07786673, -0.03930368,  0.04956227],\n",
       "         [ 0.05066265, -0.07113297,  0.02600908]],\n",
       " \n",
       "        [[ 0.03073571, -0.05369147, -0.01478257],\n",
       "         [ 0.04304235,  0.08231302, -0.06999717],\n",
       "         [-0.08136807,  0.0568596 ,  0.0557812 ]],\n",
       " \n",
       "        [[-0.06308064, -0.08195318,  0.04680472],\n",
       "         [-0.05437588, -0.07991593,  0.08013616],\n",
       "         [-0.06580728, -0.0413019 ,  0.00788335]],\n",
       " \n",
       "        [[-0.07667484, -0.03798948,  0.07332413],\n",
       "         [ 0.02910877, -0.07620484, -0.02785951],\n",
       "         [ 0.06520766, -0.01795206, -0.04308394]],\n",
       " \n",
       "        [[-0.05016496,  0.04528372, -0.07167577],\n",
       "         [-0.04679557,  0.0322326 , -0.05524226],\n",
       "         [-0.02942958,  0.0179365 , -0.01989777]],\n",
       " \n",
       "        [[-0.0409505 ,  0.07405481, -0.06882527],\n",
       "         [-0.06061251,  0.05834265, -0.00434261],\n",
       "         [ 0.04633142,  0.03784562,  0.01791214]],\n",
       " \n",
       "        [[-0.01684854,  0.06425584,  0.03269408],\n",
       "         [-0.02878343,  0.02307169, -0.0797128 ],\n",
       "         [ 0.00697656, -0.02424278, -0.05110058]],\n",
       " \n",
       "        [[-0.04445107,  0.02398925, -0.04678392],\n",
       "         [-0.00204089,  0.0480689 ,  0.04890655],\n",
       "         [ 0.04742601, -0.00260963,  0.01316853]],\n",
       " \n",
       "        [[ 0.0311352 ,  0.02481757, -0.03393856],\n",
       "         [ 0.04048301,  0.01627874, -0.06538606],\n",
       "         [ 0.07847206,  0.04168635, -0.05282368]],\n",
       " \n",
       "        [[-0.07974416,  0.02192196, -0.06159441],\n",
       "         [ 0.01148793,  0.06798366,  0.05082612],\n",
       "         [ 0.00718777, -0.07296992, -0.00155958]],\n",
       " \n",
       "        [[-0.02718389,  0.010076  , -0.02179994],\n",
       "         [ 0.07711975, -0.05396309,  0.0327669 ],\n",
       "         [ 0.05648071,  0.05572821, -0.06392303]],\n",
       " \n",
       "        [[-0.04369583, -0.00851011, -0.02625846],\n",
       "         [-0.06954957,  0.02828603,  0.05147039],\n",
       "         [ 0.07633512, -0.02272173, -0.01692457]],\n",
       " \n",
       "        [[ 0.03346886,  0.06404455, -0.07785024],\n",
       "         [-0.0490373 ,  0.00241016, -0.01858678],\n",
       "         [-0.04407069, -0.07894675, -0.03239213]],\n",
       " \n",
       "        [[-0.00698673,  0.04032857, -0.03778921],\n",
       "         [-0.06681521,  0.00466162,  0.01243454],\n",
       "         [-0.02778275,  0.05460182, -0.04244039]],\n",
       " \n",
       "        [[-0.03642618, -0.05833307, -0.04506953],\n",
       "         [-0.03199939, -0.01171666,  0.02018673],\n",
       "         [-0.06962661, -0.03709473,  0.02051375]],\n",
       " \n",
       "        [[ 0.03544734,  0.04257128, -0.05834676],\n",
       "         [-0.02710794,  0.03910359, -0.05161111],\n",
       "         [ 0.08208111, -0.01403707,  0.03255783]],\n",
       " \n",
       "        [[-0.05026012, -0.01149972,  0.06896953],\n",
       "         [ 0.01354298,  0.07662021,  0.05570202],\n",
       "         [-0.02184012, -0.023754  , -0.05966431]],\n",
       " \n",
       "        [[-0.06206441,  0.01318796, -0.04300551],\n",
       "         [ 0.00179404, -0.00402328,  0.08328066],\n",
       "         [-0.02131749,  0.00829888,  0.07567278]],\n",
       " \n",
       "        [[-0.02802729,  0.01165957, -0.07805031],\n",
       "         [ 0.04623453, -0.01141432,  0.01193701],\n",
       "         [-0.05609057, -0.04943991, -0.06910651]],\n",
       " \n",
       "        [[-0.03784803, -0.05570461, -0.02047396],\n",
       "         [ 0.05612607, -0.03169839,  0.05767311],\n",
       "         [-0.05789346,  0.03754737, -0.062673  ]]], dtype=float32), bias=-0.00057647243, layer=1, neuron_number=45, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.06836259, -0.03072008, -0.03621769],\n",
       "         [ 0.00041001, -0.06981903, -0.01230023],\n",
       "         [ 0.02871725, -0.06170142,  0.0613878 ]],\n",
       " \n",
       "        [[ 0.06846549,  0.00911   , -0.05333045],\n",
       "         [ 0.0231342 ,  0.02556914, -0.05428413],\n",
       "         [ 0.0439542 , -0.00516698, -0.0424375 ]],\n",
       " \n",
       "        [[ 0.00309653, -0.02116567, -0.04198884],\n",
       "         [-0.03462293, -0.03896617,  0.06024475],\n",
       "         [ 0.07838383, -0.00157222, -0.03149185]],\n",
       " \n",
       "        [[-0.02757259, -0.05982505, -0.06013792],\n",
       "         [ 0.04490173,  0.05834831, -0.01605712],\n",
       "         [-0.0433239 , -0.00397601, -0.05614585]],\n",
       " \n",
       "        [[ 0.06674366, -0.03093981,  0.02603354],\n",
       "         [ 0.067004  ,  0.0450015 ,  0.05436262],\n",
       "         [-0.07686599, -0.00835433, -0.01285391]],\n",
       " \n",
       "        [[-0.00187904, -0.0449209 ,  0.06224791],\n",
       "         [ 0.07541882, -0.06217958, -0.01545377],\n",
       "         [ 0.08023463,  0.00605208,  0.01149154]],\n",
       " \n",
       "        [[ 0.07115812, -0.01865378,  0.08121572],\n",
       "         [ 0.04416861,  0.05008268, -0.06303807],\n",
       "         [-0.0736919 , -0.07495657,  0.06174615]],\n",
       " \n",
       "        [[-0.0766431 , -0.02075123, -0.04069714],\n",
       "         [-0.00450087, -0.03183408,  0.03696627],\n",
       "         [ 0.01309384,  0.03479811, -0.01495037]],\n",
       " \n",
       "        [[ 0.05336011,  0.01220511, -0.03930394],\n",
       "         [-0.02604409,  0.00638402,  0.03287096],\n",
       "         [ 0.00806132,  0.00721995, -0.06210101]],\n",
       " \n",
       "        [[ 0.02937964,  0.00825534, -0.02719084],\n",
       "         [-0.00655707, -0.00913645,  0.0156991 ],\n",
       "         [ 0.0249225 ,  0.07536327,  0.07296942]],\n",
       " \n",
       "        [[-0.02531579, -0.00582184, -0.06071159],\n",
       "         [-0.00758447,  0.03135369, -0.054237  ],\n",
       "         [-0.03666708,  0.00114312, -0.03686173]],\n",
       " \n",
       "        [[-0.0334964 ,  0.0500978 , -0.06736407],\n",
       "         [-0.00957274, -0.05367791, -0.02333946],\n",
       "         [ 0.07203931, -0.02050019, -0.05291744]],\n",
       " \n",
       "        [[-0.02166014,  0.01041777,  0.01266139],\n",
       "         [-0.02411172,  0.05271982,  0.07121786],\n",
       "         [ 0.00656143,  0.00337917,  0.08081327]],\n",
       " \n",
       "        [[ 0.07652003, -0.02349727, -0.00025986],\n",
       "         [-0.05575598,  0.0823937 ,  0.07011906],\n",
       "         [ 0.00796142, -0.00542224, -0.07187114]],\n",
       " \n",
       "        [[ 0.05864893,  0.03350469, -0.07451018],\n",
       "         [ 0.00523846, -0.0158944 ,  0.03430646],\n",
       "         [ 0.05035071,  0.02236518,  0.03482547]],\n",
       " \n",
       "        [[-0.00071829,  0.058117  , -0.01810338],\n",
       "         [ 0.01711744,  0.07905648, -0.04541568],\n",
       "         [-0.02364381,  0.00839819,  0.01501589]],\n",
       " \n",
       "        [[-0.00938272,  0.02142549,  0.01282425],\n",
       "         [-0.02249244,  0.07643166, -0.07969105],\n",
       "         [-0.04589267,  0.06252147,  0.04668242]],\n",
       " \n",
       "        [[ 0.01673073,  0.06803969,  0.08178919],\n",
       "         [-0.07722569, -0.08122419, -0.04520699],\n",
       "         [-0.01917526, -0.06671739, -0.02347206]],\n",
       " \n",
       "        [[-0.05090291, -0.07488588,  0.00395563],\n",
       "         [-0.00862088, -0.03909974,  0.05387815],\n",
       "         [ 0.04954051, -0.02760416,  0.04034705]],\n",
       " \n",
       "        [[-0.03145672, -0.06968649,  0.05251949],\n",
       "         [-0.028411  ,  0.04578841, -0.01430826],\n",
       "         [-0.04730409, -0.01027536,  0.0605731 ]],\n",
       " \n",
       "        [[-0.07842557, -0.01321279,  0.00109963],\n",
       "         [ 0.07535994, -0.01876898,  0.03461165],\n",
       "         [-0.03011673,  0.0004144 , -0.01490891]],\n",
       " \n",
       "        [[ 0.01340644, -0.02910149,  0.0129606 ],\n",
       "         [ 0.02473617, -0.02193726,  0.04440581],\n",
       "         [ 0.05619077, -0.0547801 ,  0.03333387]],\n",
       " \n",
       "        [[ 0.02700795,  0.03314628, -0.07786485],\n",
       "         [ 0.01125711,  0.04144327, -0.00749356],\n",
       "         [ 0.05313023, -0.03797314,  0.08145035]],\n",
       " \n",
       "        [[ 0.03165581,  0.05441015, -0.02805925],\n",
       "         [-0.01658226, -0.07878313,  0.0121637 ],\n",
       "         [-0.01362141,  0.05860634, -0.03579972]],\n",
       " \n",
       "        [[ 0.01180164, -0.05798581, -0.03732907],\n",
       "         [-0.06112914, -0.01993489,  0.05365691],\n",
       "         [ 0.0225579 ,  0.07934359, -0.02473944]],\n",
       " \n",
       "        [[-0.06158028, -0.01195899,  0.01711313],\n",
       "         [-0.00090081,  0.06849594, -0.03164547],\n",
       "         [ 0.06566206, -0.00547149,  0.0724375 ]],\n",
       " \n",
       "        [[-0.06304277,  0.02942083, -0.04569189],\n",
       "         [ 0.03994562,  0.03757455,  0.05101356],\n",
       "         [-0.06085576, -0.04242714,  0.0565876 ]],\n",
       " \n",
       "        [[-0.01285951, -0.03925617, -0.06232255],\n",
       "         [ 0.02557995, -0.02247641, -0.06387433],\n",
       "         [-0.04393574, -0.04480798,  0.05894073]],\n",
       " \n",
       "        [[ 0.06071312, -0.03995798, -0.04430583],\n",
       "         [ 0.04404768, -0.02978443, -0.03478975],\n",
       "         [-0.00188772,  0.05296919,  0.00773994]],\n",
       " \n",
       "        [[ 0.0391631 ,  0.07275574,  0.01766861],\n",
       "         [-0.02197721,  0.00446241, -0.00124915],\n",
       "         [-0.08104259,  0.01968758, -0.04328928]],\n",
       " \n",
       "        [[-0.04112884, -0.03004203, -0.04237068],\n",
       "         [ 0.04389464,  0.08439244, -0.07050504],\n",
       "         [ 0.07402092, -0.03889496, -0.01400203]],\n",
       " \n",
       "        [[ 0.03492778,  0.02130684, -0.01892905],\n",
       "         [ 0.01660068,  0.04338093, -0.02313497],\n",
       "         [-0.02276873,  0.02381667, -0.00079413]]], dtype=float32), bias=0.0017051597, layer=1, neuron_number=46, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.0169953 ,  0.05395077,  0.01723859],\n",
       "         [ 0.04163162, -0.0035477 , -0.05188439],\n",
       "         [-0.05024109,  0.0339816 , -0.00063633]],\n",
       " \n",
       "        [[ 0.02569473, -0.02515759, -0.01640842],\n",
       "         [-0.06473076,  0.00628882, -0.01827998],\n",
       "         [ 0.05582244, -0.03233822, -0.04811195]],\n",
       " \n",
       "        [[ 0.07316033, -0.00998316,  0.02296053],\n",
       "         [-0.07114108, -0.01504145, -0.06781593],\n",
       "         [ 0.03996281,  0.02296991, -0.05015485]],\n",
       " \n",
       "        [[-0.05166087,  0.04328894,  0.08109009],\n",
       "         [ 0.00646064,  0.04978722, -0.03777854],\n",
       "         [ 0.00782514, -0.01974536,  0.08166807]],\n",
       " \n",
       "        [[-0.01174609, -0.00308052,  0.05192914],\n",
       "         [ 0.05557287, -0.01913034,  0.00940478],\n",
       "         [-0.06272385,  0.0121998 , -0.06872062]],\n",
       " \n",
       "        [[-0.06879748, -0.05809573,  0.06554803],\n",
       "         [ 0.07593373,  0.01309679, -0.05227558],\n",
       "         [-0.02048586, -0.08170868, -0.0662977 ]],\n",
       " \n",
       "        [[ 0.08069582, -0.01268686, -0.0053278 ],\n",
       "         [ 0.08134151, -0.00736147,  0.05077546],\n",
       "         [ 0.02677958, -0.07039877,  0.0620225 ]],\n",
       " \n",
       "        [[-0.05207095,  0.00857319, -0.05017244],\n",
       "         [ 0.06672714,  0.01611996,  0.05443709],\n",
       "         [-0.01588913, -0.0149872 ,  0.06373785]],\n",
       " \n",
       "        [[-0.00278667,  0.00896314, -0.04049799],\n",
       "         [ 0.02666483, -0.04785021, -0.07955086],\n",
       "         [-0.06117048, -0.04541089, -0.07911694]],\n",
       " \n",
       "        [[-0.02353822,  0.05806493,  0.00838314],\n",
       "         [ 0.04322795,  0.05987256, -0.05456365],\n",
       "         [ 0.07900357, -0.02802555, -0.06331684]],\n",
       " \n",
       "        [[-0.03378082,  0.02924618, -0.07492065],\n",
       "         [ 0.00581223,  0.0735037 ,  0.05909505],\n",
       "         [ 0.01350991, -0.02360115,  0.04854508]],\n",
       " \n",
       "        [[-0.06916649, -0.00831559,  0.01420427],\n",
       "         [-0.03418885,  0.04374759, -0.05395058],\n",
       "         [-0.08245195, -0.00047956,  0.04916518]],\n",
       " \n",
       "        [[-0.07960245, -0.06335617,  0.04821879],\n",
       "         [-0.08177849, -0.062884  , -0.0309118 ],\n",
       "         [-0.05109557,  0.07764639, -0.00677709]],\n",
       " \n",
       "        [[-0.04816496,  0.00915068, -0.05619852],\n",
       "         [ 0.07432301,  0.05025675, -0.08001531],\n",
       "         [-0.07754833, -0.04151684,  0.00315376]],\n",
       " \n",
       "        [[-0.06696062,  0.07503224,  0.00069105],\n",
       "         [-0.00766861, -0.05027646,  0.06831458],\n",
       "         [-0.01782376,  0.05318989, -0.02697864]],\n",
       " \n",
       "        [[ 0.06559055,  0.05740883,  0.06019464],\n",
       "         [ 0.04388312, -0.07275122,  0.07926948],\n",
       "         [-0.08148751,  0.08144625,  0.06015288]],\n",
       " \n",
       "        [[ 0.02845373, -0.03321671, -0.02984127],\n",
       "         [ 0.06487539, -0.04032559, -0.04990179],\n",
       "         [ 0.07048296,  0.04037396,  0.03588996]],\n",
       " \n",
       "        [[-0.04121488,  0.01886462, -0.00531683],\n",
       "         [-0.06559534,  0.05867635, -0.02185974],\n",
       "         [-0.08103146,  0.03035655, -0.05377094]],\n",
       " \n",
       "        [[ 0.00558922, -0.05267207,  0.05513382],\n",
       "         [ 0.04830737,  0.05598845, -0.04742138],\n",
       "         [ 0.03705928,  0.00326864,  0.08099471]],\n",
       " \n",
       "        [[-0.02087801,  0.05932462,  0.00825377],\n",
       "         [-0.07883897,  0.01331228,  0.01495012],\n",
       "         [-0.05786643, -0.04746243,  0.03102759]],\n",
       " \n",
       "        [[ 0.02482715,  0.07174215, -0.02465306],\n",
       "         [-0.06901385, -0.0180343 , -0.0271527 ],\n",
       "         [ 0.05711513, -0.06050932,  0.02964862]],\n",
       " \n",
       "        [[ 0.06842554, -0.0366203 , -0.03307135],\n",
       "         [ 0.01104636,  0.0604606 ,  0.05021619],\n",
       "         [ 0.06940544, -0.06110012,  0.00861999]],\n",
       " \n",
       "        [[-0.07383045,  0.01139844, -0.06533105],\n",
       "         [-0.04995739,  0.05736265, -0.01885742],\n",
       "         [-0.02291898,  0.07797252, -0.08294891]],\n",
       " \n",
       "        [[-0.03707688,  0.03309023,  0.00405947],\n",
       "         [-0.05057122, -0.02824925, -0.08045956],\n",
       "         [ 0.04710513,  0.01344219,  0.06361572]],\n",
       " \n",
       "        [[-0.08054181,  0.05741944,  0.01881201],\n",
       "         [ 0.03306207, -0.00238789, -0.00034212],\n",
       "         [ 0.05022798,  0.00625883, -0.05865592]],\n",
       " \n",
       "        [[-0.05896543,  0.00481236, -0.02620406],\n",
       "         [-0.04606685,  0.0518459 ,  0.07294522],\n",
       "         [-0.00400549, -0.01748535, -0.01926226]],\n",
       " \n",
       "        [[-0.06601463,  0.03684014,  0.03443537],\n",
       "         [ 0.03715798, -0.0567554 , -0.05864538],\n",
       "         [ 0.00307552, -0.03025266,  0.00634775]],\n",
       " \n",
       "        [[-0.02811752, -0.07468558,  0.06383548],\n",
       "         [-0.06189837,  0.04917037, -0.06878902],\n",
       "         [ 0.04161125,  0.02962064,  0.03421589]],\n",
       " \n",
       "        [[-0.03543581,  0.04208602, -0.0583733 ],\n",
       "         [ 0.03849418, -0.03938303,  0.05596938],\n",
       "         [-0.00851818, -0.02103865,  0.05253246]],\n",
       " \n",
       "        [[ 0.05621929,  0.0394867 , -0.05500965],\n",
       "         [ 0.01343082, -0.03096524,  0.02901745],\n",
       "         [ 0.06644655, -0.06612847, -0.01357138]],\n",
       " \n",
       "        [[-0.05040608,  0.07979051,  0.04802955],\n",
       "         [ 0.02396124,  0.01591241, -0.01958683],\n",
       "         [-0.08066978, -0.0756588 ,  0.03939468]],\n",
       " \n",
       "        [[ 0.07504578, -0.0176596 ,  0.05165803],\n",
       "         [ 0.02532873, -0.04627306, -0.00920993],\n",
       "         [ 0.08011513, -0.03831171,  0.06349123]]], dtype=float32), bias=3.6229671e-05, layer=1, neuron_number=47, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.03749766,  0.07010435,  0.08178068],\n",
       "         [-0.05532425,  0.02954443,  0.00921002],\n",
       "         [-0.0718355 , -0.07211382, -0.03166178]],\n",
       " \n",
       "        [[ 0.04580483,  0.06334082,  0.02179245],\n",
       "         [ 0.0589414 ,  0.00180684,  0.0344645 ],\n",
       "         [ 0.05541171,  0.01769043,  0.0701073 ]],\n",
       " \n",
       "        [[-0.07932875,  0.07991102,  0.01710478],\n",
       "         [ 0.03293518,  0.03355698, -0.07288145],\n",
       "         [ 0.01900689, -0.08046342,  0.07026034]],\n",
       " \n",
       "        [[ 0.08159085,  0.0694904 , -0.06612641],\n",
       "         [-0.03942245, -0.03409864, -0.02231361],\n",
       "         [ 0.0154551 , -0.07345574, -0.06219973]],\n",
       " \n",
       "        [[ 0.07711247,  0.03375658, -0.05690345],\n",
       "         [-0.00370695, -0.02285325, -0.06990147],\n",
       "         [ 0.03021381,  0.06544827,  0.00070694]],\n",
       " \n",
       "        [[-0.01058526,  0.05529231, -0.03386574],\n",
       "         [ 0.01685636, -0.00743693, -0.07438701],\n",
       "         [ 0.01462549,  0.04703466,  0.0516115 ]],\n",
       " \n",
       "        [[-0.08019272, -0.03594492, -0.02827799],\n",
       "         [-0.04995234,  0.00373398,  0.07071722],\n",
       "         [ 0.06828013,  0.0355533 ,  0.07671508]],\n",
       " \n",
       "        [[-0.05866584,  0.00242594,  0.0115321 ],\n",
       "         [ 0.07696901,  0.04238913,  0.00329977],\n",
       "         [ 0.02251207,  0.00110664,  0.05224527]],\n",
       " \n",
       "        [[-0.06744128,  0.06563611,  0.01178654],\n",
       "         [-0.0798877 ,  0.02582108,  0.0434127 ],\n",
       "         [ 0.01384624,  0.02859873,  0.0723123 ]],\n",
       " \n",
       "        [[-0.05625083, -0.01696892,  0.03490421],\n",
       "         [-0.07536877,  0.0722608 , -0.06129709],\n",
       "         [-0.05234145, -0.03200667, -0.05848598]],\n",
       " \n",
       "        [[ 0.01274656, -0.01242493,  0.04943505],\n",
       "         [-0.08058336,  0.02627449, -0.06601135],\n",
       "         [ 0.01782505,  0.06668573,  0.05821367]],\n",
       " \n",
       "        [[ 0.07894486, -0.05678563,  0.00476488],\n",
       "         [ 0.05449525, -0.02744422, -0.01703586],\n",
       "         [ 0.07796255,  0.00285   ,  0.03942315]],\n",
       " \n",
       "        [[ 0.00991009, -0.07688675,  0.01386265],\n",
       "         [ 0.00081195, -0.06191732,  0.0698266 ],\n",
       "         [ 0.05846674, -0.03269026,  0.05310921]],\n",
       " \n",
       "        [[ 0.04395592,  0.00511826, -0.04749647],\n",
       "         [-0.05722427,  0.04705399, -0.05037134],\n",
       "         [-0.02691045,  0.0526039 , -0.03292535]],\n",
       " \n",
       "        [[-0.02165204,  0.04660672,  0.02888714],\n",
       "         [ 0.02295459,  0.0424169 , -0.07057966],\n",
       "         [-0.04365714,  0.01374993, -0.01821741]],\n",
       " \n",
       "        [[ 0.06094637,  0.0581027 ,  0.00957228],\n",
       "         [ 0.07482722, -0.0334611 , -0.05300749],\n",
       "         [ 0.06841429, -0.08094962,  0.02081193]],\n",
       " \n",
       "        [[-0.05033856,  0.02357196,  0.00222829],\n",
       "         [-0.03737665,  0.02743752, -0.02343225],\n",
       "         [-0.04090584,  0.00219304,  0.01826217]],\n",
       " \n",
       "        [[ 0.01985634,  0.03654238, -0.07918845],\n",
       "         [-0.07560779, -0.00422079,  0.05661746],\n",
       "         [-0.05919191,  0.055783  ,  0.02113573]],\n",
       " \n",
       "        [[-0.06967267, -0.03068867,  0.0617263 ],\n",
       "         [-0.05451514, -0.0702799 ,  0.07375006],\n",
       "         [ 0.06856097,  0.05798574, -0.04263126]],\n",
       " \n",
       "        [[-0.04865639, -0.04234984, -0.05520211],\n",
       "         [ 0.04311939,  0.01809753, -0.07031128],\n",
       "         [-0.0059517 , -0.03321696, -0.07268301]],\n",
       " \n",
       "        [[ 0.02199598,  0.07677877,  0.04056052],\n",
       "         [-0.04352093,  0.00380472,  0.0199353 ],\n",
       "         [-0.07139324, -0.04109208, -0.04627388]],\n",
       " \n",
       "        [[ 0.0159252 ,  0.06551036, -0.05924214],\n",
       "         [ 0.01644593, -0.06640019, -0.02728729],\n",
       "         [ 0.03777086,  0.03766948, -0.03678813]],\n",
       " \n",
       "        [[ 0.02991949, -0.0342021 ,  0.08210978],\n",
       "         [ 0.07302491,  0.01416525, -0.01132447],\n",
       "         [ 0.07512783,  0.05157515,  0.00179621]],\n",
       " \n",
       "        [[ 0.04470289,  0.04959356,  0.02542762],\n",
       "         [ 0.04402738,  0.06161395, -0.02498238],\n",
       "         [ 0.0543493 , -0.03941526, -0.00464673]],\n",
       " \n",
       "        [[-0.07412831,  0.03729751,  0.07253122],\n",
       "         [-0.01145442, -0.00869736,  0.07937853],\n",
       "         [-0.03275907,  0.06769959,  0.02235072]],\n",
       " \n",
       "        [[ 0.01107835, -0.08015853, -0.0183771 ],\n",
       "         [-0.06146586,  0.03316033, -0.06373645],\n",
       "         [ 0.07799893,  0.04045412,  0.06521846]],\n",
       " \n",
       "        [[ 0.06216856,  0.04511522, -0.04487113],\n",
       "         [ 0.0613343 ,  0.059082  , -0.07092955],\n",
       "         [-0.07617752, -0.01183636, -0.06402113]],\n",
       " \n",
       "        [[ 0.08154605, -0.07131283, -0.04390999],\n",
       "         [-0.0664444 , -0.03268463,  0.05198716],\n",
       "         [ 0.0048829 , -0.04317455, -0.0526896 ]],\n",
       " \n",
       "        [[ 0.0692699 ,  0.07725255,  0.0499853 ],\n",
       "         [ 0.04424855, -0.03498811, -0.02292291],\n",
       "         [-0.04475389,  0.00157855, -0.0248288 ]],\n",
       " \n",
       "        [[ 0.01782556, -0.00329299, -0.02144475],\n",
       "         [ 0.0336189 ,  0.08202022, -0.06066856],\n",
       "         [-0.03186147,  0.01490281, -0.067571  ]],\n",
       " \n",
       "        [[-0.01924445,  0.07700305, -0.076667  ],\n",
       "         [ 0.04529562, -0.00283163,  0.00429948],\n",
       "         [ 0.00039014,  0.06558509,  0.08401936]],\n",
       " \n",
       "        [[ 0.07812063,  0.06224722,  0.03196345],\n",
       "         [ 0.05898558,  0.07078731,  0.02372609],\n",
       "         [-0.05115112, -0.02333998,  0.04224078]]], dtype=float32), bias=-0.00013102806, layer=1, neuron_number=48, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.00812443, -0.07659955, -0.01442751],\n",
       "         [-0.00155198,  0.06674521, -0.07137565],\n",
       "         [ 0.07508095,  0.02767272, -0.06239096]],\n",
       " \n",
       "        [[-0.02048886, -0.02780795,  0.07841423],\n",
       "         [ 0.01081787,  0.00941637,  0.02015708],\n",
       "         [-0.01564027, -0.03260082, -0.03201108]],\n",
       " \n",
       "        [[ 0.02514849,  0.08173793, -0.00063659],\n",
       "         [-0.02863382, -0.01075077, -0.07226168],\n",
       "         [ 0.04488044, -0.02786202,  0.03594414]],\n",
       " \n",
       "        [[-0.05310533, -0.02775294, -0.05708298],\n",
       "         [ 0.02145356, -0.03559671,  0.07168852],\n",
       "         [-0.03071655, -0.03587238,  0.03805681]],\n",
       " \n",
       "        [[-0.08159757,  0.03835507,  0.0309093 ],\n",
       "         [ 0.05891419, -0.05724001,  0.00372878],\n",
       "         [ 0.03400492,  0.01886258, -0.01209689]],\n",
       " \n",
       "        [[-0.01115024,  0.0283839 , -0.00215924],\n",
       "         [ 0.04432544,  0.00113044,  0.04735693],\n",
       "         [ 0.06719195, -0.05135418,  0.02061204]],\n",
       " \n",
       "        [[-0.03342843,  0.04705564, -0.00482956],\n",
       "         [-0.01417392, -0.02117205, -0.03992978],\n",
       "         [-0.05811264,  0.07211917,  0.0147425 ]],\n",
       " \n",
       "        [[-0.08069203,  0.01673356, -0.06766989],\n",
       "         [-0.07170441,  0.02306155, -0.06583434],\n",
       "         [-0.04896857,  0.00558115, -0.06890209]],\n",
       " \n",
       "        [[ 0.01643837, -0.01168128, -0.07936554],\n",
       "         [ 0.01430826, -0.07575091,  0.03129831],\n",
       "         [ 0.00044432,  0.04877337,  0.02399108]],\n",
       " \n",
       "        [[ 0.02651841,  0.07630452, -0.06530105],\n",
       "         [ 0.01995968,  0.06021372,  0.04426839],\n",
       "         [ 0.00516492,  0.06289974,  0.00109698]],\n",
       " \n",
       "        [[-0.02386775,  0.04902242,  0.07589668],\n",
       "         [ 0.06282166,  0.0109213 , -0.01328296],\n",
       "         [ 0.03180967,  0.0440292 ,  0.0386422 ]],\n",
       " \n",
       "        [[ 0.04782441,  0.0561632 , -0.03174849],\n",
       "         [ 0.06937995, -0.06768422,  0.02620058],\n",
       "         [ 0.05086394, -0.00722467, -0.00566773]],\n",
       " \n",
       "        [[-0.0217051 , -0.0496379 ,  0.03207673],\n",
       "         [-0.02070936, -0.06272247,  0.01495527],\n",
       "         [-0.01569517,  0.02617528, -0.02401756]],\n",
       " \n",
       "        [[-0.02565252,  0.02225984, -0.00851035],\n",
       "         [-0.02735122,  0.00116315, -0.06853988],\n",
       "         [ 0.00194807, -0.04511976,  0.05288301]],\n",
       " \n",
       "        [[ 0.00419209, -0.03851124, -0.02272833],\n",
       "         [-0.08041854,  0.02805604, -0.07950303],\n",
       "         [ 0.06082831,  0.03660289, -0.0734963 ]],\n",
       " \n",
       "        [[-0.00168598,  0.06189893,  0.01198315],\n",
       "         [-0.04701594, -0.01293863, -0.05877009],\n",
       "         [-0.04606104, -0.0616447 , -0.01574775]],\n",
       " \n",
       "        [[-0.04657973,  0.02669306, -0.05708389],\n",
       "         [ 0.0547522 ,  0.06129853, -0.04099857],\n",
       "         [-0.00969614,  0.05855392,  0.05856333]],\n",
       " \n",
       "        [[-0.0648738 , -0.05971464,  0.07574251],\n",
       "         [ 0.0643072 ,  0.07060245, -0.03481721],\n",
       "         [ 0.04276431,  0.0316574 , -0.06418457]],\n",
       " \n",
       "        [[-0.03454547,  0.038926  ,  0.02824527],\n",
       "         [ 0.07461582, -0.05755342, -0.07063179],\n",
       "         [-0.06129748,  0.01402468, -0.01406647]],\n",
       " \n",
       "        [[-0.01589249,  0.02651696, -0.0470662 ],\n",
       "         [ 0.07385087, -0.04469446, -0.01069416],\n",
       "         [ 0.03896932, -0.00758547,  0.00230462]],\n",
       " \n",
       "        [[-0.05040976, -0.06058944,  0.05587665],\n",
       "         [ 0.04038961,  0.02271567,  0.03092935],\n",
       "         [ 0.07571664,  0.01038096,  0.05413727]],\n",
       " \n",
       "        [[ 0.00057782,  0.00694308, -0.0256517 ],\n",
       "         [ 0.05654632, -0.07525244,  0.06185816],\n",
       "         [ 0.08136458,  0.0067482 , -0.02910447]],\n",
       " \n",
       "        [[ 0.00888166, -0.03783358,  0.06031089],\n",
       "         [-0.04505639, -0.06440653, -0.02514648],\n",
       "         [ 0.07147533,  0.0203304 , -0.00916899]],\n",
       " \n",
       "        [[ 0.02935787,  0.05009444,  0.03815334],\n",
       "         [-0.06794623,  0.03082728,  0.06741418],\n",
       "         [ 0.03205447, -0.05387209,  0.07578602]],\n",
       " \n",
       "        [[ 0.02174138, -0.02544683,  0.01085211],\n",
       "         [-0.05140873,  0.02872902,  0.00305909],\n",
       "         [-0.05598535,  0.03066205,  0.0400387 ]],\n",
       " \n",
       "        [[ 0.03134076,  0.00580902,  0.03720017],\n",
       "         [-0.05761069,  0.0268961 ,  0.05486689],\n",
       "         [-0.05589179, -0.03359722,  0.00780205]],\n",
       " \n",
       "        [[-0.05122967,  0.05501884, -0.04495275],\n",
       "         [-0.02189001, -0.03006239, -0.04052885],\n",
       "         [-0.06336503,  0.0597943 , -0.01852226]],\n",
       " \n",
       "        [[ 0.00501174, -0.07014997,  0.05177236],\n",
       "         [-0.06539328,  0.00402337,  0.00707496],\n",
       "         [-0.01384774, -0.07382493,  0.0251208 ]],\n",
       " \n",
       "        [[-0.05881985,  0.07340331, -0.0566889 ],\n",
       "         [ 0.07595624, -0.0764168 , -0.07972964],\n",
       "         [ 0.06938314, -0.01678905,  0.04114923]],\n",
       " \n",
       "        [[ 0.06892608, -0.06937143, -0.08193363],\n",
       "         [ 0.00842945, -0.04806022, -0.03585533],\n",
       "         [ 0.06194825, -0.08305249,  0.06708848]],\n",
       " \n",
       "        [[-0.0616605 , -0.01027962, -0.04808042],\n",
       "         [ 0.02996638,  0.00542696, -0.06819226],\n",
       "         [ 0.06886762,  0.02620772,  0.05953623]],\n",
       " \n",
       "        [[-0.0652903 , -0.01251053,  0.01329468],\n",
       "         [-0.00287183,  0.0483186 ,  0.07580081],\n",
       "         [ 0.00616032, -0.01029751, -0.01872076]]], dtype=float32), bias=-0.00022495864, layer=1, neuron_number=49, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.02515568, -0.05489329,  0.07317328],\n",
       "         [ 0.01397658,  0.01215396, -0.03125867],\n",
       "         [ 0.02642368, -0.08233526, -0.06660011]],\n",
       " \n",
       "        [[ 0.0240956 , -0.06720984, -0.08296355],\n",
       "         [ 0.07531536, -0.00109074, -0.06348985],\n",
       "         [ 0.07940216, -0.07242136, -0.05957359]],\n",
       " \n",
       "        [[ 0.07129197,  0.00101567, -0.0358857 ],\n",
       "         [ 0.00956841,  0.07731996, -0.05046986],\n",
       "         [-0.06143214, -0.08309866,  0.00650324]],\n",
       " \n",
       "        [[ 0.02387896, -0.02304506, -0.02339411],\n",
       "         [ 0.08150563, -0.07488533,  0.02034208],\n",
       "         [-0.0780414 , -0.03670898, -0.06162615]],\n",
       " \n",
       "        [[-0.03361257, -0.05562225,  0.02206823],\n",
       "         [ 0.03635226,  0.02378434, -0.02869427],\n",
       "         [ 0.03017655,  0.04013292, -0.06955837]],\n",
       " \n",
       "        [[ 0.00580369, -0.04971714,  0.0341504 ],\n",
       "         [-0.0732984 , -0.01328522,  0.06768069],\n",
       "         [ 0.07055617, -0.03981217,  0.03547798]],\n",
       " \n",
       "        [[ 0.03352435, -0.00281165, -0.02910948],\n",
       "         [-0.02203302, -0.02413786, -0.03958227],\n",
       "         [-0.08016413,  0.00151123,  0.05057825]],\n",
       " \n",
       "        [[ 0.05020756, -0.03589677,  0.04838273],\n",
       "         [ 0.08299288,  0.0508203 , -0.01923936],\n",
       "         [-0.04031515, -0.01867875,  0.07159991]],\n",
       " \n",
       "        [[-0.01785883, -0.0684092 ,  0.0348259 ],\n",
       "         [-0.05460538,  0.06794492, -0.06077679],\n",
       "         [-0.06150473,  0.03988946,  0.00526146]],\n",
       " \n",
       "        [[ 0.0700281 , -0.07982228,  0.0240464 ],\n",
       "         [ 0.0092987 , -0.05969498,  0.05128878],\n",
       "         [-0.03093552,  0.06792142, -0.03747733]],\n",
       " \n",
       "        [[ 0.00202584, -0.0077181 ,  0.05998715],\n",
       "         [ 0.04594829,  0.06938651,  0.01397535],\n",
       "         [-0.01975125, -0.06905998, -0.00613306]],\n",
       " \n",
       "        [[-0.06616341, -0.02367006, -0.07441913],\n",
       "         [-0.04526636,  0.07733683, -0.02751256],\n",
       "         [-0.08245931,  0.01328967,  0.01720737]],\n",
       " \n",
       "        [[ 0.0743685 ,  0.00702884,  0.00497145],\n",
       "         [-0.01395801, -0.0223263 , -0.01944466],\n",
       "         [-0.04811262, -0.00477741, -0.07838035]],\n",
       " \n",
       "        [[ 0.04913176,  0.06335437,  0.04127375],\n",
       "         [-0.03975559,  0.00344921,  0.04184319],\n",
       "         [-0.04973848, -0.02583536,  0.05966913]],\n",
       " \n",
       "        [[ 0.06708242,  0.0756126 , -0.00194141],\n",
       "         [-0.04525168,  0.02024151, -0.04453482],\n",
       "         [-0.03310046,  0.03387718, -0.02780138]],\n",
       " \n",
       "        [[-0.03984357,  0.01476507,  0.05841927],\n",
       "         [ 0.06889963,  0.01934781,  0.03193657],\n",
       "         [ 0.00560379,  0.00942534, -0.06886279]],\n",
       " \n",
       "        [[ 0.07169502,  0.00637626, -0.07594642],\n",
       "         [-0.00615905, -0.07976206, -0.08187351],\n",
       "         [-0.04507519, -0.04065428,  0.06832922]],\n",
       " \n",
       "        [[-0.0129722 , -0.00299731,  0.0032935 ],\n",
       "         [ 0.07413776,  0.08099255, -0.00186844],\n",
       "         [ 0.01304088,  0.04859595, -0.07855527]],\n",
       " \n",
       "        [[ 0.04617524, -0.00035379,  0.04716792],\n",
       "         [ 0.01915046,  0.06107969,  0.07445404],\n",
       "         [-0.03912805,  0.08050114, -0.01965385]],\n",
       " \n",
       "        [[-0.05976279,  0.06108303,  0.05665011],\n",
       "         [ 0.00390633,  0.02747131, -0.0021695 ],\n",
       "         [-0.00609984, -0.07585557,  0.02697799]],\n",
       " \n",
       "        [[ 0.02497981,  0.00437439, -0.05843811],\n",
       "         [ 0.06814019,  0.01753641, -0.05690219],\n",
       "         [-0.06728078,  0.0379619 , -0.0409618 ]],\n",
       " \n",
       "        [[-0.05027536,  0.0042301 ,  0.01387294],\n",
       "         [ 0.00740829,  0.00427562, -0.04556933],\n",
       "         [ 0.0632033 , -0.07631921, -0.07455291]],\n",
       " \n",
       "        [[-0.01646778, -0.07258923,  0.01670459],\n",
       "         [ 0.00848931,  0.02496665, -0.04525233],\n",
       "         [-0.06327014,  0.07761082, -0.04488206]],\n",
       " \n",
       "        [[ 0.0437675 , -0.01183513,  0.01391598],\n",
       "         [-0.01381219,  0.02347648, -0.05238859],\n",
       "         [-0.00403571,  0.04030491, -0.05636487]],\n",
       " \n",
       "        [[-0.03810355, -0.08409892,  0.05547927],\n",
       "         [ 0.02176874,  0.04356542,  0.00794696],\n",
       "         [-0.0118098 , -0.07882001,  0.05679177]],\n",
       " \n",
       "        [[-0.06299867,  0.0374752 , -0.03159268],\n",
       "         [-0.01853803,  0.01412314, -0.01334111],\n",
       "         [-0.06234343, -0.01081653,  0.07802361]],\n",
       " \n",
       "        [[ 0.06968257,  0.01172425, -0.02760643],\n",
       "         [-0.05128637, -0.0703321 , -0.07113888],\n",
       "         [-0.05707548, -0.0553445 ,  0.05856222]],\n",
       " \n",
       "        [[ 0.06516226,  0.07905616, -0.06403144],\n",
       "         [-0.05987198,  0.0748695 , -0.07843776],\n",
       "         [-0.04031152,  0.02693933, -0.02246698]],\n",
       " \n",
       "        [[ 0.04979547,  0.03626825, -0.07194819],\n",
       "         [ 0.07601261,  0.00658455, -0.00813963],\n",
       "         [ 0.02236175, -0.01620337, -0.04314896]],\n",
       " \n",
       "        [[-0.01711372, -0.01770289,  0.03453438],\n",
       "         [ 0.02851632,  0.06750683, -0.05819181],\n",
       "         [ 0.02767042, -0.04566594,  0.00927533]],\n",
       " \n",
       "        [[ 0.00588848, -0.00142878,  0.05582784],\n",
       "         [ 0.07561588,  0.01265752,  0.05458556],\n",
       "         [-0.03451861,  0.07357235, -0.0734653 ]],\n",
       " \n",
       "        [[ 0.02744459,  0.02000606, -0.07440016],\n",
       "         [-0.07677164,  0.02626742, -0.05134592],\n",
       "         [-0.03865964, -0.08080264,  0.01156672]]], dtype=float32), bias=-0.0014807894, layer=1, neuron_number=50, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ -7.30192661e-02,   4.85247746e-02,   3.41438651e-02],\n",
       "         [  4.94489372e-02,  -4.83708195e-02,  -2.64309160e-03],\n",
       "         [ -1.75568629e-02,   6.03202172e-03,  -7.06433430e-02]],\n",
       " \n",
       "        [[ -7.56238773e-02,  -6.71645701e-02,   3.04521658e-02],\n",
       "         [ -1.60760991e-03,   7.70574883e-02,   5.48199043e-02],\n",
       "         [  6.24040626e-02,  -2.68215220e-02,   4.26147841e-02]],\n",
       " \n",
       "        [[  4.55303453e-02,   5.05684726e-02,   6.21076301e-02],\n",
       "         [  3.86472754e-02,   6.20210022e-02,  -3.45967859e-02],\n",
       "         [ -2.16874778e-02,   1.34815462e-02,  -4.73080315e-02]],\n",
       " \n",
       "        [[ -2.58320794e-02,   1.78815052e-02,   5.31861149e-02],\n",
       "         [  3.32382601e-03,  -3.27983312e-02,   4.63755876e-02],\n",
       "         [ -5.03758602e-02,   7.69190565e-02,  -6.02285750e-02]],\n",
       " \n",
       "        [[  5.82109913e-02,   2.06266344e-02,  -1.30421603e-02],\n",
       "         [  3.17303144e-04,   5.84453642e-02,  -3.91745791e-02],\n",
       "         [ -5.65018281e-02,  -5.99937625e-02,  -1.75908860e-02]],\n",
       " \n",
       "        [[ -6.56223446e-02,  -1.57789383e-02,  -2.38731261e-02],\n",
       "         [  5.92035539e-02,  -1.80981606e-02,  -1.63190998e-02],\n",
       "         [  6.79966509e-02,   5.28350286e-02,   6.82218149e-02]],\n",
       " \n",
       "        [[  5.37564158e-02,  -8.34634677e-02,  -1.85890123e-02],\n",
       "         [  4.49240915e-02,   4.17737365e-02,   2.93250680e-02],\n",
       "         [  1.24514280e-02,   6.48459494e-02,  -7.02315569e-02]],\n",
       " \n",
       "        [[ -7.56780431e-02,  -4.88543436e-02,  -3.82283633e-03],\n",
       "         [  6.29590601e-02,   1.62466764e-02,  -5.77810630e-02],\n",
       "         [ -2.14688890e-02,  -3.98899540e-02,  -5.09311585e-03]],\n",
       " \n",
       "        [[  1.96373202e-02,   5.72036840e-02,  -1.56149073e-02],\n",
       "         [  6.27808720e-02,  -1.09143844e-02,   3.92182693e-02],\n",
       "         [  5.84801212e-02,  -6.25552237e-02,   3.29763703e-02]],\n",
       " \n",
       "        [[ -3.09368353e-02,  -7.22227246e-02,   9.92194749e-03],\n",
       "         [ -7.06617236e-02,   7.36031830e-02,   1.12715261e-02],\n",
       "         [  2.26927642e-02,  -6.69567659e-02,  -3.60332849e-03]],\n",
       " \n",
       "        [[ -6.50259554e-02,   3.85882333e-02,  -3.99484038e-02],\n",
       "         [  4.32301983e-02,  -5.63585833e-02,   5.37938178e-02],\n",
       "         [  6.33395538e-02,   1.23353852e-02,   7.99503252e-02]],\n",
       " \n",
       "        [[ -7.20412061e-02,   6.76260069e-02,   5.26741929e-02],\n",
       "         [ -7.94032142e-02,   7.69532025e-02,  -4.44425158e-02],\n",
       "         [  7.28227794e-02,  -5.35703041e-02,  -5.85099729e-03]],\n",
       " \n",
       "        [[ -4.93209772e-02,   4.83898334e-02,  -4.32937145e-02],\n",
       "         [  4.79048193e-02,  -7.19732866e-02,   4.90605161e-02],\n",
       "         [ -6.26902133e-02,   6.34645969e-02,   2.85415142e-03]],\n",
       " \n",
       "        [[  9.42915678e-03,   8.13304633e-02,  -9.88490414e-03],\n",
       "         [ -1.99100021e-02,  -7.02785850e-02,   2.87786368e-02],\n",
       "         [  7.01825097e-02,  -1.55656394e-02,  -3.15099023e-02]],\n",
       " \n",
       "        [[  6.70294976e-03,   1.63370534e-03,   3.55878472e-02],\n",
       "         [ -7.13026077e-02,  -6.51153550e-02,  -3.31313945e-02],\n",
       "         [  5.46565875e-02,  -4.51613888e-02,  -7.46111125e-02]],\n",
       " \n",
       "        [[ -3.26811299e-02,  -6.95579126e-02,  -2.16757543e-02],\n",
       "         [  3.79108414e-02,  -7.17229247e-02,  -3.07676475e-02],\n",
       "         [  2.51504593e-02,  -6.02994114e-02,   4.79223505e-02]],\n",
       " \n",
       "        [[ -4.63848449e-02,  -2.66338345e-02,  -6.29260093e-02],\n",
       "         [  5.81852198e-02,  -5.41306362e-02,   7.70409107e-02],\n",
       "         [  3.37109491e-02,   4.57044840e-02,   1.85822919e-02]],\n",
       " \n",
       "        [[  6.79083094e-02,   4.17374000e-02,   5.51779335e-03],\n",
       "         [ -1.17490301e-03,  -3.28840688e-02,   2.34760083e-02],\n",
       "         [  1.07722664e-02,   1.85878333e-02,  -4.48187022e-03]],\n",
       " \n",
       "        [[  7.12809637e-02,   3.27667184e-02,  -6.01746663e-02],\n",
       "         [  6.48126081e-02,  -8.20786282e-02,   3.98362093e-02],\n",
       "         [  7.45755509e-02,   6.33483753e-02,   7.60618970e-02]],\n",
       " \n",
       "        [[  4.53109257e-02,  -5.36477044e-02,   7.28800744e-02],\n",
       "         [  6.20240308e-02,   4.30135056e-02,  -8.17104280e-02],\n",
       "         [  4.42649238e-03,  -5.48697170e-03,  -2.49156076e-02]],\n",
       " \n",
       "        [[ -5.25741205e-02,   8.03071782e-02,   2.97568329e-02],\n",
       "         [  6.09969161e-02,   7.07160458e-02,  -4.64904262e-03],\n",
       "         [ -1.39856443e-03,  -5.00791296e-02,   7.24765435e-02]],\n",
       " \n",
       "        [[  3.95259038e-02,  -6.74230084e-02,   8.18149522e-02],\n",
       "         [ -1.33542251e-02,   5.91773503e-02,   7.47685730e-02],\n",
       "         [ -6.86236918e-02,   6.33688876e-04,  -7.12002441e-02]],\n",
       " \n",
       "        [[ -1.36637194e-02,   6.80324808e-02,   5.14256470e-02],\n",
       "         [ -6.28321916e-02,  -4.88264672e-02,   5.91498762e-02],\n",
       "         [ -7.54257385e-03,   6.31261244e-02,   5.80066666e-02]],\n",
       " \n",
       "        [[  1.48297958e-02,  -6.15065582e-02,  -2.02478450e-02],\n",
       "         [  7.80756846e-02,  -4.85292040e-02,  -2.39381567e-02],\n",
       "         [  1.53020080e-02,  -6.80115148e-02,   5.30689061e-02]],\n",
       " \n",
       "        [[ -3.80308591e-02,   2.13926006e-03,   8.19922984e-03],\n",
       "         [ -2.79540811e-02,  -8.29580501e-02,  -6.42812625e-02],\n",
       "         [  3.84612121e-02,   3.94697679e-04,   3.73499729e-02]],\n",
       " \n",
       "        [[  5.89563418e-03,  -5.39527312e-02,  -7.21487552e-02],\n",
       "         [ -7.34341964e-02,  -8.18995177e-05,  -4.35749441e-02],\n",
       "         [  3.34840417e-02,  -8.07501078e-02,   1.24222152e-02]],\n",
       " \n",
       "        [[  5.13453558e-02,  -2.97697503e-02,   6.64037168e-02],\n",
       "         [  7.18497336e-02,  -6.15548305e-02,   4.55034114e-02],\n",
       "         [  6.74522668e-02,  -6.50649220e-02,  -1.82812791e-02]],\n",
       " \n",
       "        [[ -4.95614000e-02,  -6.19949438e-02,   5.96099123e-02],\n",
       "         [ -5.35730608e-02,   4.91776168e-02,   7.83182755e-02],\n",
       "         [  1.91748720e-02,   5.18141054e-02,  -7.56108295e-03]],\n",
       " \n",
       "        [[ -4.32058461e-02,  -3.56234983e-02,   4.74265292e-02],\n",
       "         [ -2.21782066e-02,  -1.00840256e-02,   7.62695298e-02],\n",
       "         [  5.80076762e-02,   2.80672144e-02,   9.49755404e-03]],\n",
       " \n",
       "        [[  2.71673631e-02,   4.12465148e-02,   5.92497401e-02],\n",
       "         [ -2.82829208e-03,  -6.19704509e-03,   2.58922484e-02],\n",
       "         [ -3.24029475e-02,  -4.74407561e-02,  -8.32363665e-02]],\n",
       " \n",
       "        [[ -7.00477958e-02,  -4.03049365e-02,  -7.93403089e-02],\n",
       "         [  1.03673544e-02,   5.48769943e-02,  -4.80425544e-03],\n",
       "         [  5.69177046e-02,   1.70000643e-02,  -2.75734216e-02]],\n",
       " \n",
       "        [[ -2.15059388e-02,  -4.84599881e-02,   3.25011648e-02],\n",
       "         [ -1.69297028e-02,   3.13787423e-02,   7.69316629e-02],\n",
       "         [ -4.91483733e-02,  -6.60581663e-02,  -8.77051987e-03]]], dtype=float32), bias=-0.00015541763, layer=1, neuron_number=51, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.02371191,  0.03508382,  0.04360914],\n",
       "         [ 0.03811665, -0.02012223, -0.06950157],\n",
       "         [-0.02848861,  0.02346344,  0.07182664]],\n",
       " \n",
       "        [[-0.02625925, -0.0217755 ,  0.02549617],\n",
       "         [ 0.06328171, -0.05170344, -0.06293584],\n",
       "         [ 0.03675528, -0.02239181, -0.030366  ]],\n",
       " \n",
       "        [[-0.05686377,  0.00028648, -0.06297314],\n",
       "         [ 0.00471752,  0.03530798, -0.00784483],\n",
       "         [-0.05129522,  0.04190577, -0.03204704]],\n",
       " \n",
       "        [[ 0.03010812, -0.04226308,  0.03141949],\n",
       "         [-0.0766232 ,  0.01961084, -0.02301735],\n",
       "         [-0.02977693,  0.0582475 , -0.01450494]],\n",
       " \n",
       "        [[ 0.07348914,  0.02316627, -0.00076468],\n",
       "         [ 0.00540669,  0.06282815,  0.02059845],\n",
       "         [-0.07182239, -0.04210717, -0.07402533]],\n",
       " \n",
       "        [[ 0.01480585, -0.06599389, -0.03365822],\n",
       "         [-0.04258013,  0.05661474, -0.02947906],\n",
       "         [-0.06187407,  0.00288747,  0.067199  ]],\n",
       " \n",
       "        [[ 0.06891897, -0.00125476, -0.01478291],\n",
       "         [-0.06834246,  0.00886101, -0.0112942 ],\n",
       "         [-0.02532399, -0.019699  ,  0.04864004]],\n",
       " \n",
       "        [[ 0.00855135,  0.0639551 ,  0.06993215],\n",
       "         [ 0.02946247, -0.01111024, -0.07512068],\n",
       "         [-0.04496535, -0.07428968, -0.06643394]],\n",
       " \n",
       "        [[-0.04996888,  0.05511592, -0.05902559],\n",
       "         [ 0.01089157, -0.02294395,  0.07972632],\n",
       "         [-0.0460241 ,  0.08056534, -0.0505027 ]],\n",
       " \n",
       "        [[ 0.06873123,  0.00789497,  0.02761151],\n",
       "         [ 0.05596099, -0.06743148,  0.01491976],\n",
       "         [-0.07654282,  0.07394126,  0.08202013]],\n",
       " \n",
       "        [[-0.01389481, -0.03121529,  0.0420081 ],\n",
       "         [-0.0816192 ,  0.01415402,  0.01969567],\n",
       "         [-0.0601813 , -0.04206383,  0.06683367]],\n",
       " \n",
       "        [[ 0.05101259,  0.06979241,  0.02111248],\n",
       "         [ 0.08337057,  0.062482  ,  0.04256068],\n",
       "         [ 0.04609077, -0.03380457, -0.04207157]],\n",
       " \n",
       "        [[ 0.02288811,  0.04233193,  0.01185344],\n",
       "         [-0.04537816, -0.06967857,  0.07603753],\n",
       "         [-0.01953321,  0.02401472,  0.00066573]],\n",
       " \n",
       "        [[ 0.04343024,  0.08231692, -0.01995302],\n",
       "         [-0.03306657,  0.00675735,  0.0519677 ],\n",
       "         [-0.0682496 ,  0.05987354,  0.0748962 ]],\n",
       " \n",
       "        [[-0.08204088, -0.04808975, -0.07112068],\n",
       "         [-0.00225514,  0.04746757, -0.06160342],\n",
       "         [ 0.02468613,  0.05236477, -0.07320131]],\n",
       " \n",
       "        [[-0.04626376,  0.05550233,  0.04262529],\n",
       "         [ 0.07474204, -0.04951598,  0.04679898],\n",
       "         [ 0.05627375,  0.02701799, -0.02235486]],\n",
       " \n",
       "        [[ 0.06899299, -0.02058518,  0.03923482],\n",
       "         [ 0.01806612, -0.03459848,  0.06022632],\n",
       "         [-0.0264576 , -0.03270881, -0.0791998 ]],\n",
       " \n",
       "        [[ 0.01196114, -0.03162589, -0.06593027],\n",
       "         [ 0.07221059, -0.08099171,  0.05227821],\n",
       "         [-0.01364692,  0.00379626,  0.05618757]],\n",
       " \n",
       "        [[ 0.03257584,  0.01048403,  0.08058791],\n",
       "         [-0.00528417, -0.07229923,  0.05373091],\n",
       "         [-0.00236768,  0.05739088,  0.01496343]],\n",
       " \n",
       "        [[ 0.07692983,  0.05603853,  0.05335796],\n",
       "         [ 0.07941909, -0.01403401, -0.0356915 ],\n",
       "         [ 0.04361988,  0.08193772, -0.005224  ]],\n",
       " \n",
       "        [[ 0.06226834, -0.06831553, -0.04069773],\n",
       "         [-0.03710959,  0.01933728,  0.05598738],\n",
       "         [-0.03088919, -0.06889041,  0.0077727 ]],\n",
       " \n",
       "        [[ 0.06209604, -0.07739511, -0.011174  ],\n",
       "         [-0.07262107,  0.0161535 , -0.07509028],\n",
       "         [-0.01546844, -0.07772784, -0.08128121]],\n",
       " \n",
       "        [[-0.06722014, -0.02092668,  0.06616575],\n",
       "         [ 0.02620068,  0.0334354 , -0.00847581],\n",
       "         [-0.04594273, -0.06948052,  0.02301614]],\n",
       " \n",
       "        [[-0.06308996,  0.0499473 ,  0.03243756],\n",
       "         [ 0.06078848, -0.03814724,  0.03545494],\n",
       "         [-0.02239585,  0.0468319 , -0.00878626]],\n",
       " \n",
       "        [[ 0.00696482, -0.00545039,  0.03600146],\n",
       "         [ 0.03295562,  0.0074852 , -0.02870108],\n",
       "         [ 0.05368286,  0.05131677, -0.03228923]],\n",
       " \n",
       "        [[-0.05244036,  0.0786546 ,  0.00398538],\n",
       "         [-0.02785708, -0.06623063, -0.01655463],\n",
       "         [-0.05920735,  0.04761997,  0.00106856]],\n",
       " \n",
       "        [[-0.01743129, -0.0052444 , -0.03594195],\n",
       "         [ 0.03582517, -0.0307565 , -0.03884095],\n",
       "         [ 0.05013879, -0.01733438,  0.06850825]],\n",
       " \n",
       "        [[ 0.06809516, -0.02035049, -0.00855974],\n",
       "         [-0.01899227,  0.040601  , -0.00057604],\n",
       "         [-0.04584919, -0.00385985,  0.0634473 ]],\n",
       " \n",
       "        [[-0.06922632,  0.07694636, -0.02516637],\n",
       "         [-0.02371902,  0.01351854, -0.05547816],\n",
       "         [-0.06245751,  0.02086903,  0.00260738]],\n",
       " \n",
       "        [[-0.06318035, -0.01150297, -0.04556463],\n",
       "         [-0.00091675,  0.01526816, -0.05070294],\n",
       "         [-0.01302676, -0.04006489, -0.08161362]],\n",
       " \n",
       "        [[-0.0037018 , -0.04205791,  0.04083899],\n",
       "         [-0.061608  ,  0.05429026,  0.04439818],\n",
       "         [-0.03759523,  0.06119714, -0.05106022]],\n",
       " \n",
       "        [[-0.07951532, -0.02400705,  0.04922765],\n",
       "         [ 0.00242495,  0.0698432 , -0.02842021],\n",
       "         [-0.0255846 , -0.03614361,  0.0771992 ]]], dtype=float32), bias=-0.00034325873, layer=1, neuron_number=52, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.06724674, -0.00724788, -0.06383081],\n",
       "         [ 0.04289812, -0.07529076, -0.08121251],\n",
       "         [-0.02041194, -0.05122713,  0.01077464]],\n",
       " \n",
       "        [[-0.06358155,  0.02942806, -0.05058132],\n",
       "         [-0.05544488,  0.04578552,  0.02070683],\n",
       "         [-0.0300135 , -0.06610942,  0.00225703]],\n",
       " \n",
       "        [[ 0.05226688, -0.06926946, -0.06792472],\n",
       "         [-0.00606443,  0.06598376,  0.03693002],\n",
       "         [ 0.0750979 ,  0.02667584, -0.06536663]],\n",
       " \n",
       "        [[ 0.01845964,  0.04606438,  0.06068625],\n",
       "         [-0.07937669, -0.04344373, -0.00788473],\n",
       "         [-0.03452425,  0.06323898, -0.00847402]],\n",
       " \n",
       "        [[-0.06729887, -0.03572719, -0.00311005],\n",
       "         [ 0.00356384,  0.07915539, -0.05200084],\n",
       "         [ 0.03781117,  0.07671223, -0.00308279]],\n",
       " \n",
       "        [[ 0.01328256, -0.06619851, -0.00150951],\n",
       "         [ 0.01804841,  0.0219425 ,  0.01577172],\n",
       "         [ 0.0405441 , -0.06114021,  0.06582757]],\n",
       " \n",
       "        [[-0.06110402, -0.01040364, -0.00263169],\n",
       "         [ 0.00015835,  0.06329324,  0.03195605],\n",
       "         [ 0.0701416 , -0.00890916,  0.01068502]],\n",
       " \n",
       "        [[ 0.02755166,  0.03316277, -0.00572894],\n",
       "         [ 0.07397275,  0.06944861, -0.05940161],\n",
       "         [ 0.07183335, -0.08098377, -0.0670798 ]],\n",
       " \n",
       "        [[-0.02018716, -0.07249679,  0.03163536],\n",
       "         [ 0.04116163,  0.00676158, -0.03072289],\n",
       "         [ 0.02355844,  0.02537389, -0.07280587]],\n",
       " \n",
       "        [[-0.01894467, -0.05379746, -0.05628416],\n",
       "         [-0.03822616,  0.05825792, -0.04474271],\n",
       "         [ 0.00199493, -0.04862191, -0.00201624]],\n",
       " \n",
       "        [[-0.03210129,  0.01614231, -0.01895563],\n",
       "         [-0.02295603,  0.07552278, -0.02303791],\n",
       "         [ 0.06815685,  0.02068552,  0.01206695]],\n",
       " \n",
       "        [[-0.03129938,  0.00155389,  0.07894103],\n",
       "         [-0.03220522, -0.074371  ,  0.03852952],\n",
       "         [ 0.02577829,  0.01131092, -0.01877903]],\n",
       " \n",
       "        [[-0.00590439,  0.07380813,  0.06832589],\n",
       "         [ 0.05943252,  0.06743491,  0.01743708],\n",
       "         [ 0.00856698, -0.03110031, -0.02792004]],\n",
       " \n",
       "        [[-0.01220385, -0.0498088 , -0.03815124],\n",
       "         [-0.0691558 ,  0.01386111, -0.07529067],\n",
       "         [-0.03334643, -0.04839539,  0.00394899]],\n",
       " \n",
       "        [[-0.00168238,  0.06201111, -0.04800863],\n",
       "         [ 0.00063721,  0.0558748 ,  0.04563694],\n",
       "         [ 0.05040536,  0.01741662, -0.00962687]],\n",
       " \n",
       "        [[-0.00980402,  0.02874125,  0.01745491],\n",
       "         [ 0.05362787,  0.05222071,  0.06766181],\n",
       "         [-0.01843787, -0.01817662, -0.01471739]],\n",
       " \n",
       "        [[-0.07556147,  0.03675144, -0.00786013],\n",
       "         [-0.08281269, -0.02743806, -0.06944968],\n",
       "         [ 0.01201334, -0.043629  , -0.00398361]],\n",
       " \n",
       "        [[ 0.06544543, -0.06614925, -0.06400146],\n",
       "         [-0.07706331, -0.06094726, -0.00810455],\n",
       "         [-0.05037231, -0.02874602, -0.04037991]],\n",
       " \n",
       "        [[ 0.00395705, -0.01867357, -0.08061472],\n",
       "         [ 0.01702044, -0.01416907, -0.04892336],\n",
       "         [ 0.04043287, -0.02205667,  0.03932698]],\n",
       " \n",
       "        [[ 0.00824207, -0.00850199,  0.08167749],\n",
       "         [-0.06736226, -0.04588562, -0.04594934],\n",
       "         [-0.06652339, -0.02822443,  0.07026023]],\n",
       " \n",
       "        [[ 0.0269969 ,  0.03279418, -0.0723298 ],\n",
       "         [ 0.00630466, -0.05370303,  0.07224021],\n",
       "         [-0.01405852, -0.03507819, -0.0766591 ]],\n",
       " \n",
       "        [[-0.01148486, -0.0347035 , -0.04111896],\n",
       "         [ 0.0345637 ,  0.04822985, -0.02337546],\n",
       "         [ 0.06043727,  0.0141997 , -0.01762605]],\n",
       " \n",
       "        [[-0.07502666,  0.02475417,  0.07392091],\n",
       "         [ 0.01032099,  0.03825851, -0.07811894],\n",
       "         [ 0.02064382,  0.02842232, -0.03057848]],\n",
       " \n",
       "        [[-0.08041546, -0.06022323,  0.03378087],\n",
       "         [-0.05995381,  0.00877171, -0.04170106],\n",
       "         [-0.01157206,  0.05333018, -0.04669667]],\n",
       " \n",
       "        [[ 0.05809447, -0.0139111 ,  0.06306621],\n",
       "         [-0.06569236, -0.08011118,  0.05821516],\n",
       "         [-0.01921903, -0.06356594, -0.06279268]],\n",
       " \n",
       "        [[-0.02780703, -0.06932762,  0.07353576],\n",
       "         [-0.0244769 ,  0.08068583,  0.04073721],\n",
       "         [-0.06545401,  0.03558485,  0.03597031]],\n",
       " \n",
       "        [[ 0.03556268,  0.00627564, -0.03047322],\n",
       "         [ 0.05121574, -0.02616652,  0.08348127],\n",
       "         [-0.02396122, -0.02681638, -0.02892062]],\n",
       " \n",
       "        [[ 0.05159865, -0.05403217,  0.06700109],\n",
       "         [-0.03431835,  0.02787279,  0.01947832],\n",
       "         [-0.00126243,  0.06367555, -0.05140489]],\n",
       " \n",
       "        [[-0.0447191 , -0.00792667,  0.06348398],\n",
       "         [ 0.06323328,  0.00027301,  0.06002282],\n",
       "         [ 0.05167169,  0.05723641, -0.05759072]],\n",
       " \n",
       "        [[-0.00028733,  0.0043886 ,  0.05891996],\n",
       "         [ 0.07259687,  0.03311713,  0.06907547],\n",
       "         [-0.01023093,  0.01152064, -0.03789223]],\n",
       " \n",
       "        [[-0.04597228, -0.03429467,  0.0033896 ],\n",
       "         [-0.02149742,  0.06280985, -0.0677899 ],\n",
       "         [ 0.00190118, -0.02664339, -0.02748527]],\n",
       " \n",
       "        [[ 0.06549436, -0.02250222,  0.04993241],\n",
       "         [ 0.07581721,  0.02795823,  0.04073038],\n",
       "         [-0.01620085, -0.0780111 , -0.08222681]]], dtype=float32), bias=0.001434716, layer=1, neuron_number=53, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.05665764,  0.00123747, -0.0016376 ],\n",
       "         [-0.02792316, -0.06979113,  0.01049171],\n",
       "         [ 0.07532951, -0.03496401,  0.01465485]],\n",
       " \n",
       "        [[ 0.05103603, -0.07077547,  0.06408782],\n",
       "         [ 0.07987442,  0.06779867,  0.06589068],\n",
       "         [-0.07487408,  0.03078885,  0.05851726]],\n",
       " \n",
       "        [[-0.06380829, -0.05500017, -0.05170177],\n",
       "         [-0.04009355,  0.0526226 ,  0.04059943],\n",
       "         [-0.03035706, -0.05925676,  0.04008085]],\n",
       " \n",
       "        [[ 0.06830244,  0.04716533,  0.04634221],\n",
       "         [-0.07669973, -0.03378193, -0.05476782],\n",
       "         [-0.05133538, -0.00071283,  0.06911685]],\n",
       " \n",
       "        [[ 0.04531919, -0.07428454,  0.06208275],\n",
       "         [-0.03454474,  0.08002923, -0.00666824],\n",
       "         [ 0.01360361, -0.00571454,  0.02438248]],\n",
       " \n",
       "        [[ 0.05727343,  0.01399607, -0.02527344],\n",
       "         [-0.06045453, -0.04805577, -0.00248387],\n",
       "         [-0.02483115, -0.07653393,  0.07553019]],\n",
       " \n",
       "        [[ 0.07254348, -0.04846441,  0.08154643],\n",
       "         [ 0.00306433, -0.05804633,  0.04217729],\n",
       "         [ 0.02566862,  0.01412289,  0.0712856 ]],\n",
       " \n",
       "        [[ 0.07187609,  0.00783895, -0.06317312],\n",
       "         [-0.0706634 , -0.04071098, -0.0108538 ],\n",
       "         [-0.03908186,  0.07843279, -0.0651074 ]],\n",
       " \n",
       "        [[-0.019456  , -0.02448709, -0.06356511],\n",
       "         [-0.05830185,  0.05114441, -0.04992723],\n",
       "         [ 0.02793815,  0.07402308, -0.00617942]],\n",
       " \n",
       "        [[-0.07155427, -0.05146232, -0.02553416],\n",
       "         [ 0.01527432,  0.00344508,  0.04095588],\n",
       "         [ 0.04602972,  0.03371596,  0.04920885]],\n",
       " \n",
       "        [[-0.03801217,  0.05626081,  0.00048089],\n",
       "         [-0.04905514, -0.03541503,  0.01769713],\n",
       "         [-0.05274054,  0.04592232, -0.06874373]],\n",
       " \n",
       "        [[ 0.0623209 ,  0.05415084,  0.0060711 ],\n",
       "         [ 0.07257613, -0.0588962 , -0.0243577 ],\n",
       "         [-0.02809471,  0.05799209, -0.01657991]],\n",
       " \n",
       "        [[ 0.02632108,  0.04717946, -0.03476248],\n",
       "         [-0.04558866,  0.06776883, -0.00383185],\n",
       "         [-0.05411961, -0.07381621, -0.05370821]],\n",
       " \n",
       "        [[ 0.00716016,  0.07057977,  0.03026382],\n",
       "         [-0.03613093,  0.00531404,  0.00784711],\n",
       "         [-0.03357458,  0.01792751,  0.00180018]],\n",
       " \n",
       "        [[-0.02375623, -0.05795557, -0.00843871],\n",
       "         [-0.02831638, -0.01439596,  0.0091679 ],\n",
       "         [-0.03967724,  0.05039473, -0.02613173]],\n",
       " \n",
       "        [[-0.0417168 , -0.05070379, -0.07884461],\n",
       "         [ 0.07969757, -0.00741058, -0.07650245],\n",
       "         [ 0.02164568, -0.07240152, -0.00218105]],\n",
       " \n",
       "        [[ 0.07011218, -0.03532255,  0.03838277],\n",
       "         [-0.0703391 ,  0.05191134,  0.08024146],\n",
       "         [-0.07079463, -0.01845939,  0.00868369]],\n",
       " \n",
       "        [[-0.05770522,  0.02941786,  0.02996693],\n",
       "         [-0.02069347,  0.05234959, -0.02800955],\n",
       "         [-0.02214999,  0.01364571,  0.02295811]],\n",
       " \n",
       "        [[ 0.02174071, -0.03525992,  0.07273991],\n",
       "         [ 0.00420005,  0.01619042,  0.06605184],\n",
       "         [ 0.06854107, -0.07108498, -0.01352615]],\n",
       " \n",
       "        [[ 0.01996111,  0.0636408 ,  0.02780449],\n",
       "         [-0.02180115, -0.05129509,  0.04990381],\n",
       "         [ 0.02276417,  0.04108446, -0.03877571]],\n",
       " \n",
       "        [[-0.066279  , -0.04624902, -0.02271299],\n",
       "         [ 0.00057151,  0.06552723,  0.0118162 ],\n",
       "         [ 0.07198414,  0.06547195,  0.01624498]],\n",
       " \n",
       "        [[-0.00554843, -0.0666859 , -0.04887374],\n",
       "         [ 0.06632512,  0.00728542, -0.02726473],\n",
       "         [ 0.01500135, -0.07763062, -0.06600133]],\n",
       " \n",
       "        [[ 0.04720645,  0.07858855,  0.04824279],\n",
       "         [-0.0142569 ,  0.00818156,  0.01472993],\n",
       "         [-0.06478829, -0.06932799, -0.04917495]],\n",
       " \n",
       "        [[-0.04062149,  0.02587325, -0.0592167 ],\n",
       "         [ 0.06934404,  0.0477934 ,  0.02455663],\n",
       "         [ 0.05857142, -0.08039227,  0.01636649]],\n",
       " \n",
       "        [[-0.06376677,  0.06680209,  0.04664685],\n",
       "         [-0.04731194, -0.00164874, -0.05730174],\n",
       "         [ 0.07864141,  0.00754651, -0.03249136]],\n",
       " \n",
       "        [[ 0.05156725, -0.03180082,  0.01021508],\n",
       "         [-0.07970131,  0.04676019,  0.06257108],\n",
       "         [-0.04462033,  0.0436953 , -0.03264903]],\n",
       " \n",
       "        [[-0.03089492, -0.08137587,  0.02893829],\n",
       "         [ 0.04010721, -0.06761165, -0.00340339],\n",
       "         [ 0.06972559, -0.00758398, -0.02493733]],\n",
       " \n",
       "        [[ 0.07157315,  0.0064838 ,  0.03306263],\n",
       "         [-0.08027922, -0.00162048, -0.02973223],\n",
       "         [ 0.07556031, -0.07211086,  0.05432725]],\n",
       " \n",
       "        [[ 0.00616361,  0.08096103, -0.00246593],\n",
       "         [ 0.07145218, -0.0101042 , -0.01155005],\n",
       "         [-0.04056247,  0.00802584,  0.05996183]],\n",
       " \n",
       "        [[ 0.05925822,  0.01859057,  0.07393876],\n",
       "         [-0.07802127,  0.03175513,  0.02183551],\n",
       "         [-0.08056367, -0.07659865, -0.0308731 ]],\n",
       " \n",
       "        [[ 0.07527563, -0.06287347, -0.00212667],\n",
       "         [-0.00966785, -0.02559435, -0.00414747],\n",
       "         [ 0.06326728,  0.03340103,  0.06994212]],\n",
       " \n",
       "        [[-0.05212925,  0.06748733, -0.04495832],\n",
       "         [ 0.06715379, -0.01141626, -0.00093922],\n",
       "         [-0.06564232,  0.06978098,  0.04826177]]], dtype=float32), bias=0.0022516332, layer=1, neuron_number=54, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.0549935 , -0.0095223 , -0.01842866],\n",
       "         [ 0.03175447,  0.05466445,  0.06734828],\n",
       "         [-0.01918835,  0.02225205, -0.03006523]],\n",
       " \n",
       "        [[ 0.03994475, -0.01777532,  0.07963122],\n",
       "         [-0.05004066, -0.02963749,  0.03021479],\n",
       "         [-0.0357994 ,  0.05575705, -0.04578821]],\n",
       " \n",
       "        [[-0.00199685, -0.02043421,  0.07396361],\n",
       "         [ 0.00462973, -0.02275084,  0.04412465],\n",
       "         [ 0.0117376 , -0.04143555,  0.04951935]],\n",
       " \n",
       "        [[-0.01182542,  0.01392967,  0.05298096],\n",
       "         [-0.02963737, -0.03349349,  0.04983017],\n",
       "         [-0.04785187,  0.04026642,  0.0430793 ]],\n",
       " \n",
       "        [[ 0.04867416, -0.0549942 ,  0.00888185],\n",
       "         [ 0.05820433, -0.04959166,  0.06168714],\n",
       "         [ 0.03687542, -0.03446577,  0.07693956]],\n",
       " \n",
       "        [[ 0.07257297, -0.03066051,  0.06120462],\n",
       "         [ 0.00734815, -0.06356533,  0.01992478],\n",
       "         [-0.0762454 , -0.04934761, -0.07322393]],\n",
       " \n",
       "        [[-0.0174422 , -0.05722067,  0.01669146],\n",
       "         [-0.0468911 , -0.02580995,  0.06450854],\n",
       "         [ 0.05068526,  0.07260157, -0.06096839]],\n",
       " \n",
       "        [[-0.07324997, -0.00952561, -0.02563743],\n",
       "         [-0.00333122,  0.02508897, -0.0634772 ],\n",
       "         [ 0.05776682,  0.05700367,  0.06330149]],\n",
       " \n",
       "        [[-0.04761671,  0.01821387, -0.05318943],\n",
       "         [ 0.08279711, -0.04531861, -0.0280606 ],\n",
       "         [-0.01020717,  0.00402271,  0.03904963]],\n",
       " \n",
       "        [[ 0.03811489, -0.05309075, -0.03765675],\n",
       "         [-0.03988314,  0.06952523,  0.04323105],\n",
       "         [-0.00276421,  0.00169868,  0.03389819]],\n",
       " \n",
       "        [[-0.05054489,  0.04644154, -0.02006011],\n",
       "         [ 0.00544569, -0.05852347,  0.00267031],\n",
       "         [ 0.06454245,  0.0088584 , -0.02311331]],\n",
       " \n",
       "        [[ 0.00252972, -0.02106533, -0.04782564],\n",
       "         [ 0.01497179,  0.05261254, -0.07093246],\n",
       "         [ 0.01795436, -0.01864172, -0.0771213 ]],\n",
       " \n",
       "        [[ 0.0347331 ,  0.02211598, -0.03972609],\n",
       "         [ 0.05965163, -0.06513964, -0.03684846],\n",
       "         [ 0.03680096, -0.00068952,  0.06252517]],\n",
       " \n",
       "        [[-0.03156409, -0.06539091, -0.04364378],\n",
       "         [ 0.01199115, -0.07601451,  0.00520995],\n",
       "         [ 0.04004502,  0.04709889,  0.05947514]],\n",
       " \n",
       "        [[-0.03817578,  0.04331778, -0.00046738],\n",
       "         [ 0.07986731,  0.02983765, -0.03591394],\n",
       "         [ 0.00436852,  0.02151453, -0.03873581]],\n",
       " \n",
       "        [[ 0.05366722, -0.05465686, -0.05279731],\n",
       "         [-0.03149676, -0.07024371, -0.01642633],\n",
       "         [ 0.00618117,  0.02531066, -0.01232222]],\n",
       " \n",
       "        [[ 0.02155892,  0.00464759, -0.05755403],\n",
       "         [ 0.00753828,  0.01233514, -0.01875175],\n",
       "         [-0.0325034 ,  0.01319495,  0.07810379]],\n",
       " \n",
       "        [[-0.04366556,  0.00366471,  0.0465266 ],\n",
       "         [ 0.06290518,  0.06453823, -0.03646725],\n",
       "         [-0.02811461, -0.00384663,  0.07101998]],\n",
       " \n",
       "        [[-0.02911869, -0.07107923,  0.04621404],\n",
       "         [-0.00906104,  0.0465396 ,  0.01544994],\n",
       "         [-0.0652272 , -0.05379506,  0.07202005]],\n",
       " \n",
       "        [[-0.01771276, -0.04271192, -0.07021246],\n",
       "         [ 0.08108939,  0.00145648, -0.00934854],\n",
       "         [ 0.02967976, -0.06400856,  0.04994673]],\n",
       " \n",
       "        [[ 0.03198465,  0.02473301,  0.07535514],\n",
       "         [ 0.03825689,  0.05526519, -0.04694203],\n",
       "         [ 0.03994563,  0.00277152, -0.05025912]],\n",
       " \n",
       "        [[-0.00990088, -0.04609099, -0.00947101],\n",
       "         [-0.07818791, -0.00032965, -0.0096347 ],\n",
       "         [-0.04672661, -0.0142487 , -0.04818595]],\n",
       " \n",
       "        [[ 0.02369898, -0.06726999, -0.04548873],\n",
       "         [-0.03722673, -0.0411705 , -0.03320169],\n",
       "         [-0.083008  ,  0.01220013, -0.03909412]],\n",
       " \n",
       "        [[-0.0473515 , -0.02915137,  0.06387872],\n",
       "         [-0.02765899, -0.00031611, -0.04250347],\n",
       "         [ 0.07497767, -0.01850562,  0.04790925]],\n",
       " \n",
       "        [[-0.07240226,  0.04112615, -0.01434753],\n",
       "         [ 0.01895553,  0.04853651,  0.0137918 ],\n",
       "         [ 0.04440072,  0.04836673,  0.02116082]],\n",
       " \n",
       "        [[-0.00602052,  0.07921977, -0.08140801],\n",
       "         [-0.06133978, -0.03261346, -0.06850384],\n",
       "         [ 0.04903562, -0.00105911,  0.05189436]],\n",
       " \n",
       "        [[ 0.01305409, -0.07807335, -0.08293049],\n",
       "         [ 0.06109827, -0.02511647, -0.02757339],\n",
       "         [ 0.03851337,  0.00112041, -0.07464845]],\n",
       " \n",
       "        [[ 0.03969067,  0.01448507, -0.02058292],\n",
       "         [-0.05643073,  0.0689561 , -0.04188731],\n",
       "         [-0.07847759,  0.04652703, -0.06474425]],\n",
       " \n",
       "        [[ 0.06148924,  0.05457108,  0.0199849 ],\n",
       "         [ 0.02623123,  0.03502653,  0.02159957],\n",
       "         [ 0.05597516,  0.03343441, -0.04171263]],\n",
       " \n",
       "        [[-0.07052013,  0.03117344,  0.04961812],\n",
       "         [-0.03295407,  0.06168349,  0.02403235],\n",
       "         [-0.02807479, -0.05632254, -0.03558912]],\n",
       " \n",
       "        [[ 0.06765302,  0.0119352 ,  0.03753902],\n",
       "         [-0.01997478,  0.04348368,  0.04223122],\n",
       "         [-0.02738979, -0.04897809,  0.02624898]],\n",
       " \n",
       "        [[ 0.06011696,  0.07348879, -0.00356809],\n",
       "         [-0.07667129,  0.05741702,  0.07279038],\n",
       "         [-0.07496191,  0.06109853, -0.03659512]]], dtype=float32), bias=0.00053448096, layer=1, neuron_number=55, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ -6.50898516e-02,   7.98308328e-02,   5.37986271e-02],\n",
       "         [ -1.79587342e-02,   3.04031651e-02,  -1.34806167e-02],\n",
       "         [  2.79353820e-02,   5.74599504e-02,   3.86229157e-02]],\n",
       " \n",
       "        [[ -9.07275919e-03,  -6.64755479e-02,   3.92555408e-02],\n",
       "         [  1.69710405e-02,   1.11518148e-03,  -5.52419648e-02],\n",
       "         [  1.67977475e-02,   4.02116254e-02,  -4.84651849e-02]],\n",
       " \n",
       "        [[ -4.05762456e-02,  -3.54320519e-02,  -4.59159389e-02],\n",
       "         [ -4.46479507e-02,   1.55821657e-02,  -6.62525520e-02],\n",
       "         [ -2.46600471e-02,  -7.13220041e-04,  -1.63405342e-03]],\n",
       " \n",
       "        [[ -5.56363650e-02,   5.35397194e-02,   3.17894705e-02],\n",
       "         [  7.08031431e-02,  -4.53408621e-02,  -4.79613133e-02],\n",
       "         [ -4.35734466e-02,   2.38452181e-02,   6.23475499e-02]],\n",
       " \n",
       "        [[  2.03763582e-02,   3.48437726e-02,  -2.28029042e-02],\n",
       "         [ -5.71802892e-02,   1.50286183e-02,   2.92956047e-02],\n",
       "         [ -7.25172758e-02,  -1.71605963e-02,  -3.56333070e-02]],\n",
       " \n",
       "        [[ -3.75075527e-02,   4.15665703e-03,  -5.20677306e-02],\n",
       "         [  5.02089374e-02,  -8.16376507e-02,   1.09775346e-02],\n",
       "         [  7.66384066e-04,   7.65186101e-02,   2.08449718e-02]],\n",
       " \n",
       "        [[  1.10234758e-02,   2.23882534e-02,   2.96243150e-02],\n",
       "         [  4.82360311e-02,  -6.81891590e-02,  -4.64352593e-02],\n",
       "         [  6.40337095e-02,  -2.50883698e-02,   6.91762120e-02]],\n",
       " \n",
       "        [[ -2.51712464e-02,   3.86828464e-03,   7.41549209e-02],\n",
       "         [  7.07633272e-02,   2.84664389e-02,   5.99991567e-02],\n",
       "         [ -5.48976287e-02,   6.76325411e-02,  -3.04199266e-03]],\n",
       " \n",
       "        [[  5.91635816e-02,  -1.74855290e-03,  -7.32983202e-02],\n",
       "         [  2.43764929e-02,  -2.72098277e-02,  -2.14685053e-02],\n",
       "         [ -1.33113964e-02,  -6.95088878e-02,   1.28901079e-02]],\n",
       " \n",
       "        [[ -4.72747833e-02,  -5.60893044e-02,   5.70299514e-02],\n",
       "         [ -3.72330695e-02,   6.41072318e-02,  -7.82657787e-02],\n",
       "         [  7.17453957e-02,  -7.10418522e-02,   2.15092325e-03]],\n",
       " \n",
       "        [[  3.46621610e-02,  -2.57109180e-02,  -7.06381276e-02],\n",
       "         [  3.86287756e-02,  -3.71831469e-02,   2.90947426e-02],\n",
       "         [  5.95853813e-02,  -2.06035674e-02,  -6.71520233e-02]],\n",
       " \n",
       "        [[ -2.95424871e-02,  -1.09253814e-02,   1.43278278e-02],\n",
       "         [ -1.52135808e-02,  -3.77428830e-02,   2.11934205e-02],\n",
       "         [  4.90354486e-02,  -7.29506239e-02,   1.11712143e-02]],\n",
       " \n",
       "        [[ -1.43219475e-02,   3.41778547e-02,   5.09244800e-02],\n",
       "         [ -7.71941990e-02,  -9.07879509e-03,   1.83376297e-02],\n",
       "         [  8.94782785e-03,  -5.10220453e-02,   2.78346613e-02]],\n",
       " \n",
       "        [[ -2.59757936e-02,  -1.11387726e-02,  -1.63135007e-02],\n",
       "         [  5.88488989e-02,   4.91008647e-02,  -5.18747456e-02],\n",
       "         [ -5.04530370e-02,   2.79353559e-02,   4.37117927e-02]],\n",
       " \n",
       "        [[ -8.31868798e-02,   3.22787091e-02,   6.54307082e-02],\n",
       "         [ -7.29042962e-02,   7.08909109e-02,  -8.57004523e-03],\n",
       "         [  1.87305324e-02,   6.06443658e-02,   3.14443856e-02]],\n",
       " \n",
       "        [[  4.51130560e-03,  -6.00727126e-02,  -4.39684801e-02],\n",
       "         [ -4.82780077e-02,   7.00732023e-02,   6.72603473e-02],\n",
       "         [  3.71083468e-02,  -3.48655097e-02,   5.97875901e-02]],\n",
       " \n",
       "        [[ -1.78795625e-02,   1.87561773e-02,   6.21758699e-02],\n",
       "         [  5.23904935e-02,   4.16440563e-03,   7.25101084e-02],\n",
       "         [  7.33640045e-02,   2.47247405e-02,  -3.19018448e-03]],\n",
       " \n",
       "        [[ -4.84381281e-02,   7.49396384e-02,  -6.47617830e-03],\n",
       "         [ -2.47681066e-02,  -3.55321132e-02,   4.78471220e-02],\n",
       "         [ -5.66838831e-02,  -8.99433624e-03,   3.20816934e-02]],\n",
       " \n",
       "        [[  2.30658893e-02,   4.03326601e-02,   1.25059076e-02],\n",
       "         [ -3.60826142e-02,  -3.60335782e-02,   4.71680351e-02],\n",
       "         [ -4.38590050e-02,   4.43905257e-02,  -5.65224402e-02]],\n",
       " \n",
       "        [[  1.49271954e-02,  -6.71388283e-02,   7.80413672e-02],\n",
       "         [ -6.29493967e-02,   1.91289894e-02,  -3.41134816e-02],\n",
       "         [ -7.59482905e-02,  -6.63070306e-02,  -1.37764849e-02]],\n",
       " \n",
       "        [[  1.86166130e-02,   4.09161560e-02,   1.16105946e-02],\n",
       "         [  3.09387059e-03,  -4.53958809e-02,   1.10539272e-02],\n",
       "         [ -1.04108239e-02,   5.38066253e-02,   6.06851354e-02]],\n",
       " \n",
       "        [[  2.89991200e-02,  -8.13588351e-02,  -4.87021618e-02],\n",
       "         [ -8.32829028e-02,  -6.01307154e-02,   5.62563129e-02],\n",
       "         [ -5.64830899e-02,   1.94500275e-02,   6.09891936e-02]],\n",
       " \n",
       "        [[ -7.63993561e-02,   1.97415426e-02,  -7.87594616e-02],\n",
       "         [  1.43106263e-02,   1.06792524e-02,   4.24435325e-02],\n",
       "         [  2.53228657e-02,  -7.28914893e-05,  -8.22087452e-02]],\n",
       " \n",
       "        [[  7.27705210e-02,   1.23169124e-02,   3.22548002e-02],\n",
       "         [ -1.32888765e-03,  -3.23236659e-02,   6.33270293e-02],\n",
       "         [ -4.74024080e-02,  -3.33220810e-02,   3.55771743e-02]],\n",
       " \n",
       "        [[  8.66924785e-03,   3.75833223e-03,  -1.91332120e-02],\n",
       "         [ -1.33657530e-02,   8.07119347e-03,  -6.02501556e-02],\n",
       "         [ -5.12394980e-02,   4.26488258e-02,  -5.29297628e-02]],\n",
       " \n",
       "        [[  7.86027834e-02,  -3.92913446e-02,   3.47676948e-02],\n",
       "         [  3.47437561e-02,  -6.69891387e-02,   6.07602745e-02],\n",
       "         [ -4.84744348e-02,   6.71356991e-02,   6.59238696e-02]],\n",
       " \n",
       "        [[ -8.15934539e-02,   2.91003026e-02,  -1.39288390e-02],\n",
       "         [ -7.22354725e-02,  -3.60391475e-03,   2.12548720e-03],\n",
       "         [ -6.68387488e-02,  -4.18906175e-02,   3.79097462e-02]],\n",
       " \n",
       "        [[ -8.10605735e-02,  -7.50385970e-02,   5.53045981e-02],\n",
       "         [  5.47978934e-03,   4.16911021e-02,   2.37575974e-02],\n",
       "         [ -1.84792839e-02,   6.26350567e-02,  -1.28675727e-02]],\n",
       " \n",
       "        [[  6.31853491e-02,   4.92667258e-02,  -2.06276495e-02],\n",
       "         [ -7.38599375e-02,  -7.01779202e-02,   6.80714995e-02],\n",
       "         [  2.80895196e-02,  -4.47492562e-02,   3.96813825e-02]],\n",
       " \n",
       "        [[  5.37658669e-02,  -1.48155093e-02,  -6.89215437e-02],\n",
       "         [  3.62038165e-02,   4.54622582e-02,   7.43476301e-02],\n",
       "         [  2.40163226e-02,   1.98578034e-02,  -6.18872792e-02]],\n",
       " \n",
       "        [[ -7.17068911e-02,  -2.19885334e-02,  -6.23573326e-02],\n",
       "         [ -5.87862842e-02,   7.22884461e-02,   3.87862921e-02],\n",
       "         [ -8.04329813e-02,  -3.35137807e-02,  -6.27667084e-02]],\n",
       " \n",
       "        [[ -1.24383578e-02,   3.78153287e-02,   3.37269120e-02],\n",
       "         [  7.58379102e-02,  -2.83856336e-02,   1.85899846e-02],\n",
       "         [  1.33790430e-02,   5.30261807e-02,  -2.39584930e-02]]], dtype=float32), bias=0.0015748614, layer=1, neuron_number=56, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.02725248,  0.0288828 ,  0.02842518],\n",
       "         [ 0.06144338, -0.02533882, -0.00811281],\n",
       "         [ 0.04052731, -0.01215611, -0.07440329]],\n",
       " \n",
       "        [[ 0.07722744,  0.05042876, -0.04406675],\n",
       "         [-0.06732401, -0.03995425, -0.03463854],\n",
       "         [-0.04315421,  0.05576689, -0.03697565]],\n",
       " \n",
       "        [[-0.05962231,  0.04546419, -0.01249384],\n",
       "         [ 0.02093982,  0.07418896, -0.00309092],\n",
       "         [ 0.02410077,  0.0676327 , -0.02665247]],\n",
       " \n",
       "        [[ 0.01051646,  0.06014766,  0.02640226],\n",
       "         [ 0.06912343,  0.0140205 ,  0.03429904],\n",
       "         [-0.00746261, -0.07616574, -0.03019674]],\n",
       " \n",
       "        [[ 0.05040867, -0.03770586,  0.00554557],\n",
       "         [ 0.0300698 ,  0.03607485,  0.02295946],\n",
       "         [ 0.06074144, -0.02017535,  0.00632215]],\n",
       " \n",
       "        [[ 0.04272433, -0.02930121, -0.02796134],\n",
       "         [ 0.01352122,  0.04210828, -0.03708709],\n",
       "         [ 0.01722706, -0.04672819, -0.02629316]],\n",
       " \n",
       "        [[-0.0350095 , -0.06720267, -0.03519476],\n",
       "         [ 0.07658393, -0.00055705, -0.07415608],\n",
       "         [ 0.04878721,  0.05863386,  0.00603292]],\n",
       " \n",
       "        [[ 0.06826304,  0.06056879, -0.05666543],\n",
       "         [ 0.06583527,  0.0155935 , -0.05339746],\n",
       "         [ 0.03088797, -0.03719662,  0.0396767 ]],\n",
       " \n",
       "        [[-0.07856107, -0.03749495,  0.00909403],\n",
       "         [-0.00014751, -0.0589235 ,  0.07151721],\n",
       "         [-0.01597985, -0.00102987, -0.00601986]],\n",
       " \n",
       "        [[-0.00361048, -0.02754651,  0.0741028 ],\n",
       "         [-0.03058564, -0.02986103,  0.07528019],\n",
       "         [-0.025812  , -0.0562772 ,  0.06695563]],\n",
       " \n",
       "        [[ 0.02421909,  0.04958747, -0.06858423],\n",
       "         [-0.04645334, -0.03556245,  0.03308128],\n",
       "         [ 0.01399939, -0.03202993,  0.04331167]],\n",
       " \n",
       "        [[ 0.04208746,  0.0171926 , -0.08208597],\n",
       "         [-0.032398  ,  0.0344755 , -0.02239458],\n",
       "         [-0.06387857,  0.0518319 , -0.02480921]],\n",
       " \n",
       "        [[-0.05277934, -0.02551211,  0.03378414],\n",
       "         [-0.00260076,  0.02146428, -0.01880256],\n",
       "         [-0.01864574,  0.02720877,  0.07553855]],\n",
       " \n",
       "        [[-0.05771643, -0.03797058,  0.05045951],\n",
       "         [ 0.06592181, -0.00157819, -0.06158994],\n",
       "         [ 0.04347492, -0.04538403, -0.07551175]],\n",
       " \n",
       "        [[ 0.06586406, -0.07886721, -0.07205049],\n",
       "         [ 0.03689192, -0.01885909, -0.00715753],\n",
       "         [ 0.00614777, -0.06325914,  0.00439131]],\n",
       " \n",
       "        [[ 0.07846671, -0.006421  ,  0.0738221 ],\n",
       "         [ 0.01729071,  0.00424079, -0.01969925],\n",
       "         [ 0.03463215, -0.02521555, -0.01003439]],\n",
       " \n",
       "        [[-0.03092379,  0.08184624,  0.08080085],\n",
       "         [ 0.02042665, -0.00341124,  0.03320877],\n",
       "         [ 0.08235797, -0.07031897, -0.02612073]],\n",
       " \n",
       "        [[-0.07522932,  0.06250815,  0.04240127],\n",
       "         [-0.01976829, -0.01480953, -0.02556642],\n",
       "         [ 0.04182218, -0.03238898,  0.03513781]],\n",
       " \n",
       "        [[ 0.05650536, -0.05093642,  0.04082783],\n",
       "         [ 0.05129335, -0.03703606,  0.04825081],\n",
       "         [-0.04169204, -0.06925841,  0.01086205]],\n",
       " \n",
       "        [[ 0.01267935, -0.07199641, -0.06569332],\n",
       "         [-0.05093863, -0.07450799, -0.08082252],\n",
       "         [-0.08041631,  0.03962668,  0.04994871]],\n",
       " \n",
       "        [[-0.00734854, -0.04550471, -0.06292778],\n",
       "         [-0.03822653,  0.04997992, -0.04356052],\n",
       "         [ 0.07635916,  0.05148134,  0.01296441]],\n",
       " \n",
       "        [[ 0.0241794 ,  0.02729302,  0.02915306],\n",
       "         [ 0.00117423, -0.00808059, -0.01059352],\n",
       "         [-0.02319011,  0.00698423, -0.05694475]],\n",
       " \n",
       "        [[ 0.06339978,  0.07653031, -0.02394544],\n",
       "         [-0.00397943, -0.07260969,  0.06233209],\n",
       "         [ 0.0164513 ,  0.06489579,  0.04575324]],\n",
       " \n",
       "        [[ 0.01893973,  0.0711794 , -0.00016453],\n",
       "         [ 0.01618676,  0.00181785,  0.020326  ],\n",
       "         [ 0.03273326, -0.00528567,  0.0424711 ]],\n",
       " \n",
       "        [[ 0.07468189, -0.01559257,  0.04277565],\n",
       "         [ 0.0276107 ,  0.02325531, -0.02943611],\n",
       "         [ 0.01558456, -0.0141427 ,  0.05993446]],\n",
       " \n",
       "        [[ 0.00671595,  0.00865317, -0.06980926],\n",
       "         [-0.02345654,  0.05445917,  0.01786689],\n",
       "         [ 0.02634415,  0.01949366, -0.02015139]],\n",
       " \n",
       "        [[ 0.05582403,  0.0226869 , -0.06870215],\n",
       "         [ 0.0339034 ,  0.06925931,  0.06830056],\n",
       "         [ 0.06715988, -0.07626897,  0.08054544]],\n",
       " \n",
       "        [[ 0.0531951 , -0.00660552,  0.02581233],\n",
       "         [-0.02984249,  0.00342503, -0.03733612],\n",
       "         [ 0.05361738, -0.00924987,  0.07432635]],\n",
       " \n",
       "        [[ 0.08491473,  0.07506347,  0.03528684],\n",
       "         [-0.00847519, -0.00250077,  0.03207713],\n",
       "         [-0.0106922 ,  0.05850894,  0.04191771]],\n",
       " \n",
       "        [[ 0.01016717, -0.06489868, -0.05384744],\n",
       "         [ 0.07158885, -0.05148751, -0.02412157],\n",
       "         [-0.00303847, -0.0768994 , -0.06431226]],\n",
       " \n",
       "        [[-0.05166231, -0.08142645, -0.060615  ],\n",
       "         [ 0.02795465, -0.025837  ,  0.03987348],\n",
       "         [ 0.02196222,  0.06951781, -0.02133282]],\n",
       " \n",
       "        [[ 0.07951394, -0.06884374,  0.03625993],\n",
       "         [-0.07791498, -0.06465995,  0.02806925],\n",
       "         [-0.07935025, -0.03238034,  0.03730714]]], dtype=float32), bias=-0.00017353758, layer=1, neuron_number=57, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  3.50563452e-02,   3.87031250e-02,  -4.58959267e-02],\n",
       "         [  8.19233358e-02,   1.72603875e-02,  -2.29029600e-02],\n",
       "         [ -6.11180440e-02,   7.99829066e-02,   3.55080850e-02]],\n",
       " \n",
       "        [[ -7.14044645e-02,  -8.04416090e-02,   1.50023261e-03],\n",
       "         [  4.28843163e-02,  -8.08031410e-02,  -6.24813400e-02],\n",
       "         [ -1.54341031e-02,   2.92986017e-02,  -4.25904021e-02]],\n",
       " \n",
       "        [[ -2.39589233e-02,   1.87702589e-02,   6.57583401e-02],\n",
       "         [  5.24890982e-02,   2.01249346e-02,  -2.77631059e-02],\n",
       "         [  3.80170457e-02,   1.49831939e-02,  -3.55315534e-03]],\n",
       " \n",
       "        [[  3.70250233e-02,   2.94579305e-02,   3.41904834e-02],\n",
       "         [  4.82873842e-02,  -4.21813279e-02,   4.78948792e-03],\n",
       "         [ -5.16201695e-03,   3.01462635e-02,  -1.69154778e-02]],\n",
       " \n",
       "        [[  3.16559561e-02,  -7.24407062e-02,   2.36295033e-02],\n",
       "         [  5.13427295e-02,   7.04480633e-02,   2.83070784e-02],\n",
       "         [ -5.04549816e-02,   1.85911115e-02,   5.20922355e-02]],\n",
       " \n",
       "        [[ -5.02454787e-02,  -8.11312124e-02,   3.71255092e-02],\n",
       "         [  8.06685984e-02,  -7.32998922e-02,   9.73030832e-03],\n",
       "         [  1.56745054e-02,   5.67481555e-02,  -7.26822317e-02]],\n",
       " \n",
       "        [[  5.82388937e-02,  -3.73932607e-02,  -8.24915096e-02],\n",
       "         [ -8.02272186e-02,  -2.68515060e-03,  -7.74332583e-02],\n",
       "         [ -1.12435520e-02,  -3.97638641e-02,  -3.27718519e-02]],\n",
       " \n",
       "        [[ -1.92106403e-02,  -2.24479232e-02,  -7.86693618e-02],\n",
       "         [  5.77198006e-02,  -5.06875105e-02,   2.24855561e-02],\n",
       "         [ -4.87044454e-02,  -2.12209262e-02,   5.16919233e-02]],\n",
       " \n",
       "        [[ -7.95271248e-02,   7.31745809e-02,   5.92341051e-02],\n",
       "         [ -4.43269163e-02,   3.74187417e-02,   4.14410681e-02],\n",
       "         [ -4.54461314e-02,  -3.27168591e-02,  -6.24321513e-02]],\n",
       " \n",
       "        [[ -3.15656923e-02,  -7.87040442e-02,   7.44018555e-02],\n",
       "         [ -4.01191153e-02,   4.49386388e-02,   4.30658199e-02],\n",
       "         [ -4.40451242e-02,   3.75155024e-02,   4.70021218e-02]],\n",
       " \n",
       "        [[ -2.39976007e-03,  -5.93736710e-04,  -2.34039128e-02],\n",
       "         [  2.69613471e-02,   1.44980624e-02,   4.35823239e-02],\n",
       "         [ -4.61384766e-02,   4.77727652e-02,   6.25742674e-02]],\n",
       " \n",
       "        [[  4.67124488e-03,   1.91407073e-02,   6.45951703e-02],\n",
       "         [  2.23092046e-02,   8.26868042e-02,  -3.46959308e-02],\n",
       "         [ -1.59759354e-02,   4.02394831e-02,   7.74952397e-02]],\n",
       " \n",
       "        [[  2.70221382e-02,   2.52676960e-02,   6.95385188e-02],\n",
       "         [  2.89878380e-02,  -6.30358979e-02,  -4.19260748e-02],\n",
       "         [  3.26302350e-02,  -6.38915505e-03,  -5.24844266e-02]],\n",
       " \n",
       "        [[ -1.35678714e-02,   7.86665548e-03,   2.99551412e-02],\n",
       "         [  7.85320923e-02,   5.39493784e-02,  -8.21834579e-02],\n",
       "         [ -4.72679827e-03,  -4.11373600e-02,   1.34055912e-02]],\n",
       " \n",
       "        [[ -1.11206565e-02,  -1.58707500e-02,   7.68837184e-02],\n",
       "         [ -3.66290100e-02,   3.33434790e-02,  -3.34301107e-02],\n",
       "         [ -4.27404158e-02,  -2.08003540e-02,  -2.25036070e-02]],\n",
       " \n",
       "        [[  2.42547039e-03,  -7.35820383e-02,   3.52075472e-02],\n",
       "         [  2.44960468e-02,  -8.52645654e-03,  -7.39306360e-02],\n",
       "         [ -5.06901965e-02,   1.61592523e-03,  -3.77905183e-02]],\n",
       " \n",
       "        [[  1.89922135e-02,   8.02302882e-02,  -8.30256790e-02],\n",
       "         [ -1.05814319e-02,  -5.84063493e-02,  -1.71855614e-02],\n",
       "         [  1.75468549e-02,  -1.89747810e-02,  -6.53477460e-02]],\n",
       " \n",
       "        [[  2.66329292e-02,   6.70970529e-02,   2.26262026e-02],\n",
       "         [ -5.55278063e-02,   6.07874244e-02,  -8.27492923e-02],\n",
       "         [ -5.94505668e-02,  -6.96662813e-02,   4.92505468e-02]],\n",
       " \n",
       "        [[ -6.95565529e-03,   3.34345177e-02,  -5.70163270e-03],\n",
       "         [  5.28169237e-03,   5.12713529e-02,   5.39919063e-02],\n",
       "         [ -1.07500283e-03,  -1.37683703e-02,   5.55281155e-02]],\n",
       " \n",
       "        [[  7.32830912e-02,  -1.47419786e-02,  -6.68160245e-02],\n",
       "         [ -1.83385946e-02,  -7.12416545e-02,   1.80446692e-02],\n",
       "         [ -4.11446467e-02,   4.55781631e-02,   3.81001495e-02]],\n",
       " \n",
       "        [[  3.53611372e-02,  -2.47537550e-02,   2.59386376e-02],\n",
       "         [ -3.46439704e-02,  -5.02874479e-02,   2.39405036e-02],\n",
       "         [ -5.78697026e-02,  -5.33488579e-02,   7.96086118e-02]],\n",
       " \n",
       "        [[ -5.33245578e-02,   1.16043761e-02,   3.08506470e-02],\n",
       "         [  7.50586465e-02,  -2.58857813e-02,   5.74599244e-02],\n",
       "         [  9.04970616e-03,   2.99950857e-02,  -3.96593772e-02]],\n",
       " \n",
       "        [[ -1.78951095e-03,   5.66957779e-02,  -3.95032205e-02],\n",
       "         [  4.87602279e-02,   5.71313873e-02,  -3.80776040e-02],\n",
       "         [  8.18817019e-02,  -4.57284003e-02,  -8.18929449e-02]],\n",
       " \n",
       "        [[ -6.49944916e-02,   6.80119991e-02,  -8.12718198e-02],\n",
       "         [  4.31147292e-02,  -8.10269043e-02,   1.53160440e-02],\n",
       "         [ -2.06043478e-06,   1.98794547e-02,  -1.03432443e-02]],\n",
       " \n",
       "        [[ -2.99243839e-03,   1.86427012e-02,   6.46188632e-02],\n",
       "         [  4.16685678e-02,  -3.72109897e-02,   7.90463537e-02],\n",
       "         [ -3.72046488e-03,  -5.83987497e-03,   4.84478474e-02]],\n",
       " \n",
       "        [[  3.57938297e-02,  -3.69401053e-02,   3.86045082e-04],\n",
       "         [ -7.86106288e-02,   8.28883499e-02,   8.08567479e-02],\n",
       "         [  7.19601661e-02,  -7.94617236e-02,  -3.00177354e-02]],\n",
       " \n",
       "        [[ -7.18039796e-02,   5.65917976e-02,   6.22971207e-02],\n",
       "         [ -1.03550861e-02,  -7.31195435e-02,  -1.73944002e-03],\n",
       "         [  1.01697436e-02,  -3.51501890e-02,  -5.66131882e-02]],\n",
       " \n",
       "        [[  5.47151081e-02,  -3.13940383e-02,   5.47819957e-02],\n",
       "         [ -1.09261582e-02,  -6.17199801e-02,  -8.17685425e-02],\n",
       "         [ -6.47211634e-03,   5.29474840e-02,  -4.63343449e-02]],\n",
       " \n",
       "        [[ -2.31034160e-02,   7.13864192e-02,  -5.51636592e-02],\n",
       "         [ -3.02276146e-02,  -2.65716631e-02,  -1.49215488e-02],\n",
       "         [  1.74617991e-02,  -3.33650212e-04,   7.74769261e-02]],\n",
       " \n",
       "        [[  4.39432077e-02,  -5.92590123e-02,  -2.83069517e-02],\n",
       "         [  4.66986466e-03,  -5.60813658e-02,  -3.36887725e-02],\n",
       "         [ -5.07946834e-02,  -2.10475121e-02,   5.55268750e-02]],\n",
       " \n",
       "        [[ -3.35248839e-03,  -9.32185631e-03,  -4.97699231e-02],\n",
       "         [ -2.06013173e-02,  -3.63179408e-02,   1.36872884e-02],\n",
       "         [  7.92661905e-02,  -2.04287302e-02,   5.07123023e-02]],\n",
       " \n",
       "        [[  6.13866709e-02,   4.91442196e-02,   2.74611171e-02],\n",
       "         [ -5.71835265e-02,  -4.91628684e-02,   7.29072466e-02],\n",
       "         [ -3.98386866e-02,   7.01933801e-02,  -6.24473616e-02]]], dtype=float32), bias=0.00038498107, layer=1, neuron_number=58, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  2.38960842e-03,  -1.33583406e-02,  -2.29325332e-03],\n",
       "         [  6.18767962e-02,   1.63800661e-02,   8.18301663e-02],\n",
       "         [  3.19200242e-03,   7.04589710e-02,  -6.98887780e-02]],\n",
       " \n",
       "        [[ -6.03657924e-02,  -7.70855546e-02,  -8.25272724e-02],\n",
       "         [  3.18032429e-02,   5.51080815e-02,   6.03545196e-02],\n",
       "         [ -6.49954006e-02,   3.30535248e-02,  -4.26746309e-02]],\n",
       " \n",
       "        [[  4.00986113e-02,   1.62979849e-02,  -2.80968864e-02],\n",
       "         [  5.39301597e-02,  -9.56555363e-03,   3.19286548e-02],\n",
       "         [ -4.31384929e-02,  -2.20570564e-02,   4.86652460e-03]],\n",
       " \n",
       "        [[ -5.10268696e-02,  -4.60979305e-02,  -6.69248253e-02],\n",
       "         [ -1.02701550e-02,  -3.89734767e-02,   7.10612610e-02],\n",
       "         [ -3.92197333e-02,  -8.11701193e-02,   4.30114269e-02]],\n",
       " \n",
       "        [[ -4.70125005e-02,  -4.03485186e-02,   3.69869582e-02],\n",
       "         [  2.23394297e-02,   7.86198024e-03,  -7.44442418e-02],\n",
       "         [  3.34938131e-02,  -6.89104423e-02,  -8.59935395e-03]],\n",
       " \n",
       "        [[ -2.59636994e-02,  -4.44536246e-02,   8.28073267e-03],\n",
       "         [  4.60022390e-02,  -8.27631652e-02,  -6.68221191e-02],\n",
       "         [ -2.16252059e-02,  -7.76714161e-02,  -5.43780737e-02]],\n",
       " \n",
       "        [[ -3.58686745e-02,  -1.41017968e-02,   5.21553960e-03],\n",
       "         [  7.37467930e-02,   1.90208564e-04,  -8.13322291e-02],\n",
       "         [  1.97767112e-02,   1.91059720e-03,   5.40439561e-02]],\n",
       " \n",
       "        [[  4.73328829e-02,  -5.55496365e-02,   7.76002184e-02],\n",
       "         [ -7.62588233e-02,  -8.15934911e-02,   4.63982066e-03],\n",
       "         [  5.35524450e-02,  -1.60934851e-02,  -3.09749087e-03]],\n",
       " \n",
       "        [[ -1.87862255e-02,   4.17753821e-03,   6.26567826e-02],\n",
       "         [  4.45328467e-02,  -7.74573088e-02,  -2.22443435e-02],\n",
       "         [ -6.65581301e-02,   7.60888383e-02,   6.56406134e-02]],\n",
       " \n",
       "        [[  7.12892190e-02,   3.71234640e-02,  -3.19704181e-04],\n",
       "         [  5.28093949e-02,   3.72262560e-02,  -5.10988571e-02],\n",
       "         [  4.77715656e-02,   5.72081171e-02,   2.55807955e-02]],\n",
       " \n",
       "        [[ -5.05677238e-02,   2.27843467e-02,   4.03944403e-02],\n",
       "         [ -5.49742952e-02,   2.85089388e-02,  -3.76911387e-02],\n",
       "         [ -4.75958735e-02,   6.32090867e-02,  -7.20426738e-02]],\n",
       " \n",
       "        [[  2.48040948e-02,  -8.17158669e-02,  -4.17062864e-02],\n",
       "         [ -5.08399792e-02,  -4.39476334e-02,  -1.94774419e-02],\n",
       "         [  6.46448135e-02,   7.29396194e-02,  -3.35251391e-02]],\n",
       " \n",
       "        [[ -1.47043830e-02,  -3.18634920e-02,   7.29611963e-02],\n",
       "         [  1.50577221e-02,   4.39322554e-03,   2.73744985e-02],\n",
       "         [  7.95867890e-02,  -3.52607332e-02,   6.37756512e-02]],\n",
       " \n",
       "        [[ -6.69790013e-03,  -6.02438231e-04,   4.20560651e-02],\n",
       "         [  4.21918370e-02,  -8.11565295e-02,   5.88505454e-02],\n",
       "         [ -4.68020104e-02,  -7.10523501e-02,   4.10587080e-02]],\n",
       " \n",
       "        [[ -7.55225569e-02,   3.05424146e-02,   1.47600425e-02],\n",
       "         [  7.68405199e-02,  -3.90611286e-03,  -7.76162818e-02],\n",
       "         [ -2.72684544e-02,   2.66450662e-02,  -2.47327853e-02]],\n",
       " \n",
       "        [[ -3.23002115e-02,   3.05501949e-02,  -6.32762834e-02],\n",
       "         [ -3.81616689e-02,  -6.98159263e-02,   3.83202843e-02],\n",
       "         [ -4.60926890e-02,  -8.15607831e-02,  -3.96725833e-02]],\n",
       " \n",
       "        [[ -4.78535071e-02,   7.79164657e-02,   1.01969764e-02],\n",
       "         [ -7.34536946e-02,   6.47575315e-03,  -6.35997355e-02],\n",
       "         [  5.30735962e-02,   5.69310784e-03,   5.80448695e-02]],\n",
       " \n",
       "        [[ -6.56383038e-02,   6.49665892e-02,  -4.67371941e-02],\n",
       "         [ -7.98264816e-02,   5.76675199e-02,   7.53172413e-02],\n",
       "         [  6.39826804e-02,  -2.63300277e-02,  -8.55466723e-03]],\n",
       " \n",
       "        [[ -5.06348312e-02,  -7.23544955e-02,  -8.85492098e-03],\n",
       "         [  6.82383180e-02,   5.67439869e-02,   5.82920685e-02],\n",
       "         [ -2.97186002e-02,  -3.95126343e-02,  -6.15812577e-02]],\n",
       " \n",
       "        [[ -6.49677739e-02,  -5.62774912e-02,   4.48794253e-02],\n",
       "         [ -4.11015041e-02,   2.39528231e-02,  -5.37166595e-02],\n",
       "         [  3.90012898e-02,   3.59307975e-02,  -6.16029203e-02]],\n",
       " \n",
       "        [[  8.15776065e-02,  -4.48972546e-02,   2.47970205e-02],\n",
       "         [  6.92308694e-02,  -3.54965292e-02,  -1.79076232e-02],\n",
       "         [  5.81090488e-02,  -7.16068745e-02,   6.19737878e-02]],\n",
       " \n",
       "        [[  9.78547614e-03,  -5.08505665e-03,   5.59084490e-02],\n",
       "         [ -1.89397729e-03,  -4.60637398e-02,   7.74404500e-03],\n",
       "         [ -4.66068722e-02,  -1.24477753e-02,  -8.34471881e-02]],\n",
       " \n",
       "        [[ -6.06810711e-02,   6.03318866e-03,   7.43736029e-02],\n",
       "         [ -6.74965084e-02,  -6.78128796e-03,  -3.34550738e-02],\n",
       "         [  3.09707634e-02,  -1.10018747e-02,   9.74987634e-04]],\n",
       " \n",
       "        [[ -7.70486221e-02,   7.74772614e-02,  -4.18595970e-02],\n",
       "         [  4.62812558e-03,   6.01586476e-02,  -3.74255329e-02],\n",
       "         [ -4.90887053e-02,   6.99269474e-02,  -2.75972852e-05]],\n",
       " \n",
       "        [[  4.97677177e-02,   3.40965055e-02,  -1.62215736e-02],\n",
       "         [ -7.93609302e-04,   2.51770765e-02,  -1.91294234e-02],\n",
       "         [ -1.74621958e-02,  -4.35547940e-02,   2.88331844e-02]],\n",
       " \n",
       "        [[ -6.19930476e-02,   3.77022326e-02,  -6.84837103e-02],\n",
       "         [ -4.74460386e-02,  -7.30914548e-02,  -7.51295984e-02],\n",
       "         [ -6.47268966e-02,  -1.17525654e-02,   3.95483449e-02]],\n",
       " \n",
       "        [[ -2.93543693e-02,   4.00954895e-02,  -5.86242490e-02],\n",
       "         [  7.09684640e-02,   6.54638410e-02,  -3.26383375e-02],\n",
       "         [  3.09226997e-02,  -8.30439106e-02,   2.94602532e-02]],\n",
       " \n",
       "        [[ -2.94918679e-02,   7.06608891e-02,   1.89516079e-02],\n",
       "         [  3.52763198e-03,  -6.23200573e-02,  -4.99039814e-02],\n",
       "         [  7.12864026e-02,   6.70788363e-02,   3.09447385e-02]],\n",
       " \n",
       "        [[ -3.71415131e-02,   1.75009090e-02,  -2.27573197e-02],\n",
       "         [  6.69387430e-02,   1.70472655e-02,   4.76970337e-02],\n",
       "         [ -5.07038720e-02,  -7.24655017e-02,  -3.04492246e-02]],\n",
       " \n",
       "        [[ -5.37075512e-02,  -6.75289631e-02,   1.91055518e-02],\n",
       "         [  4.12995555e-02,   6.98251352e-02,   7.27449283e-02],\n",
       "         [  1.99999940e-02,   3.98655534e-02,   5.84677607e-02]],\n",
       " \n",
       "        [[  3.65500562e-02,   5.04613183e-02,  -7.18058944e-02],\n",
       "         [  7.62676671e-02,  -5.21567538e-02,  -5.84756322e-02],\n",
       "         [ -2.46917568e-02,  -5.09603918e-02,   1.45107182e-02]],\n",
       " \n",
       "        [[ -1.52017744e-02,   7.77871683e-02,   5.42035922e-02],\n",
       "         [  4.37461361e-02,  -4.61314730e-02,   1.40901553e-02],\n",
       "         [ -4.56989855e-02,  -1.40478229e-02,  -2.26487741e-02]]], dtype=float32), bias=0.00062729587, layer=1, neuron_number=59, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.02093339, -0.01525701,  0.01657317],\n",
       "         [ 0.05656234, -0.07455239,  0.03506584],\n",
       "         [-0.081113  ,  0.04098906,  0.02488753]],\n",
       " \n",
       "        [[ 0.07874295,  0.07411002,  0.03194632],\n",
       "         [-0.01554794,  0.00282748,  0.0506969 ],\n",
       "         [ 0.0117751 ,  0.03119217,  0.06194473]],\n",
       " \n",
       "        [[ 0.04434161,  0.0633719 ,  0.01037185],\n",
       "         [ 0.04060624,  0.02109866,  0.06801093],\n",
       "         [ 0.05624231, -0.06396052, -0.03253776]],\n",
       " \n",
       "        [[-0.06094346,  0.03067145,  0.00492865],\n",
       "         [-0.03737978,  0.04461341,  0.07036717],\n",
       "         [ 0.07477134,  0.06090117,  0.059484  ]],\n",
       " \n",
       "        [[-0.01792938, -0.04374322, -0.00072961],\n",
       "         [ 0.07658568, -0.02348922,  0.07372115],\n",
       "         [ 0.01727322,  0.04983101, -0.00673769]],\n",
       " \n",
       "        [[ 0.0227285 , -0.07596335, -0.05016559],\n",
       "         [-0.02486013,  0.01668568,  0.0776244 ],\n",
       "         [ 0.06689706, -0.05712647, -0.05644434]],\n",
       " \n",
       "        [[ 0.00628994, -0.03614446, -0.01783689],\n",
       "         [-0.06882075, -0.01714692, -0.04187595],\n",
       "         [ 0.01495981, -0.05370783,  0.07335038]],\n",
       " \n",
       "        [[ 0.0237812 , -0.01171885,  0.06067343],\n",
       "         [ 0.07053936,  0.01375521, -0.02755282],\n",
       "         [-0.0775753 , -0.00509732,  0.04316487]],\n",
       " \n",
       "        [[ 0.06267162,  0.02611827, -0.05894112],\n",
       "         [ 0.0338636 ,  0.03884677,  0.07279605],\n",
       "         [ 0.02199678, -0.05092669,  0.07674857]],\n",
       " \n",
       "        [[-0.03078369,  0.05964233, -0.02878034],\n",
       "         [ 0.00500992,  0.02815237, -0.03045004],\n",
       "         [ 0.00722117, -0.0620697 ,  0.06851571]],\n",
       " \n",
       "        [[ 0.06726518,  0.03448037, -0.02100471],\n",
       "         [ 0.08188296, -0.05138998, -0.06402977],\n",
       "         [-0.02306707,  0.05998646,  0.06974252]],\n",
       " \n",
       "        [[ 0.05368719,  0.05946481, -0.08081491],\n",
       "         [ 0.06714715,  0.0235039 , -0.05435441],\n",
       "         [ 0.03677334,  0.04587017,  0.03010585]],\n",
       " \n",
       "        [[ 0.06815268, -0.08406594, -0.03798991],\n",
       "         [ 0.00789687, -0.01646947,  0.01834956],\n",
       "         [-0.03508453,  0.07808392, -0.02638463]],\n",
       " \n",
       "        [[ 0.07829403,  0.0082469 ,  0.04783354],\n",
       "         [ 0.00241636,  0.04506385, -0.0499425 ],\n",
       "         [ 0.05910574, -0.05382138, -0.01785395]],\n",
       " \n",
       "        [[-0.031339  , -0.07625974,  0.04066657],\n",
       "         [ 0.0381933 ,  0.02088107, -0.06489503],\n",
       "         [ 0.02792331, -0.01451385,  0.03666399]],\n",
       " \n",
       "        [[-0.04903238, -0.04083093, -0.00330231],\n",
       "         [-0.07308272, -0.04905757,  0.05704255],\n",
       "         [ 0.02794821, -0.00261534,  0.03847136]],\n",
       " \n",
       "        [[ 0.0696927 ,  0.05072058,  0.05654082],\n",
       "         [ 0.06829228,  0.04082922,  0.04662336],\n",
       "         [-0.03545329,  0.00301597, -0.03592835]],\n",
       " \n",
       "        [[ 0.05533703,  0.01439916, -0.06696287],\n",
       "         [ 0.05253001, -0.06439228,  0.00383865],\n",
       "         [-0.02107963, -0.00067128, -0.01967749]],\n",
       " \n",
       "        [[-0.06186989, -0.07614912, -0.02300368],\n",
       "         [ 0.02557788,  0.01618972,  0.04732442],\n",
       "         [-0.03893172, -0.07411116,  0.00277834]],\n",
       " \n",
       "        [[ 0.02862682, -0.03506653, -0.06771615],\n",
       "         [-0.01035832, -0.07114766,  0.04510272],\n",
       "         [ 0.03430798,  0.04953426, -0.01818194]],\n",
       " \n",
       "        [[ 0.01423421,  0.00744117,  0.00471881],\n",
       "         [-0.03920616,  0.0740639 ,  0.00111904],\n",
       "         [-0.0455478 ,  0.02513683, -0.04040378]],\n",
       " \n",
       "        [[ 0.01976484, -0.0154687 ,  0.03292242],\n",
       "         [ 0.02299439,  0.05498599,  0.0272602 ],\n",
       "         [-0.02911653,  0.00472341,  0.07017463]],\n",
       " \n",
       "        [[ 0.0691206 , -0.02086671,  0.06016952],\n",
       "         [-0.06993965, -0.04767581,  0.00362968],\n",
       "         [ 0.04127666, -0.06797114,  0.02046999]],\n",
       " \n",
       "        [[ 0.04511068, -0.08361249,  0.0696081 ],\n",
       "         [-0.03358021,  0.07980367,  0.05080217],\n",
       "         [ 0.04935112, -0.06000107, -0.06459024]],\n",
       " \n",
       "        [[ 0.00351733,  0.05535274,  0.01677721],\n",
       "         [ 0.07887205, -0.0729254 ,  0.07327107],\n",
       "         [-0.02447853,  0.01807361,  0.02358823]],\n",
       " \n",
       "        [[-0.05097637,  0.04614493,  0.0162241 ],\n",
       "         [ 0.01366303, -0.01146673,  0.06293829],\n",
       "         [ 0.05283981,  0.0383825 , -0.07084151]],\n",
       " \n",
       "        [[ 0.00038298,  0.07555957, -0.01545355],\n",
       "         [ 0.06433006, -0.04973654, -0.00484403],\n",
       "         [ 0.04710953, -0.06118903,  0.07264333]],\n",
       " \n",
       "        [[-0.04564466,  0.03767957, -0.03778759],\n",
       "         [ 0.02992641, -0.00773703, -0.0353973 ],\n",
       "         [ 0.04178574, -0.07304876, -0.01553248]],\n",
       " \n",
       "        [[-0.03947221,  0.02036091, -0.04010676],\n",
       "         [ 0.021763  , -0.02706038, -0.0669127 ],\n",
       "         [-0.07622591,  0.04991322,  0.06595886]],\n",
       " \n",
       "        [[-0.03283583, -0.01770729,  0.01602538],\n",
       "         [-0.05984178,  0.04282565,  0.03363497],\n",
       "         [-0.022257  ,  0.05252916,  0.0621951 ]],\n",
       " \n",
       "        [[-0.01279977, -0.00702937,  0.03557356],\n",
       "         [-0.01495636,  0.05172961, -0.05739904],\n",
       "         [-0.05830047,  0.0527208 , -0.08192768]],\n",
       " \n",
       "        [[ 0.04367931, -0.02275993,  0.07493923],\n",
       "         [-0.04589196,  0.01437543, -0.06992818],\n",
       "         [-0.0719616 , -0.00299884, -0.00808464]]], dtype=float32), bias=-0.0004391041, layer=1, neuron_number=60, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[ 0.05946526, -0.02291526, -0.08299099],\n",
       "         [ 0.01336671, -0.06343078,  0.03129439],\n",
       "         [ 0.03915637,  0.06493703,  0.07661098]],\n",
       " \n",
       "        [[ 0.04836588,  0.02597794, -0.04465383],\n",
       "         [-0.00268327,  0.02602226, -0.01748004],\n",
       "         [-0.01201825,  0.07324582, -0.00524369]],\n",
       " \n",
       "        [[-0.06477116, -0.00087202, -0.04912811],\n",
       "         [ 0.0205465 ,  0.04661389, -0.01883692],\n",
       "         [-0.03413523,  0.01368198,  0.03821381]],\n",
       " \n",
       "        [[ 0.02570692, -0.00532748,  0.02541346],\n",
       "         [ 0.06532771,  0.02162877,  0.02845446],\n",
       "         [-0.02983742,  0.05607262, -0.06588513]],\n",
       " \n",
       "        [[ 0.02054499,  0.02540589, -0.0691663 ],\n",
       "         [-0.00242878,  0.01174426, -0.08249211],\n",
       "         [ 0.01219323,  0.00417449,  0.00398495]],\n",
       " \n",
       "        [[-0.07282175,  0.01267747, -0.06948793],\n",
       "         [-0.0484795 , -0.03041833,  0.08127408],\n",
       "         [-0.03339016,  0.0420893 , -0.02566106]],\n",
       " \n",
       "        [[-0.04929796,  0.07624699,  0.06213059],\n",
       "         [-0.05920149,  0.03874402, -0.03127015],\n",
       "         [ 0.0209536 , -0.0356358 ,  0.00399088]],\n",
       " \n",
       "        [[ 0.02364735, -0.05592399, -0.06401289],\n",
       "         [-0.0004259 , -0.00431343,  0.02298483],\n",
       "         [ 0.05497767, -0.07309153, -0.05247946]],\n",
       " \n",
       "        [[ 0.03093052,  0.07237158, -0.05475641],\n",
       "         [-0.04474308, -0.0367846 , -0.07293977],\n",
       "         [-0.04732295,  0.04849564,  0.06771661]],\n",
       " \n",
       "        [[ 0.07024076, -0.03707089, -0.06445453],\n",
       "         [ 0.07793036,  0.00178902,  0.01313852],\n",
       "         [-0.01234877, -0.0042588 , -0.08369303]],\n",
       " \n",
       "        [[ 0.05839026,  0.02090693,  0.04700327],\n",
       "         [-0.06771731, -0.03062674,  0.08288746],\n",
       "         [ 0.05094247, -0.00852493,  0.02585865]],\n",
       " \n",
       "        [[-0.01248889,  0.01155759,  0.04520539],\n",
       "         [ 0.03140373, -0.06542429,  0.00595926],\n",
       "         [ 0.00122809, -0.04407955,  0.02701018]],\n",
       " \n",
       "        [[ 0.00731289,  0.0484776 , -0.0456905 ],\n",
       "         [-0.00551882, -0.01787406, -0.01529882],\n",
       "         [ 0.03547372, -0.07706936,  0.070525  ]],\n",
       " \n",
       "        [[ 0.06094093,  0.02577496, -0.03599131],\n",
       "         [-0.08186509, -0.07106039, -0.00645051],\n",
       "         [-0.08360425, -0.04153194,  0.04452617]],\n",
       " \n",
       "        [[-0.06940335, -0.03329508, -0.05659995],\n",
       "         [ 0.07679226, -0.03921988,  0.02204128],\n",
       "         [ 0.02661623,  0.06112087,  0.02634502]],\n",
       " \n",
       "        [[-0.08064029,  0.04483898, -0.03622137],\n",
       "         [ 0.03979604,  0.02231409,  0.03609377],\n",
       "         [ 0.03584102, -0.00308175, -0.06424535]],\n",
       " \n",
       "        [[ 0.03308419, -0.05557727,  0.02331829],\n",
       "         [-0.03429693, -0.02964541, -0.03814885],\n",
       "         [ 0.0377441 ,  0.08038348, -0.02343822]],\n",
       " \n",
       "        [[ 0.02588833,  0.05668643,  0.01500466],\n",
       "         [ 0.00220376, -0.02974055,  0.02475498],\n",
       "         [ 0.05732609, -0.05386211,  0.02032591]],\n",
       " \n",
       "        [[ 0.03911662,  0.01298825,  0.05163135],\n",
       "         [ 0.07526653, -0.07597719, -0.05442607],\n",
       "         [-0.05540296, -0.02794191, -0.04402665]],\n",
       " \n",
       "        [[ 0.02810385,  0.05800134, -0.03292532],\n",
       "         [-0.01893061, -0.00551869,  0.01764833],\n",
       "         [-0.00211102, -0.05443315,  0.05494055]],\n",
       " \n",
       "        [[ 0.05734846,  0.07857239, -0.01939359],\n",
       "         [ 0.02438777,  0.01535949, -0.02040306],\n",
       "         [ 0.05668837,  0.04724747, -0.03062209]],\n",
       " \n",
       "        [[ 0.03292215,  0.03391587, -0.05033216],\n",
       "         [ 0.07501006,  0.03746481, -0.01762164],\n",
       "         [ 0.01682073, -0.05501875,  0.08124383]],\n",
       " \n",
       "        [[ 0.05456881,  0.06115279,  0.02340735],\n",
       "         [ 0.01839001,  0.03212502,  0.06716346],\n",
       "         [ 0.04417707, -0.0500807 ,  0.05922919]],\n",
       " \n",
       "        [[ 0.05103012, -0.03783588, -0.00994543],\n",
       "         [ 0.01842207, -0.02920622, -0.04420709],\n",
       "         [ 0.04860363, -0.04311646, -0.08158834]],\n",
       " \n",
       "        [[-0.03821077, -0.08233751,  0.04305826],\n",
       "         [ 0.01136408,  0.08079805,  0.06528286],\n",
       "         [ 0.07966627,  0.04049684, -0.00232474]],\n",
       " \n",
       "        [[ 0.02585387,  0.04678683,  0.06049731],\n",
       "         [ 0.07171066, -0.08417732, -0.03439253],\n",
       "         [-0.00791061,  0.00562366,  0.05905462]],\n",
       " \n",
       "        [[ 0.03388219,  0.04601076,  0.04615594],\n",
       "         [ 0.04755101, -0.04296769,  0.02553227],\n",
       "         [-0.06910673,  0.0605996 ,  0.04985522]],\n",
       " \n",
       "        [[ 0.03332571,  0.03355445,  0.00805008],\n",
       "         [ 0.0730266 , -0.03515306, -0.0239845 ],\n",
       "         [-0.04282732,  0.04941299,  0.04704352]],\n",
       " \n",
       "        [[-0.00840933,  0.04393527, -0.08406402],\n",
       "         [ 0.06779449, -0.05845482, -0.04221789],\n",
       "         [-0.03944371,  0.06138703, -0.06179009]],\n",
       " \n",
       "        [[-0.03099507, -0.00678552,  0.02117439],\n",
       "         [ 0.04090377, -0.02466326,  0.07017104],\n",
       "         [-0.05872655, -0.06977703, -0.07752647]],\n",
       " \n",
       "        [[ 0.02469937,  0.07966012,  0.0574398 ],\n",
       "         [-0.01992183,  0.06634753, -0.01154231],\n",
       "         [-0.02101763,  0.04258381, -0.08114377]],\n",
       " \n",
       "        [[ 0.06823153, -0.05464451, -0.07190083],\n",
       "         [ 0.02011155, -0.04431068,  0.02676146],\n",
       "         [ 0.05718137, -0.02169983, -0.06768516]]], dtype=float32), bias=-0.00064056099, layer=1, neuron_number=61, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[-0.01464706, -0.02355027, -0.04694302],\n",
       "         [ 0.06667777, -0.07673869,  0.05489066],\n",
       "         [ 0.02962892, -0.01354652, -0.07592086]],\n",
       " \n",
       "        [[ 0.07802344, -0.02360611, -0.01228579],\n",
       "         [ 0.06438362, -0.0548086 , -0.06965667],\n",
       "         [-0.0680564 , -0.02888814,  0.03875127]],\n",
       " \n",
       "        [[-0.03298619, -0.06690288,  0.07790241],\n",
       "         [ 0.00554302,  0.06392909,  0.07650899],\n",
       "         [-0.07680139,  0.00601491,  0.06205585]],\n",
       " \n",
       "        [[-0.02365061,  0.05076459,  0.08392416],\n",
       "         [-0.08197019,  0.02966288,  0.07746682],\n",
       "         [ 0.02670114, -0.0375907 , -0.02395262]],\n",
       " \n",
       "        [[-0.04791771, -0.03866978,  0.00568575],\n",
       "         [-0.0585025 ,  0.02255787,  0.07815987],\n",
       "         [ 0.06825417,  0.03456519, -0.01152677]],\n",
       " \n",
       "        [[ 0.02092082, -0.07049198,  0.03713429],\n",
       "         [ 0.00153419,  0.06860431,  0.03034235],\n",
       "         [-0.01233781, -0.06644477,  0.04828558]],\n",
       " \n",
       "        [[-0.03459704,  0.03992748,  0.01261483],\n",
       "         [-0.03725266,  0.02912384,  0.07466225],\n",
       "         [ 0.03317685, -0.02327457, -0.03984484]],\n",
       " \n",
       "        [[-0.04823535,  0.02113569, -0.02699629],\n",
       "         [ 0.04263699, -0.05029444, -0.07318643],\n",
       "         [-0.07537477,  0.03659547,  0.04508277]],\n",
       " \n",
       "        [[-0.02930363,  0.05893826,  0.03682877],\n",
       "         [ 0.05401559,  0.00652724,  0.07485771],\n",
       "         [-0.05224125,  0.01175086,  0.03097303]],\n",
       " \n",
       "        [[ 0.04714731,  0.07627863, -0.01682621],\n",
       "         [-0.02414868, -0.00433383, -0.01538513],\n",
       "         [-0.07464022, -0.04465009, -0.06761837]],\n",
       " \n",
       "        [[ 0.00555766, -0.0735893 ,  0.04605711],\n",
       "         [-0.02633029, -0.01952709, -0.04449563],\n",
       "         [-0.00595109, -0.00620072,  0.03672227]],\n",
       " \n",
       "        [[-0.07038641,  0.02119688, -0.0800306 ],\n",
       "         [ 0.07928766,  0.05838197,  0.0658415 ],\n",
       "         [-0.07403894,  0.01845166, -0.034081  ]],\n",
       " \n",
       "        [[ 0.05022608, -0.03726551, -0.04134579],\n",
       "         [-0.02639029, -0.02609961, -0.06336945],\n",
       "         [ 0.07598665,  0.05816364, -0.00675784]],\n",
       " \n",
       "        [[ 0.06131496, -0.02058594,  0.06176473],\n",
       "         [-0.00479392, -0.07006273,  0.0215897 ],\n",
       "         [-0.05958692, -0.06037058, -0.08044964]],\n",
       " \n",
       "        [[-0.08060148,  0.06775784, -0.03779297],\n",
       "         [ 0.01507149,  0.04097543,  0.06418692],\n",
       "         [ 0.05842105,  0.05750749, -0.04612095]],\n",
       " \n",
       "        [[-0.03675806,  0.02007744,  0.01570619],\n",
       "         [-0.00707098,  0.0828529 , -0.02628869],\n",
       "         [ 0.02533696, -0.05653204, -0.07222635]],\n",
       " \n",
       "        [[-0.03431159,  0.07503614, -0.01518253],\n",
       "         [ 0.06345811, -0.04616218,  0.05990824],\n",
       "         [ 0.04686887,  0.03506903, -0.04007737]],\n",
       " \n",
       "        [[ 0.04892665, -0.03210713, -0.00357673],\n",
       "         [-0.03292687, -0.03307097,  0.07029842],\n",
       "         [-0.02567485,  0.026823  ,  0.01288028]],\n",
       " \n",
       "        [[ 0.00618552, -0.04053162, -0.02672764],\n",
       "         [ 0.08049594, -0.05882142,  0.03852244],\n",
       "         [-0.03777346,  0.06636437,  0.01551385]],\n",
       " \n",
       "        [[ 0.03739751, -0.04668817, -0.0483898 ],\n",
       "         [ 0.04057098, -0.05169694,  0.02587787],\n",
       "         [-0.00561318, -0.06786545,  0.01140771]],\n",
       " \n",
       "        [[ 0.06808951,  0.04500312, -0.03690543],\n",
       "         [-0.06107939,  0.04031467, -0.05378919],\n",
       "         [-0.08064947, -0.05107161, -0.05952035]],\n",
       " \n",
       "        [[ 0.07781446, -0.03001377, -0.0453508 ],\n",
       "         [ 0.02609717, -0.01026834, -0.0369745 ],\n",
       "         [-0.04742937,  0.04218499,  0.06706061]],\n",
       " \n",
       "        [[ 0.03376287,  0.0507367 ,  0.0078972 ],\n",
       "         [-0.07097001, -0.07945086, -0.06033209],\n",
       "         [-0.05644935, -0.06629985, -0.00075048]],\n",
       " \n",
       "        [[-0.00061209, -0.03448007, -0.04768265],\n",
       "         [ 0.0004045 , -0.08160736,  0.00128606],\n",
       "         [-0.00617692,  0.02351724, -0.07934245]],\n",
       " \n",
       "        [[-0.03601495,  0.02940344,  0.01501433],\n",
       "         [-0.00401001,  0.07473227,  0.02962666],\n",
       "         [-0.04628757, -0.02912942, -0.02546448]],\n",
       " \n",
       "        [[-0.0688015 , -0.03227992, -0.02046397],\n",
       "         [-0.03287172, -0.06342757,  0.06885035],\n",
       "         [-0.05970391, -0.04806678,  0.00832173]],\n",
       " \n",
       "        [[ 0.04834738,  0.01741374, -0.05172996],\n",
       "         [-0.00956375,  0.04071018,  0.04743227],\n",
       "         [ 0.03083629, -0.04933117, -0.03998537]],\n",
       " \n",
       "        [[-0.05221092, -0.02828247,  0.01182215],\n",
       "         [ 0.01894161, -0.08321836, -0.07817948],\n",
       "         [ 0.06240915,  0.00064634,  0.06545208]],\n",
       " \n",
       "        [[-0.02195471, -0.02492335, -0.03093945],\n",
       "         [-0.04036259,  0.02636701, -0.03784139],\n",
       "         [-0.02039841,  0.05775267, -0.04716471]],\n",
       " \n",
       "        [[-0.04795489, -0.01593304,  0.00026927],\n",
       "         [ 0.03788095,  0.07573815, -0.05752467],\n",
       "         [-0.08000495,  0.05516966, -0.02271504]],\n",
       " \n",
       "        [[-0.07988755,  0.03565805, -0.04939167],\n",
       "         [-0.02854676,  0.06573428,  0.05822891],\n",
       "         [-0.02358196, -0.00026405, -0.03255834]],\n",
       " \n",
       "        [[-0.07851461,  0.004752  ,  0.05090928],\n",
       "         [-0.02753964,  0.06247005,  0.00800074],\n",
       "         [ 0.02915147,  0.0590097 ,  0.08209521]]], dtype=float32), bias=0.00030698831, layer=1, neuron_number=62, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}}),\n",
       " GraphableNeruon(weights=array([[[  2.63265707e-02,   2.60764472e-02,   7.10992590e-02],\n",
       "         [ -8.24187398e-02,  -7.98825845e-02,  -3.61018889e-02],\n",
       "         [  8.71295854e-03,  -1.68821849e-02,   6.68018684e-02]],\n",
       " \n",
       "        [[ -6.15046173e-02,   4.28832509e-02,  -2.58789677e-02],\n",
       "         [  1.09517593e-02,   3.32726985e-02,   7.83770904e-03],\n",
       "         [ -5.97176999e-02,  -6.34617805e-02,   2.92196572e-02]],\n",
       " \n",
       "        [[  4.29022359e-03,  -4.86492664e-02,   2.89045461e-02],\n",
       "         [  7.55587593e-03,  -7.11550191e-02,  -5.53077161e-02],\n",
       "         [  5.86467944e-02,  -6.91040931e-03,   6.92436919e-02]],\n",
       " \n",
       "        [[ -5.59516661e-02,   1.64702665e-02,  -1.58215873e-02],\n",
       "         [  3.32153961e-02,   8.00154135e-02,  -6.96258023e-02],\n",
       "         [  8.61069188e-03,   2.19985712e-02,   5.37087861e-03]],\n",
       " \n",
       "        [[  2.34986506e-02,   3.06804068e-02,   1.99840590e-03],\n",
       "         [  3.82536128e-02,   4.43808623e-02,   4.38505150e-02],\n",
       "         [ -7.37819970e-02,   5.09923324e-02,   4.66680527e-02]],\n",
       " \n",
       "        [[  1.07646827e-02,   7.78877921e-03,   6.67765066e-02],\n",
       "         [  2.15266598e-03,   2.20956579e-02,   5.89198805e-02],\n",
       "         [  5.50152594e-03,   5.27957864e-02,   1.75412036e-02]],\n",
       " \n",
       "        [[ -7.65424743e-02,  -7.44420439e-02,   1.65233612e-02],\n",
       "         [  7.10810050e-02,  -7.41246268e-02,   1.48310605e-02],\n",
       "         [  7.27048740e-02,   4.85801548e-02,   5.83135150e-03]],\n",
       " \n",
       "        [[  3.37252170e-02,  -7.84422979e-02,   6.42172024e-02],\n",
       "         [  7.37381727e-02,  -8.01185146e-02,   6.95687439e-03],\n",
       "         [ -5.29308580e-02,  -6.41153753e-02,  -3.01288348e-02]],\n",
       " \n",
       "        [[  3.35166827e-02,   7.56449103e-02,   5.32886311e-02],\n",
       "         [ -2.06359709e-03,   4.56396267e-02,  -2.16209125e-02],\n",
       "         [ -4.10808288e-02,  -2.72899494e-03,  -7.98293129e-02]],\n",
       " \n",
       "        [[  7.38718212e-02,   5.91046363e-02,   7.55973533e-02],\n",
       "         [ -5.16334623e-02,   1.54788047e-02,   7.90150538e-02],\n",
       "         [  8.00797120e-02,  -3.75671010e-03,   6.24881685e-02]],\n",
       " \n",
       "        [[  8.02393258e-02,   4.94131222e-02,  -2.52234079e-02],\n",
       "         [  4.62614186e-02,   1.05663156e-02,   1.76761355e-02],\n",
       "         [  2.03776006e-02,  -2.41206996e-02,   6.45414963e-02]],\n",
       " \n",
       "        [[ -1.66337676e-02,   2.68348791e-02,  -1.35017950e-02],\n",
       "         [ -6.34461716e-02,  -5.37715144e-02,   7.47233033e-02],\n",
       "         [ -3.25215980e-02,   4.27258164e-02,   1.57302171e-02]],\n",
       " \n",
       "        [[ -2.04460248e-02,   7.68937021e-02,  -3.16017680e-02],\n",
       "         [ -6.17128350e-02,  -1.55577343e-02,  -9.68183484e-03],\n",
       "         [ -2.17593685e-02,   3.19803394e-02,   2.48088147e-02]],\n",
       " \n",
       "        [[ -4.27473485e-02,  -8.10838863e-02,   4.77174073e-02],\n",
       "         [  2.16486081e-02,  -8.12720433e-02,  -3.07420846e-02],\n",
       "         [  1.45042418e-02,   4.10530232e-02,  -1.86890680e-02]],\n",
       " \n",
       "        [[ -7.31495693e-02,   7.02436343e-02,  -5.22702672e-02],\n",
       "         [ -7.91250542e-02,  -7.56724924e-02,  -6.51103929e-02],\n",
       "         [  7.79461861e-02,  -4.25675847e-02,  -6.93117082e-02]],\n",
       " \n",
       "        [[  5.39778844e-02,  -2.36007310e-02,   2.72959471e-02],\n",
       "         [ -7.37732127e-02,  -7.87083283e-02,  -8.05809200e-02],\n",
       "         [ -7.28307515e-02,  -4.87925634e-02,  -7.02385157e-02]],\n",
       " \n",
       "        [[  5.85054345e-02,  -3.12104244e-02,   1.97393652e-02],\n",
       "         [ -6.00071736e-02,   6.80810213e-02,   1.49382483e-02],\n",
       "         [ -4.80652153e-02,   2.46684123e-02,   6.65685907e-02]],\n",
       " \n",
       "        [[ -6.21095300e-02,  -6.07658327e-02,  -7.65117109e-02],\n",
       "         [ -6.66679535e-03,   3.48655917e-02,   6.47089854e-02],\n",
       "         [ -6.89283684e-02,  -6.96209073e-02,   3.16163488e-02]],\n",
       " \n",
       "        [[  3.58645655e-02,  -3.21070813e-02,  -2.52324659e-02],\n",
       "         [  4.87857386e-02,  -4.94336747e-02,  -7.90272504e-02],\n",
       "         [ -5.24189845e-02,   4.29580249e-02,   6.82071447e-02]],\n",
       " \n",
       "        [[ -1.67620387e-02,   3.46493833e-02,  -7.78028518e-02],\n",
       "         [ -8.18925872e-02,  -6.97532371e-02,   3.09488084e-02],\n",
       "         [ -4.06947508e-02,   5.68280555e-02,   1.66668892e-02]],\n",
       " \n",
       "        [[ -3.93108092e-02,   5.89554533e-02,  -8.08774456e-02],\n",
       "         [  8.75724480e-03,  -6.25837520e-02,  -2.87226494e-02],\n",
       "         [ -3.25393192e-02,   3.88009436e-02,   8.08711052e-02]],\n",
       " \n",
       "        [[ -2.71696877e-03,  -6.01995783e-03,   4.56621349e-02],\n",
       "         [  6.28781021e-02,   2.98424140e-02,  -2.94152759e-02],\n",
       "         [  5.51109314e-02,  -2.36459449e-02,  -1.78855229e-02]],\n",
       " \n",
       "        [[ -7.36196041e-02,  -3.22478898e-02,   1.17181055e-02],\n",
       "         [  3.55549715e-02,   4.24039885e-02,   8.02372396e-03],\n",
       "         [ -2.41593830e-02,  -2.12869328e-02,  -6.48139268e-02]],\n",
       " \n",
       "        [[  7.90358111e-02,   5.77230752e-02,  -7.06159044e-03],\n",
       "         [ -6.50965869e-02,  -6.67105690e-02,   3.62075754e-02],\n",
       "         [ -3.39930356e-02,   7.32282251e-02,   5.47228232e-02]],\n",
       " \n",
       "        [[ -2.48838980e-02,  -2.16378551e-02,   7.76203647e-02],\n",
       "         [ -7.83872209e-04,  -6.69047758e-02,  -8.41176417e-03],\n",
       "         [ -1.11210523e-02,  -2.86558382e-02,   1.45284664e-02]],\n",
       " \n",
       "        [[  1.08162081e-02,   6.11305982e-02,   1.60779536e-03],\n",
       "         [  7.07637668e-02,  -8.10487792e-02,  -3.09797749e-03],\n",
       "         [  2.11141072e-03,   7.56098107e-02,   7.78751746e-02]],\n",
       " \n",
       "        [[  6.59182295e-02,  -8.06444958e-02,   1.65604055e-02],\n",
       "         [  1.86295994e-02,  -6.96967021e-02,  -2.61090007e-02],\n",
       "         [  4.82561737e-02,   2.88578793e-02,   2.26380117e-02]],\n",
       " \n",
       "        [[  1.93082038e-02,   3.74282040e-02,   2.97005773e-02],\n",
       "         [  3.16289961e-02,  -4.82774600e-02,   2.91742776e-02],\n",
       "         [  8.87693651e-03,  -3.37328352e-02,   6.05735481e-02]],\n",
       " \n",
       "        [[  6.09158874e-02,  -5.09626493e-02,   1.05924075e-02],\n",
       "         [ -1.32813193e-02,  -6.98071197e-02,  -3.71179618e-02],\n",
       "         [  7.50073791e-02,  -2.41317432e-02,  -5.58931306e-02]],\n",
       " \n",
       "        [[ -5.81320636e-02,  -4.98948939e-05,   7.80191123e-02],\n",
       "         [ -7.83854946e-02,  -3.21967877e-03,  -6.44914955e-02],\n",
       "         [ -6.62094578e-02,  -7.69668967e-02,  -4.77662273e-02]],\n",
       " \n",
       "        [[  7.18145166e-03,   7.88784251e-02,   2.33046617e-02],\n",
       "         [ -6.39434112e-03,  -7.00818747e-02,  -2.87237335e-02],\n",
       "         [ -2.05601957e-02,   7.83003569e-02,  -5.52828498e-02]],\n",
       " \n",
       "        [[  7.01367036e-02,   8.25968310e-02,  -7.16580525e-02],\n",
       "         [ -2.06876267e-02,   3.96768674e-02,  -7.73511380e-02],\n",
       "         [ -2.43579950e-02,  -8.11480731e-02,  -2.31113788e-02]]], dtype=float32), bias=0.0021168794, layer=1, neuron_number=63, layer_type={'use_bias': True, 'strides': (1, 1), 'dilation_rate': (1, 1), 'trainable': True, 'data_format': 'channels_last', 'kernel_regularizer': None, 'padding': 'valid', 'name': 'conv2d_8', 'bias_regularizer': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'filters': 64, 'kernel_size': (3, 3), 'activity_regularizer': None, 'bias_constraint': None, 'kernel_constraint': None, 'activation': 'relu', 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'distribution': 'uniform', 'seed': None, 'mode': 'fan_avg', 'scale': 1.0}}})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
